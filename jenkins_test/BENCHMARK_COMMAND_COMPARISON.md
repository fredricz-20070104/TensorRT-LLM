# Benchmark å‘½ä»¤å¯¹æ¯”è¯¦è§£

> å¯¹æ¯” test_perf_sanity.pyã€run_benchmark.shã€run_benchmark_nv_sa.sh ç”Ÿæˆçš„ benchmark å‘½ä»¤

---

## ğŸ“‹ ä¸‰ç§ Benchmark å‘½ä»¤æ¥æº

### 1. test_perf_sanity.pyï¼ˆè‡ªåŠ¨åŒ–æµ‹è¯•ï¼‰
- **ä½ç½®ï¼š** `tests/integration/defs/perf/test_perf_sanity.py`
- **ç”¨é€”ï¼š** Jenkins CI/CD è‡ªåŠ¨åŒ–æ€§èƒ½æµ‹è¯•
- **ç‰¹ç‚¹ï¼š** é›†æˆåˆ° pytest æ¡†æ¶ï¼Œè‡ªåŠ¨ä¸Šä¼ åˆ° OpenSearch

### 2. run_benchmark.shï¼ˆæ‰‹åŠ¨æµ‹è¯• - Dataset æ¨¡å¼ï¼‰
- **ä½ç½®ï¼š** `examples/disaggregated/slurm/benchmark/run_benchmark.sh`
- **ç”¨é€”ï¼š** æ‰‹åŠ¨è¿è¡Œ disagg æ€§èƒ½æµ‹è¯•ï¼Œä½¿ç”¨çœŸå® dataset
- **ç‰¹ç‚¹ï¼š** æ”¯æŒå¤šè½®æµ‹è¯•ï¼Œä¿å­˜è¯¦ç»†æ—¥å¿—

### 3. run_benchmark_nv_sa.shï¼ˆæ‰‹åŠ¨æµ‹è¯• - Random æ¨¡å¼ï¼‰
- **ä½ç½®ï¼š** `examples/disaggregated/slurm/benchmark/run_benchmark_nv_sa.sh`
- **ç”¨é€”ï¼š** æ‰‹åŠ¨è¿è¡Œ disagg æ€§èƒ½æµ‹è¯•ï¼Œä½¿ç”¨éšæœºæ•°æ®
- **ç‰¹ç‚¹ï¼š** å…‹éš†å¤–éƒ¨ bench_serving ä»“åº“ï¼Œä½¿ç”¨è‡ªå®šä¹‰è„šæœ¬

---

## ğŸ” å‘½ä»¤è¯¦ç»†å¯¹æ¯”

### test_perf_sanity.py ç”Ÿæˆçš„å‘½ä»¤

#### å®Œæ•´å‘½ä»¤ç¤ºä¾‹ï¼ˆBENCHMARK èŠ‚ç‚¹ï¼‰

```bash
python -m tensorrt_llm.serve.scripts.benchmark_serving \
    --model /data/DeepSeek-R1/DeepSeek-R1-FP4 \
    --tokenizer /data/DeepSeek-R1/DeepSeek-R1-FP4 \
    --dataset-name random \
    --random-ids \
    --num-prompts 768 \
    --max-concurrency 768 \
    --random-input-len 1024 \
    --random-output-len 1024 \
    --random-range-ratio 0.0 \
    --ignore-eos \
    --percentile-metrics ttft,tpot,itl,e2el \
    --dataset-path /data/datasets/ShareGPT_V3_unfiltered_cleaned_split.json \
    --backend openai \
    --use-chat-template \
    --trust-remote-code \
    --host <disagg_server_hostname> \
    --port <disagg_server_port>
```

#### å‚æ•°æ¥æºï¼ˆä» YAML é…ç½®ï¼‰

**YAML é…ç½®ç¤ºä¾‹ï¼š**

```yaml
# deepseek-r1-fp4_1k1k_ctx1_gen1_dep8_bs768_eplb0_mtp0_ccb-UCX.yaml

benchmark:
  concurrency: 768
  iterations: 1
  isl: 1024
  osl: 1024
  random_range_ratio: 0.0
  backend: openai
  use_chat_template: true
  streaming: true
  trust_remote_code: true
```

#### ä»£ç ç”Ÿæˆé€»è¾‘ï¼ˆtest_perf_sanity.py:388-430ï¼‰

```python
class ClientConfig:
    def to_cmd(self) -> List[str]:
        """Generate benchmark command."""
        # 1. è·å–æ¨¡å‹è·¯å¾„
        model_dir = get_model_dir(self.model_name)
        self.model_path = model_dir if os.path.exists(model_dir) else self.model_name
        
        # 2. è·å– dataset è·¯å¾„
        dataset_path = get_dataset_path()  
        # â†’ /data/datasets/ShareGPT_V3_unfiltered_cleaned_split.json
        
        # 3. æ„å»ºåŸºç¡€å‘½ä»¤
        benchmark_cmd = [
            "python",
            "-m",
            "tensorrt_llm.serve.scripts.benchmark_serving",
            "--model", self.model_path,
            "--tokenizer", self.model_path,
            "--dataset-name", "random",           # â† å›ºå®šä½¿ç”¨ random
            "--random-ids",                        # â† ç”Ÿæˆéšæœº token IDs
            "--num-prompts", str(self.concurrency * self.iterations),  # 768 * 1 = 768
            "--max-concurrency", str(self.concurrency),                 # 768
            "--random-input-len", str(self.isl),                        # 1024
            "--random-output-len", str(self.osl),                       # 1024
            "--random-range-ratio", str(self.random_range_ratio),       # 0.0
            "--ignore-eos",
            "--percentile-metrics", "ttft,tpot,itl,e2el",
        ]
        
        # 4. å¯é€‰ï¼šæ·»åŠ  dataset-pathï¼ˆå¦‚æœæ–‡ä»¶å­˜åœ¨ï¼‰
        if dataset_path and os.path.exists(dataset_path):
            benchmark_cmd.append("--dataset-path")
            benchmark_cmd.append(dataset_path)
        
        # 5. å¯é€‰ï¼šbackend
        if self.backend:  # openai
            benchmark_cmd.append("--backend")
            benchmark_cmd.append(self.backend)
        
        # 6. å¯é€‰ï¼šuse-chat-template
        if self.use_chat_template:  # true
            benchmark_cmd.append("--use-chat-template")
        
        # 7. å¯é€‰ï¼šstreaming
        if not self.streaming:  # streaming=true â†’ ä¸æ·»åŠ  --non-streaming
            benchmark_cmd.append("--non-streaming")
        
        # 8. å¯é€‰ï¼štrust-remote-code
        if self.trust_remote_code:  # true
            benchmark_cmd.append("--trust-remote-code")
        
        return benchmark_cmd
```

#### å…³é”®å‚æ•°è¯´æ˜

| å‚æ•° | å€¼ | è¯´æ˜ |
|------|-----|------|
| `--dataset-name` | `random` | **å›ºå®šä½¿ç”¨ random æ¨¡å¼** |
| `--random-ids` | - | ä½¿ç”¨éšæœºç”Ÿæˆçš„ token IDs |
| `--dataset-path` | `/data/datasets/ShareGPT_V3_unfiltered_cleaned_split.json` | â“ **è™½ç„¶è®¾ç½®äº†ï¼Œä½†å› ä¸º `--dataset-name=random`ï¼Œå®é™…ä¸ä½¿ç”¨** |
| `--num-prompts` | `768` | = concurrency Ã— iterations |
| `--max-concurrency` | `768` | å¹¶å‘æ•° |
| `--random-input-len` | `1024` | éšæœºè¾“å…¥é•¿åº¦ |
| `--random-output-len` | `1024` | éšæœºè¾“å‡ºé•¿åº¦ |
| `--random-range-ratio` | `0.0` | è¾“å…¥é•¿åº¦å˜åŒ–èŒƒå›´ï¼ˆ0 è¡¨ç¤ºå›ºå®šï¼‰ |
| `--backend` | `openai` | ä½¿ç”¨ OpenAI API æ ¼å¼ |
| `--use-chat-template` | - | ä½¿ç”¨å¯¹è¯æ¨¡æ¿ |
| `--trust-remote-code` | - | ä¿¡ä»»è¿œç¨‹ä»£ç  |

#### â“ ç–‘é—®ï¼šdataset-path çš„ä½œç”¨

**è™½ç„¶ä»£ç æ·»åŠ äº† `--dataset-path`ï¼Œä½†å®é™…ä¸ä¼šä½¿ç”¨ï¼š**

```python
# benchmark_serving.py çš„é€»è¾‘ï¼ˆç®€åŒ–ï¼‰
if args.dataset_name == "random":
    # ä½¿ç”¨éšæœºæ•°æ®ç”Ÿæˆ
    prompts = generate_random_prompts(
        num_prompts=args.num_prompts,
        input_len=args.random_input_len,
        output_len=args.random_output_len,
        random_ids=args.random_ids
    )
    # âŒ ä¸ä¼šè¯»å– dataset_path
elif args.dataset_name == "trtllm_custom":
    # âœ… è¯»å– dataset_path
    prompts = load_dataset(args.dataset_path)
```

**ç»“è®ºï¼š**
- âœ… `test_perf_sanity.py` ä½¿ç”¨ **random æ¨¡å¼**
- âŒ `--dataset-path` è™½ç„¶ä¼ é€’äº†ï¼Œä½†**ä¸ä¼šè¢«ä½¿ç”¨**
- âœ… æ•°æ®å®Œå…¨éšæœºç”Ÿæˆ

---

### run_benchmark.sh ç”Ÿæˆçš„å‘½ä»¤

#### å®Œæ•´å‘½ä»¤ç¤ºä¾‹

```bash
# Warmup (å¦‚æœ ucx_warmup_requests > 0)
python -m tensorrt_llm.serve.scripts.benchmark_serving \
    --model DeepSeek-R1 \
    --dataset-name random \
    --random-ids \
    --random-input-len 100 \
    --random-output-len 10 \
    --num-prompts 10 \
    --host node001 \
    --port 8000 \
    --ignore-eos \
    --non-streaming

# å®é™… Benchmark
for concurrency in ${concurrency_list}; do
    python -m tensorrt_llm.serve.scripts.benchmark_serving \
        --model DeepSeek-R1 \
        --backend openai \
        --host node001 \
        --port 8000 \
        --dataset-name trtllm_custom \
        --dataset-path /data/ShareGPT_V3_unfiltered_cleaned_split.json \
        --num-prompts 768 \
        --max-concurrency 768 \
        --trust-remote-code \
        --ignore-eos \
        --no-test-input \
        --save-result \
        --result-dir /logs/concurrency_768 \
        --result-filename result.json \
        --percentile-metrics ttft,tpot,itl,e2el \
        # --non-streaming (å¦‚æœ streaming=false)
done
```

#### å‚æ•°è¯´æ˜

| å‚æ•° | å€¼ | è¯´æ˜ |
|------|-----|------|
| `--dataset-name` | **`trtllm_custom`** | âœ… **ä½¿ç”¨çœŸå® dataset** |
| `--dataset-path` | `/data/ShareGPT_V3_...` | âœ… **å®é™…è¯»å–æ–‡ä»¶** |
| `--num-prompts` | `768` | = concurrency Ã— multi_round |
| `--max-concurrency` | `768` | å¹¶å‘æ•° |
| `--save-result` | - | ä¿å­˜ç»“æœåˆ°æ–‡ä»¶ |
| `--result-dir` | `/logs/concurrency_768` | ç»“æœç›®å½• |
| `--result-filename` | `result.json` | ç»“æœæ–‡ä»¶å |
| `--no-test-input` | - | ä¸æµ‹è¯•è¾“å…¥æœ‰æ•ˆæ€§ |
| `--backend` | `openai` | OpenAI API æ ¼å¼ |

#### å…³é”®å·®å¼‚

1. âœ… **ä½¿ç”¨çœŸå® dataset**ï¼ˆ`trtllm_custom`ï¼‰
2. âœ… **ä¿å­˜è¯¦ç»†ç»“æœ**åˆ° JSON æ–‡ä»¶
3. âœ… **æ”¯æŒ UCX warmup**ï¼ˆé¢„çƒ­ UCX è¿æ¥ï¼‰
4. âœ… **å¤„ç† CTX/GEN æ—¥å¿—**ï¼ˆæå– ctx-only å’Œ gen-only è¯·æ±‚ï¼‰

---

### run_benchmark_nv_sa.sh ç”Ÿæˆçš„å‘½ä»¤

#### å®Œæ•´å‘½ä»¤ç¤ºä¾‹

```bash
# 1. å…‹éš†å¤–éƒ¨ benchmark ä»“åº“
git clone https://github.com/kedarpotdar-nv/bench_serving.git /tmp/bench_serving

# 2. Warmup (å¦‚æœ ucx_warmup_requests > 0)
python -m tensorrt_llm.serve.scripts.benchmark_serving \
    --model DeepSeek-R1 \
    --dataset-name random \
    --random-ids \
    --random-input-len 100 \
    --random-output-len 10 \
    --num-prompts 10 \
    --host node001 \
    --port 8000 \
    --ignore-eos \
    --non-streaming

# 3. å®é™… Benchmarkï¼ˆä½¿ç”¨å¤–éƒ¨è„šæœ¬ï¼‰
for concurrency in ${concurrency_list}; do
    python /tmp/bench_serving/benchmark_serving.py \
        --model DeepSeek-R1 \
        --host node001 \
        --port 8000 \
        --dataset-name random \
        --num-prompts 768 \
        --max-concurrency 768 \
        --trust-remote-code \
        --ignore-eos \
        --random-input-len 1024 \
        --random-output-len 1024 \
        --random-range-ratio 0.0 \
        --save-result \
        --use-chat-template \
        --result-dir /logs/concurrency_768 \
        --result-filename result.json \
        --percentile-metrics ttft,tpot,itl,e2el \
        # --non-streaming (å¦‚æœ streaming=false)
done
```

#### å‚æ•°è¯´æ˜

| å‚æ•° | å€¼ | è¯´æ˜ |
|------|-----|------|
| `--dataset-name` | **`random`** | âœ… **ä½¿ç”¨éšæœºæ•°æ®** |
| `--random-input-len` | `1024` | éšæœºè¾“å…¥é•¿åº¦ |
| `--random-output-len` | `1024` | éšæœºè¾“å‡ºé•¿åº¦ |
| `--random-range-ratio` | `0.0` | è¾“å…¥é•¿åº¦å˜åŒ–èŒƒå›´ |
| `--use-chat-template` | - | ä½¿ç”¨å¯¹è¯æ¨¡æ¿ |
| `--save-result` | - | ä¿å­˜ç»“æœ |

#### å…³é”®å·®å¼‚

1. âœ… **ä½¿ç”¨å¤–éƒ¨ benchmark_serving.py**ï¼ˆä» GitHub å…‹éš†ï¼‰
2. âœ… **ä½¿ç”¨ random æ¨¡å¼**ï¼ˆç±»ä¼¼ test_perf_sanity.pyï¼‰
3. âœ… **æ”¯æŒ UCX warmup**
4. âœ… **å¤„ç† CTX/GEN æ—¥å¿—**

---

## ğŸ“Š ä¸‰ç§å‘½ä»¤å¯¹æ¯”æ€»ç»“

### æ ¸å¿ƒå·®å¼‚è¡¨

| ç»´åº¦ | test_perf_sanity.py | run_benchmark.sh | run_benchmark_nv_sa.sh |
|------|---------------------|------------------|------------------------|
| **æ•°æ®æº** | âœ… Random | âœ… **çœŸå® Dataset** | âœ… Random |
| **dataset-name** | `random` | `trtllm_custom` | `random` |
| **dataset-path** | âŒ ä¼ é€’ä½†ä¸ä½¿ç”¨ | âœ… å®é™…ä½¿ç”¨ | âŒ ä¸ä¼ é€’ |
| **random-ids** | âœ… æ˜¯ | âŒ å¦ | âŒ å¦ |
| **UCX Warmup** | âŒ å¦ | âœ… æ˜¯ | âœ… æ˜¯ |
| **ä¿å­˜ç»“æœ** | âŒ å¦ï¼ˆå†…å­˜å¤„ç†ï¼‰ | âœ… JSON æ–‡ä»¶ | âœ… JSON æ–‡ä»¶ |
| **æ—¥å¿—å¤„ç†** | âŒ å¦ | âœ… æå– ctx/gen only | âœ… æå– ctx/gen only |
| **Benchmark è„šæœ¬** | å†…ç½® | å†…ç½® | å¤–éƒ¨å…‹éš† |
| **é€‚ç”¨åœºæ™¯** | CI/CD è‡ªåŠ¨åŒ– | æ‰‹åŠ¨æµ‹è¯•ï¼ˆçœŸå®æ•°æ®ï¼‰ | æ‰‹åŠ¨æµ‹è¯•ï¼ˆéšæœºæ•°æ®ï¼‰ |
| **é›†æˆæ–¹å¼** | pytest æ¡†æ¶ | ç‹¬ç«‹ shell è„šæœ¬ | ç‹¬ç«‹ shell è„šæœ¬ |

---

## ğŸ¯ å…³é”®é—®é¢˜è§£ç­”

### Q1: test_perf_sanity.py çš„ BENCHMARK éœ€è¦ dataset file å—ï¼Ÿ

**ç­”æ¡ˆï¼šè™½ç„¶ä¼ é€’äº† `--dataset-path`ï¼Œä½†å®é™…ä¸ä½¿ç”¨ã€‚**

**åŸå› ï¼š**

```python
# test_perf_sanity.py (388-430 è¡Œ)
benchmark_cmd = [
    # ...
    "--dataset-name", "random",  # â† å›ºå®šä½¿ç”¨ random æ¨¡å¼
    "--random-ids",               # â† ç”Ÿæˆéšæœº token IDs
    # ...
]

# è™½ç„¶æ·»åŠ äº† dataset-path
if dataset_path and os.path.exists(dataset_path):
    benchmark_cmd.append("--dataset-path")
    benchmark_cmd.append(dataset_path)

# ä½† benchmark_serving.py çš„é€»è¾‘æ˜¯ï¼š
# if dataset_name == "random":
#     ä½¿ç”¨éšæœºæ•°æ®ï¼Œä¸è¯»å– dataset_path
```

**ç»“è®ºï¼š**
- âŒ **ä¸éœ€è¦** dataset file
- âœ… æ•°æ®å®Œå…¨éšæœºç”Ÿæˆ
- âœ… å¦‚æœ dataset file ä¸å­˜åœ¨ï¼Œä¹Ÿä¸ä¼šæŠ¥é”™

---

### Q2: ä¸ºä»€ä¹ˆ test_perf_sanity.py è¦ä¼ é€’ dataset-pathï¼Ÿ

**å¯èƒ½çš„åŸå› ï¼š**

1. **å‘åå…¼å®¹**ï¼šå†å²ä»£ç å¯èƒ½æ›¾ä½¿ç”¨ `trtllm_custom` æ¨¡å¼
2. **è°ƒè¯•æ–¹ä¾¿**ï¼šå¦‚æœéœ€è¦åˆ‡æ¢åˆ°çœŸå®æ•°æ®ï¼Œåªéœ€ä¿®æ”¹ `--dataset-name`
3. **ä»£ç å¤ç”¨**ï¼š`ClientConfig` ç±»å¯èƒ½åŒæ—¶ç”¨äºå…¶ä»–æµ‹è¯•

**å®é™…æ•ˆæœï¼š**
- âœ… ä¸å½±å“æµ‹è¯•è¿è¡Œ
- âœ… å‚æ•°ä¼šè¢«å¿½ç•¥

---

### Q3: ä¸‰ç§å‘½ä»¤çš„æ€§èƒ½æµ‹è¯•ç»“æœæœ‰å·®å¼‚å—ï¼Ÿ

**æœ‰æ˜¾è‘—å·®å¼‚ï¼**

| ç»´åº¦ | Random æ¨¡å¼ | Dataset æ¨¡å¼ |
|------|-------------|--------------|
| **è¾“å…¥é•¿åº¦** | å›ºå®šæˆ–éšæœºèŒƒå›´ | çœŸå®åˆ†å¸ƒ |
| **è¾“å‡ºé•¿åº¦** | å›ºå®šæˆ–éšæœºèŒƒå›´ | çœŸå®åˆ†å¸ƒ |
| **Token åˆ†å¸ƒ** | å‡åŒ€éšæœº | çœŸå®æ–‡æœ¬ |
| **æ€§èƒ½ç»“æœ** | ç†æƒ³æƒ…å†µ | çœŸå®åœºæ™¯ |
| **å¯é‡å¤æ€§** | âœ… é«˜ | âŒ ä½ï¼ˆä¾èµ– datasetï¼‰ |

**å»ºè®®ï¼š**
- âœ… **CI/CD è‡ªåŠ¨åŒ–**ï¼šä½¿ç”¨ random æ¨¡å¼ï¼ˆtest_perf_sanity.pyï¼‰
- âœ… **çœŸå®åœºæ™¯æµ‹è¯•**ï¼šä½¿ç”¨ dataset æ¨¡å¼ï¼ˆrun_benchmark.shï¼‰
- âœ… **æ€§èƒ½å¯¹æ¯”**ï¼šä¸¤ç§æ¨¡å¼éƒ½è¿è¡Œ

---

## ğŸ“ å®Œæ•´å‘½ä»¤å¯¹æ¯”ï¼ˆå¹¶æ’ï¼‰

### test_perf_sanity.py

```bash
python -m tensorrt_llm.serve.scripts.benchmark_serving \
    --model /data/DeepSeek-R1/DeepSeek-R1-FP4 \
    --tokenizer /data/DeepSeek-R1/DeepSeek-R1-FP4 \
    --dataset-name random \                          # â† Random æ¨¡å¼
    --random-ids \                                   # â† éšæœº token IDs
    --num-prompts 768 \
    --max-concurrency 768 \
    --random-input-len 1024 \                        # â† å›ºå®šè¾“å…¥é•¿åº¦
    --random-output-len 1024 \                       # â† å›ºå®šè¾“å‡ºé•¿åº¦
    --random-range-ratio 0.0 \                       # â† æ— å˜åŒ–
    --ignore-eos \
    --percentile-metrics ttft,tpot,itl,e2el \
    --dataset-path /data/datasets/ShareGPT_V3_...json \  # â† ä¸ä½¿ç”¨
    --backend openai \
    --use-chat-template \
    --trust-remote-code \
    --host <disagg_server_hostname> \
    --port <disagg_server_port>
```

### run_benchmark.sh

```bash
python -m tensorrt_llm.serve.scripts.benchmark_serving \
    --model DeepSeek-R1 \
    --backend openai \
    --host node001 \
    --port 8000 \
    --dataset-name trtllm_custom \                   # â† Dataset æ¨¡å¼
    --dataset-path /data/ShareGPT_V3_...json \       # â† å®é™…ä½¿ç”¨
    --num-prompts 768 \
    --max-concurrency 768 \
    --trust-remote-code \
    --ignore-eos \
    --no-test-input \                                # â† é¢å¤–å‚æ•°
    --save-result \                                  # â† ä¿å­˜ç»“æœ
    --result-dir /logs/concurrency_768 \             # â† ç»“æœç›®å½•
    --result-filename result.json \                  # â† ç»“æœæ–‡ä»¶
    --percentile-metrics ttft,tpot,itl,e2el
```

### run_benchmark_nv_sa.sh

```bash
python /tmp/bench_serving/benchmark_serving.py \    # â† å¤–éƒ¨è„šæœ¬
    --model DeepSeek-R1 \
    --host node001 \
    --port 8000 \
    --dataset-name random \                          # â† Random æ¨¡å¼
    --num-prompts 768 \
    --max-concurrency 768 \
    --trust-remote-code \
    --ignore-eos \
    --random-input-len 1024 \                        # â† éšæœºè¾“å…¥é•¿åº¦
    --random-output-len 1024 \                       # â† éšæœºè¾“å‡ºé•¿åº¦
    --random-range-ratio 0.0 \                       # â† æ— å˜åŒ–
    --save-result \                                  # â† ä¿å­˜ç»“æœ
    --use-chat-template \                            # â† å¯¹è¯æ¨¡æ¿
    --result-dir /logs/concurrency_768 \             # â† ç»“æœç›®å½•
    --result-filename result.json \                  # â† ç»“æœæ–‡ä»¶
    --percentile-metrics ttft,tpot,itl,e2el
```

---

## ğŸ”§ å¦‚ä½•åˆ‡æ¢æ¨¡å¼ï¼Ÿ

### ä¿®æ”¹ test_perf_sanity.py ä½¿ç”¨çœŸå® Dataset

**æ–¹æ³• 1: ä¿®æ”¹ä»£ç ï¼ˆä¸æ¨èï¼‰**

```python
# test_perf_sanity.py (388-430 è¡Œ)
def to_cmd(self) -> List[str]:
    # ...
    benchmark_cmd = [
        # ...
        "--dataset-name", "trtllm_custom",  # â† æ”¹ä¸º trtllm_custom
        # "--random-ids",                   # â† åˆ é™¤è¿™è¡Œ
        # ...
    ]
```

**æ–¹æ³• 2: é€šè¿‡ YAML é…ç½®ï¼ˆæ¨èï¼‰**

åœ¨ YAML ä¸­æ·»åŠ  `dataset_mode` å­—æ®µï¼š

```yaml
benchmark:
  dataset_mode: "trtllm_custom"  # â† æ–°å¢å­—æ®µ
  concurrency: 768
  iterations: 1
  isl: 1024
  osl: 1024
```

ç„¶åä¿®æ”¹ `ClientConfig` ç±»æ”¯æŒè¿™ä¸ªå­—æ®µï¼š

```python
class ClientConfig:
    def __init__(self, client_config_data: dict, model_name: str, env_vars: str = ""):
        # ...
        self.dataset_mode = client_config_data.get("dataset_mode", "random")
    
    def to_cmd(self) -> List[str]:
        # ...
        benchmark_cmd = [
            # ...
            "--dataset-name", self.dataset_mode,  # â† ä½¿ç”¨é…ç½®å€¼
        ]
        
        if self.dataset_mode == "random":
            benchmark_cmd.append("--random-ids")
            benchmark_cmd.extend([
                "--random-input-len", str(self.isl),
                "--random-output-len", str(self.osl),
                "--random-range-ratio", str(self.random_range_ratio),
            ])
        
        # dataset-path åœ¨ä¸¤ç§æ¨¡å¼ä¸‹éƒ½å¯ä»¥æ·»åŠ 
        if dataset_path and os.path.exists(dataset_path):
            benchmark_cmd.append("--dataset-path")
            benchmark_cmd.append(dataset_path)
```

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

1. **test_perf_sanity.py**: `tests/integration/defs/perf/test_perf_sanity.py`
2. **run_benchmark.sh**: `examples/disaggregated/slurm/benchmark/run_benchmark.sh`
3. **run_benchmark_nv_sa.sh**: `examples/disaggregated/slurm/benchmark/run_benchmark_nv_sa.sh`
4. **benchmark_serving.py**: `tensorrt_llm/serve/scripts/benchmark_serving.py`ï¼ˆå†…ç½®ï¼‰
5. **å¤–éƒ¨ bench_serving**: https://github.com/kedarpotdar-nv/bench_serving.git

---

## âœ… æ€»ç»“

### æ ¸å¿ƒè¦ç‚¹

1. **test_perf_sanity.py çš„ BENCHMARK**
   - âœ… ä½¿ç”¨ **random æ¨¡å¼**
   - âŒ **ä¸éœ€è¦** dataset fileï¼ˆè™½ç„¶ä¼ é€’äº†è·¯å¾„ï¼‰
   - âœ… æ•°æ®å®Œå…¨éšæœºç”Ÿæˆ
   - âœ… é€‚åˆ CI/CD è‡ªåŠ¨åŒ–

2. **run_benchmark.sh**
   - âœ… ä½¿ç”¨ **çœŸå® dataset**ï¼ˆ`trtllm_custom`ï¼‰
   - âœ… éœ€è¦ dataset file
   - âœ… ä¿å­˜è¯¦ç»†ç»“æœ
   - âœ… é€‚åˆçœŸå®åœºæ™¯æµ‹è¯•

3. **run_benchmark_nv_sa.sh**
   - âœ… ä½¿ç”¨ **random æ¨¡å¼**
   - âŒ ä¸éœ€è¦ dataset file
   - âœ… ä½¿ç”¨å¤–éƒ¨ benchmark è„šæœ¬
   - âœ… é€‚åˆå¿«é€Ÿæµ‹è¯•

### æ¨èä½¿ç”¨åœºæ™¯

| åœºæ™¯ | æ¨èæ–¹æ¡ˆ | åŸå›  |
|------|---------|------|
| **CI/CD è‡ªåŠ¨åŒ–** | test_perf_sanity.py | ç¨³å®šã€å¯é‡å¤ã€å¿«é€Ÿ |
| **çœŸå®åœºæ™¯è¯„ä¼°** | run_benchmark.sh | çœŸå®æ•°æ®åˆ†å¸ƒ |
| **å¿«é€ŸéªŒè¯** | run_benchmark_nv_sa.sh | ç®€å•ã€å¿«é€Ÿ |
| **æ€§èƒ½å›å½’æµ‹è¯•** | test_perf_sanity.py | é›†æˆ OpenSearch |
| **å¯¹å¤–æ¼”ç¤º** | run_benchmark.sh | çœŸå®åœºæ™¯ |

---

**ç°åœ¨ä½ å®Œå…¨æ¸…æ¥šä¸‰ç§ benchmark å‘½ä»¤çš„åŒºåˆ«äº†å—ï¼Ÿ** ğŸš€
