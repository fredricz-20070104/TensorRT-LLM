# æ€§èƒ½æµ‹è¯•æ‰§è¡Œæµç¨‹è¯¦è§£

## ğŸ“‹ ç›®å½•

1. [æ ¸å¿ƒç»“è®º](#æ ¸å¿ƒç»“è®º)
2. [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
3. [TestList ç®¡ç†æ–¹æ¡ˆ](#testlist-ç®¡ç†æ–¹æ¡ˆ)
4. [âš ï¸ å¾…ä¿®å¤çš„è®¾è®¡é—®é¢˜](#å¾…ä¿®å¤çš„è®¾è®¡é—®é¢˜)
5. [å…³é”®é…ç½®è¯´æ˜](#å…³é”®é…ç½®è¯´æ˜)
6. [æµ‹è¯•æ–‡ä»¶è¯´æ˜](#æµ‹è¯•æ–‡ä»¶è¯´æ˜)
7. [æ‰§è¡Œæµç¨‹è¯¦è§£](#æ‰§è¡Œæµç¨‹è¯¦è§£)

---

## å¿«é€Ÿå¼€å§‹

### ğŸš€ æ¨èæ–¹å¼ï¼šä½¿ç”¨ç»Ÿä¸€ TestList

```groovy
// Jenkins Pipeline å‚æ•°
TESTLIST: gb200_unified_suite  // ä¸€ä¸ªæ–‡ä»¶åŒ…å«æ‰€æœ‰ç±»å‹çš„æµ‹è¯•ï¼
CLUSTER: gb200
```

```bash
# æœ¬åœ°è°ƒè¯•ï¼šè¿è¡Œæ•´ä¸ªå¥—ä»¶
./scripts/run_perf_tests.sh \
    --testlist testlists/gb200_unified_suite.yml \
    --trtllm-dir /path/to/TensorRT-LLM

# åªè¿è¡Œ single-agg æµ‹è¯•
./scripts/run_perf_tests.sh \
    --testlist testlists/gb200_unified_suite.yml \
    --trtllm-dir /path/to/TensorRT-LLM \
    --mode single-agg

# è¯•è¿è¡Œ
./scripts/run_perf_tests.sh \
    --testlist testlists/gb200_unified_suite.yml \
    --trtllm-dir /path/to/TensorRT-LLM \
    --dry-run
```

### ä½¿ç”¨ TestList è¿è¡Œæµ‹è¯•ï¼ˆå…¼å®¹æ–¹å¼ï¼‰

```groovy
// Jenkins Pipeline å‚æ•°
TESTLIST: single_agg/gb200_perf_sanity  // é€‰æ‹©é¢„å®šä¹‰çš„ testlist
CLUSTER: gb200                           // é€‰æ‹©ç›®æ ‡é›†ç¾¤
```

### æ‰‹åŠ¨è¿è¡Œå•ä¸ªé…ç½®ï¼ˆè°ƒè¯•ï¼‰

```groovy
// Jenkins Pipeline å‚æ•°
TESTLIST: manual                                    // é€‰æ‹©æ‰‹åŠ¨æ¨¡å¼
MANUAL_TEST_MODE: single-agg                        // æŒ‡å®šæµ‹è¯•æ¨¡å¼
CONFIG_FILE: deepseek_r1_fp4_v2_grace_blackwell     // æŒ‡å®šé…ç½®æ–‡ä»¶
CLUSTER: gb200                                      // é€‰æ‹©ç›®æ ‡é›†ç¾¤
```

### æœ¬åœ°è°ƒè¯•ï¼ˆè„±ç¦» Jenkinsï¼‰

```bash
# æ–¹å¼1: ä½¿ç”¨ testlist
cd /path/to/jenkins_test
./scripts/run_single_agg_test.sh \
    --testlist testlists/single_agg/gb200_perf_sanity.yml \
    --trtllm-dir /path/to/TensorRT-LLM

# æ–¹å¼2: ç›´æ¥æŒ‡å®šé…ç½®æ–‡ä»¶
./scripts/run_single_agg_test.sh \
    --config-file deepseek_r1_fp4_v2_grace_blackwell \
    --trtllm-dir /path/to/TensorRT-LLM

# è¯•è¿è¡Œæ¨¡å¼ï¼ˆæŸ¥çœ‹å°†æ‰§è¡Œçš„å‘½ä»¤ï¼‰
./scripts/run_single_agg_test.sh \
    --testlist testlists/single_agg/gb200_perf_sanity.yml \
    --trtllm-dir /path/to/TensorRT-LLM \
    --dry-run
```

---

## TestList ç®¡ç†æ–¹æ¡ˆ

### ğŸ¯ æ–¹æ¡ˆé€‰æ‹©

æˆ‘ä»¬æä¾›ä¸¤ç§æ–¹æ¡ˆï¼Œæ ¹æ®éœ€æ±‚é€‰æ‹©ï¼š

#### æ–¹æ¡ˆ A: ç»Ÿä¸€ TestListï¼ˆæ¨è - All-in-Oneï¼‰

**é€‚ç”¨åœºæ™¯**ï¼š
- âœ… éœ€è¦ç®¡ç†å¤§é‡æµ‹è¯•ç”¨ä¾‹
- âœ… å¸Œæœ›ä¸€ä¸ªæ–‡ä»¶ç®¡ç†æ‰€æœ‰ç±»å‹çš„æµ‹è¯•
- âœ… éœ€è¦è‡ªåŠ¨è¯†åˆ«æµ‹è¯•ç±»å‹

**ç‰¹ç‚¹**ï¼š
- ä¸€ä¸ª YAML æ–‡ä»¶åŒ…å« single-aggã€multi-aggã€disagg æ‰€æœ‰æµ‹è¯•
- è‡ªåŠ¨è¯†åˆ«æµ‹è¯•ç±»å‹ï¼ˆæ ¹æ®èŠ‚ç‚¹æ•°å’Œæ ‡è®°ï¼‰
- ç»Ÿä¸€æ‰§è¡Œå…¥å£ `run_perf_tests.sh`

#### æ–¹æ¡ˆ B: åˆ†ç±» TestListï¼ˆå…¼å®¹ test-dbï¼‰

**é€‚ç”¨åœºæ™¯**ï¼š
- âœ… éœ€è¦å®Œå…¨å…¼å®¹ç°æœ‰ test-db æ ¼å¼
- âœ… å¸Œæœ›æŒ‰æµ‹è¯•ç±»å‹åˆ†ç›®å½•ç®¡ç†
- âœ… éœ€è¦ç‹¬ç«‹è¿è¡ŒæŸç§ç±»å‹çš„æµ‹è¯•

**ç‰¹ç‚¹**ï¼š
- æŒ‰ç›®å½•åˆ†ç±»ï¼š`single_agg/`ã€`multi_agg/`ã€`disagg/`
- å®Œå…¨å…¼å®¹ test-db æ ¼å¼
- ç‹¬ç«‹çš„æ‰§è¡Œè„šæœ¬

---

### æ–¹æ¡ˆ A: ç»Ÿä¸€ TestList æ ¼å¼

#### æ–‡ä»¶ç»“æ„

```
jenkins_test/
â”œâ”€â”€ testlists/
â”‚   â”œâ”€â”€ gb200_unified_suite.yml     # ç»Ÿä¸€æµ‹è¯•å¥—ä»¶
â”‚   â”œâ”€â”€ gb300_unified_suite.yml
â”‚   â””â”€â”€ b200_unified_suite.yml
â”‚
â”œâ”€â”€ configs/                         # é…ç½®æ–‡ä»¶ï¼ˆå…±äº«ï¼‰
â”‚   â”œâ”€â”€ single_agg/
â”‚   â”œâ”€â”€ multi_agg/
â”‚   â””â”€â”€ disagg/
â”‚
â””â”€â”€ scripts/
    â”œâ”€â”€ run_perf_tests.sh            # ç»Ÿä¸€æ‰§è¡Œå…¥å£
    â”œâ”€â”€ parse_unified_testlist.py    # ç»Ÿä¸€è§£æå™¨
    â”œâ”€â”€ run_single_agg_test.sh       # è¢«è°ƒç”¨
    â”œâ”€â”€ run_multi_agg_test.sh        # è¢«è°ƒç”¨
    â””â”€â”€ run_disagg_test.sh           # è¢«è°ƒç”¨
```

#### ç»Ÿä¸€ TestList æ ¼å¼

```yaml
# jenkins_test/testlists/gb200_unified_suite.yml
version: 1.0.0

metadata:
  description: "GB200 ç»Ÿä¸€æ€§èƒ½æµ‹è¯•å¥—ä»¶"
  cluster: gb200
  owner: perf-team

gb200_unified_perf_suite:
# ========================================
# Single Node Agg æµ‹è¯•
# ========================================
- condition:
    ranges:
      system_gpu_count:
        gte: 4
        lte: 4
    wildcards:
      gpu:
      - '*gb200*'
    terms:
      stage: pre_merge
      backend: pytorch
      nodes: 1                    # â† è‡ªåŠ¨è¯†åˆ«ï¼š1èŠ‚ç‚¹ = single-agg
  tests:
  - perf/test_perf_sanity.py::test_e2e[aggr_upload-deepseek_r1_fp4_v2_grace_blackwell-r1_fp4_v2_tp4_mtp3_1k1k]
  - perf/test_perf_sanity.py::test_e2e[aggr_upload-deepseek_r1_fp4_v2_grace_blackwell-r1_fp4_v2_dep4_mtp1_1k1k] TIMEOUT (90)

# ========================================
# Multi Node Agg æµ‹è¯•
# ========================================
- condition:
    ranges:
      system_gpu_count:
        gte: 8
        lte: 8
    terms:
      stage: post_merge
      backend: pytorch
      nodes: 2                    # â† è‡ªåŠ¨è¯†åˆ«ï¼š2èŠ‚ç‚¹ = multi-agg
  tests:
  - perf/test_perf_sanity.py::test_e2e[aggr_upload-deepseek_r1_fp4_v2_2_nodes_grace_blackwell-r1_fp4_v2_dep8_mtp1_1k1k] TIMEOUT (90)

# ========================================
# Disagg æµ‹è¯•
# ========================================
- condition:
    ranges:
      system_gpu_count:
        gte: 12
        lte: 12
    terms:
      stage: post_merge
      backend: pytorch
      nodes: 3                    # â† èŠ‚ç‚¹æ•°
      test_type: disagg           # â† æ˜ç¡®æ ‡è¯†ï¼šdisagg
  tests:
  - perf/test_perf_sanity.py::test_e2e[disagg_upload-deepseek-r1-fp4_1k1k_ctx1_gen1_dep8_bs768_eplb0_mtp0_ccb-UCX] TIMEOUT (90)
```

#### è‡ªåŠ¨è¯†åˆ«è§„åˆ™

```python
# è¯†åˆ«ä¼˜å…ˆçº§ï¼š
1. condition.terms.test_type == "disagg" â†’ disagg
2. test_line åŒ…å« "disagg_upload"       â†’ disagg
3. condition.terms.nodes == 1           â†’ single-agg
4. condition.terms.nodes > 1            â†’ multi-agg
```

#### ä½¿ç”¨ç»Ÿä¸€ TestList

```bash
# è¿è¡Œæ•´ä¸ªå¥—ä»¶
./scripts/run_perf_tests.sh \
    --testlist testlists/gb200_unified_suite.yml \
    --trtllm-dir /path/to/TensorRT-LLM

# åªè¿è¡Œ single-agg æµ‹è¯•
./scripts/run_perf_tests.sh \
    --testlist testlists/gb200_unified_suite.yml \
    --trtllm-dir /path/to/TensorRT-LLM \
    --mode single-agg

# åªè¿è¡Œ multi-agg æµ‹è¯•
./scripts/run_perf_tests.sh \
    --testlist testlists/gb200_unified_suite.yml \
    --trtllm-dir /path/to/TensorRT-LLM \
    --mode multi-agg

# é‡åˆ°é”™è¯¯å°±åœæ­¢
./scripts/run_perf_tests.sh \
    --testlist testlists/gb200_unified_suite.yml \
    --trtllm-dir /path/to/TensorRT-LLM \
    --stop-on-error

# è¯•è¿è¡Œ
./scripts/run_perf_tests.sh \
    --testlist testlists/gb200_unified_suite.yml \
    --trtllm-dir /path/to/TensorRT-LLM \
    --dry-run
```

#### æŸ¥çœ‹ TestList å†…å®¹

```bash
# æŸ¥çœ‹ç»Ÿè®¡æ‘˜è¦
python3 scripts/parse_unified_testlist.py \
    testlists/gb200_unified_suite.yml \
    --summary

# è¾“å‡º:
# ============================================================
# TestList: gb200_unified_perf_suite
# ============================================================
# æ€»æµ‹è¯•æ•°: 10
#   - Single-Agg: 6
#   - Multi-Agg:  3
#   - Disagg:     1
# ============================================================

# æŸ¥çœ‹è¯¦ç»†ä¿¡æ¯ï¼ˆç¾åŒ–è¾“å‡ºï¼‰
python3 scripts/parse_unified_testlist.py \
    testlists/gb200_unified_suite.yml \
    --pretty

# åªæŸ¥çœ‹ single-agg æµ‹è¯•
python3 scripts/parse_unified_testlist.py \
    testlists/gb200_unified_suite.yml \
    --mode single-agg \
    --pretty
```

---

### æ–¹æ¡ˆ B: åˆ†ç±» TestList æ ¼å¼ï¼ˆå…¼å®¹æ–¹å¼ï¼‰

```
jenkins_test/
â”œâ”€â”€ testlists/                           # TestList æ–‡ä»¶ï¼ˆtest-db æ ¼å¼ï¼‰
â”‚   â”œâ”€â”€ single_agg/
â”‚   â”‚   â”œâ”€â”€ gb200_perf_sanity.yml       # GB200 å•èŠ‚ç‚¹æ€§èƒ½æµ‹è¯•
â”‚   â”‚   â””â”€â”€ gb300_perf_sanity.yml       # GB300 å•èŠ‚ç‚¹æ€§èƒ½æµ‹è¯•
â”‚   â”œâ”€â”€ multi_agg/
â”‚   â”‚   â””â”€â”€ gb200_2nodes_perf.yml       # GB200 åŒèŠ‚ç‚¹èšåˆæµ‹è¯•
â”‚   â””â”€â”€ disagg/
â”‚       â””â”€â”€ gb200_3nodes_sanity.yml     # GB200 3èŠ‚ç‚¹åˆ†ç¦»å¼æµ‹è¯•
â”‚
â”œâ”€â”€ configs/                             # é…ç½®æ–‡ä»¶ï¼ˆæŒ‰æµ‹è¯•æ¨¡å¼åˆ†ç±»ï¼‰
â”‚   â”œâ”€â”€ single_agg/
â”‚   â”‚   â”œâ”€â”€ deepseek_r1_fp4_v2_grace_blackwell.yml
â”‚   â”‚   â”œâ”€â”€ deepseek_v32_fp4_grace_blackwell.yml
â”‚   â”‚   â””â”€â”€ k2_thinking_fp4_grace_blackwell.yml
â”‚   â”œâ”€â”€ multi_agg/
â”‚   â”‚   â””â”€â”€ deepseek_r1_fp4_v2_2_nodes_grace_blackwell.yml
â”‚   â””â”€â”€ disagg/
â”‚       â””â”€â”€ deepseek-r1-fp4_1k1k_ctx1_gen1_dep8_bs768_eplb0_mtp0_ccb-UCX.yaml
â”‚
â””â”€â”€ scripts/
    â”œâ”€â”€ parse_testlist.py                # TestList è§£æå·¥å…·
    â”œâ”€â”€ run_single_agg_test.sh           # å•èŠ‚ç‚¹æµ‹è¯•è„šæœ¬
    â”œâ”€â”€ run_multi_agg_test.sh            # å¤šèŠ‚ç‚¹èšåˆæµ‹è¯•è„šæœ¬
    â””â”€â”€ run_disagg_test.sh               # åˆ†ç¦»å¼æµ‹è¯•è„šæœ¬
```

### TestList æ ¼å¼ï¼ˆå®Œå…¨å…¼å®¹ test-dbï¼‰

```yaml
# jenkins_test/testlists/single_agg/gb200_perf_sanity.yml
version: 0.0.1
gb200_single_agg_perf_sanity:
- condition:
    ranges:
      system_gpu_count:
        gte: 4
        lte: 4
    wildcards:
      gpu:
      - '*gb200*'
      linux_distribution_name: ubuntu*
      cpu: aarch64
    terms:
      stage: pre_merge
      backend: pytorch
  tests:
  # DeepSeek-R1 FP4 é…ç½®
  - perf/test_perf_sanity.py::test_e2e[aggr_upload-deepseek_r1_fp4_v2_grace_blackwell-r1_fp4_v2_tp4_mtp3_1k1k]
  - perf/test_perf_sanity.py::test_e2e[aggr_upload-deepseek_r1_fp4_v2_grace_blackwell-r1_fp4_v2_dep4_mtp1_1k1k] TIMEOUT (90)
  
  # DeepSeek-V32 FP4 é…ç½®
  - perf/test_perf_sanity.py::test_e2e[aggr_upload-deepseek_v32_fp4_grace_blackwell-v32_fp4_tep4_mtp3_1k1k]
```

### é…ç½®åç§°æ˜ å°„è§„åˆ™

```
æµ‹è¯•åç§°æ ¼å¼ï¼š
  test_e2e[aggr_upload-{config_file}-{config_name}]

æ˜ å°„è§„åˆ™ï¼š
  {config_file} â†’ jenkins_test/configs/{test_mode}/{config_file}.yml
  {config_name} â†’ é…ç½®æ–‡ä»¶ä¸­çš„ server_configs[name={config_name}]

ç¤ºä¾‹ï¼š
  aggr_upload-deepseek_r1_fp4_v2_grace_blackwell-r1_fp4_v2_tp4_mtp3_1k1k
  â†“
  é…ç½®æ–‡ä»¶: jenkins_test/configs/single_agg/deepseek_r1_fp4_v2_grace_blackwell.yml
  é…ç½®é¡¹:   server_configs ä¸­ name="r1_fp4_v2_tp4_mtp3_1k1k" çš„é…ç½®
```

### é…ç½®æ–‡ä»¶æ ¼å¼ï¼ˆä¿æŒç°æœ‰æ ¼å¼ï¼‰

```yaml
# jenkins_test/configs/single_agg/deepseek_r1_fp4_v2_grace_blackwell.yml
metadata:
  model_name: deepseek_r1_0528_fp4_v2
  supported_gpus:
  - GB200

hardware:
  gpus_per_node: 4

server_configs:
  - name: "r1_fp4_v2_tp4_mtp3_1k1k"
    model_name: "deepseek_r1_0528_fp4_v2"
    tensor_parallel_size: 4
    moe_expert_parallel_size: 1
    pipeline_parallel_size: 1
    max_batch_size: 4
    max_num_tokens: 8192
    attn_backend: "TRTLLM"
    # ... å®Œæ•´é…ç½®

  - name: "r1_fp4_v2_dep4_mtp1_1k1k"
    model_name: "deepseek_r1_0528_fp4_v2"
    tensor_parallel_size: 4
    moe_expert_parallel_size: 4
    # ... å®Œæ•´é…ç½®
```

### å®Œæ•´æ‰§è¡Œæµç¨‹

#### æ–¹å¼1: ä½¿ç”¨ TestListï¼ˆæ¨èï¼‰

```
ç”¨æˆ·æ“ä½œï¼š
1. åœ¨ Jenkins é€‰æ‹© TESTLIST: single_agg/gb200_perf_sanity
2. é€‰æ‹© CLUSTER: gb200
3. ç‚¹å‡»æ„å»º

Pipeline è‡ªåŠ¨æ‰§è¡Œï¼š
1. å‚æ•°éªŒè¯å’Œæ¨¡å¼è¯†åˆ«
   â”œâ”€ è¯†åˆ« test_mode = single-agg
   â”œâ”€ è®¾ç½® TESTLIST_FILE = jenkins_test/testlists/single_agg/gb200_perf_sanity.yml
   â””â”€ è®¾ç½® USE_TESTLIST = true

2. å‡†å¤‡å·¥ä½œç¯å¢ƒ
   â”œâ”€ å…‹éš†/æ›´æ–° TensorRT-LLM
   â””â”€ éªŒè¯ä¾èµ–æ–‡ä»¶

3. åŠ è½½é›†ç¾¤é…ç½®
   â”œâ”€ ä» jenkins_test/config/clusters.conf åŠ è½½ gb200 é…ç½®
   â””â”€ è®¾ç½®ç¯å¢ƒå˜é‡

4. è¿è¡Œæµ‹è¯•
   â”œâ”€ è°ƒç”¨ run_single_agg_test.sh --testlist testlists/single_agg/gb200_perf_sanity.yml
   â”œâ”€ è„šæœ¬ä½¿ç”¨ parse_testlist.py è§£æ testlist
   â”œâ”€ æå–æµ‹è¯•åˆ—è¡¨:
   â”‚   [
   â”‚     {config_file: "deepseek_r1_fp4_v2_grace_blackwell", config_name: "r1_fp4_v2_tp4_mtp3_1k1k", timeout: 7200},
   â”‚     {config_file: "deepseek_r1_fp4_v2_grace_blackwell", config_name: "r1_fp4_v2_dep4_mtp1_1k1k", timeout: 5400},
   â”‚     ...
   â”‚   ]
   â”œâ”€ å¯¹æ¯ä¸ªæµ‹è¯•:
   â”‚   â”œâ”€ æŸ¥æ‰¾é…ç½®æ–‡ä»¶: jenkins_test/configs/single_agg/deepseek_r1_fp4_v2_grace_blackwell.yml
   â”‚   â”œâ”€ æ„é€  pytest å‘½ä»¤:
   â”‚   â”‚   pytest tests/integration/defs/perf/test_perf_sanity.py::test_e2e \
   â”‚   â”‚     -k 'aggr_upload-deepseek_r1_fp4_v2_grace_blackwell and r1_fp4_v2_tp4_mtp3_1k1k'
   â”‚   â”œâ”€ ä½¿ç”¨ srun åœ¨é›†ç¾¤ä¸Šè¿è¡Œ
   â”‚   â””â”€ æ”¶é›†ç»“æœ
   â””â”€ è¾“å‡ºæ€»ç»“
```

#### æ–¹å¼2: æ‰‹åŠ¨æ¨¡å¼ï¼ˆè°ƒè¯•ï¼‰

```
ç”¨æˆ·æ“ä½œï¼š
1. åœ¨ Jenkins é€‰æ‹© TESTLIST: manual
2. é€‰æ‹© MANUAL_TEST_MODE: single-agg
3. è¾“å…¥ CONFIG_FILE: deepseek_r1_fp4_v2_grace_blackwell
4. é€‰æ‹© CLUSTER: gb200
5. ç‚¹å‡»æ„å»º

Pipeline è‡ªåŠ¨æ‰§è¡Œï¼š
1. å‚æ•°éªŒè¯å’Œæ¨¡å¼è¯†åˆ«
   â”œâ”€ è¯†åˆ«è¿è¡Œæ¨¡å¼ = æ‰‹åŠ¨
   â”œâ”€ è®¾ç½® test_mode = single-agg
   â””â”€ è®¾ç½® USE_TESTLIST = false

2-3. å‡†å¤‡ç¯å¢ƒå’ŒåŠ è½½é…ç½®ï¼ˆåŒæ–¹å¼1ï¼‰

4. è¿è¡Œæµ‹è¯•
   â”œâ”€ è°ƒç”¨ run_single_agg_test.sh --config-file deepseek_r1_fp4_v2_grace_blackwell
   â”œâ”€ è„šæœ¬ç›´æ¥æŸ¥æ‰¾é…ç½®æ–‡ä»¶
   â”œâ”€ è¿è¡Œè¯¥é…ç½®æ–‡ä»¶ä¸­çš„æ‰€æœ‰ server_configs
   â””â”€ è¾“å‡ºç»“æœ
```

#### æ–¹å¼3: æœ¬åœ°è°ƒè¯•ï¼ˆè„±ç¦» Jenkinsï¼‰

```bash
# è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆæ¨¡æ‹Ÿé›†ç¾¤é…ç½®ï¼‰
export CLUSTER_ACCOUNT="coreai_comparch_trtllm"
export CLUSTER_PARTITION="batch"
export CLUSTER_LLM_DATA="/lustre/fs1/portfolios/coreai/projects/coreai_comparch_trtllm/common"
export DOCKER_IMAGE="nvcr.io/nvidia/tensorrt-llm:latest"
export MPI_TYPE="pmix"

# ä½¿ç”¨ testlist
cd /path/to/jenkins_test
./scripts/run_single_agg_test.sh \
    --testlist testlists/single_agg/gb200_perf_sanity.yml \
    --trtllm-dir /path/to/TensorRT-LLM

# æˆ–ç›´æ¥æŒ‡å®šé…ç½®æ–‡ä»¶
./scripts/run_single_agg_test.sh \
    --config-file deepseek_r1_fp4_v2_grace_blackwell \
    --trtllm-dir /path/to/TensorRT-LLM

# è¯•è¿è¡Œï¼ˆæŸ¥çœ‹å°†æ‰§è¡Œçš„å‘½ä»¤ï¼Œä¸å®é™…è¿è¡Œï¼‰
./scripts/run_single_agg_test.sh \
    --testlist testlists/single_agg/gb200_perf_sanity.yml \
    --trtllm-dir /path/to/TensorRT-LLM \
    --dry-run
```

### è°ƒè¯•æŠ€å·§

#### 1. æŸ¥çœ‹ TestList è§£æç»“æœ

```bash
# è§£æ testlist å¹¶æŸ¥çœ‹ JSON è¾“å‡º
python3 scripts/parse_testlist.py \
    testlists/single_agg/gb200_perf_sanity.yml \
    --pretty

# è¾“å‡ºç¤ºä¾‹:
{
  "testlist_name": "gb200_single_agg_perf_sanity",
  "test_mode": "single-agg",
  "tests": [
    {
      "test_type": "aggr",
      "config_file": "deepseek_r1_fp4_v2_grace_blackwell",
      "config_name": "r1_fp4_v2_tp4_mtp3_1k1k",
      "timeout": 7200,
      "raw": "perf/test_perf_sanity.py::test_e2e[aggr_upload-...]"
    },
    ...
  ]
}
```

#### 2. å•ç‹¬æµ‹è¯•é…ç½®æ–‡ä»¶æŸ¥æ‰¾é€»è¾‘

```bash
# æµ‹è¯•é…ç½®æ–‡ä»¶æŸ¥æ‰¾
config_file="deepseek_r1_fp4_v2_grace_blackwell"

for path in \
    "jenkins_test/configs/single_agg/${config_file}.yaml" \
    "jenkins_test/configs/single_agg/${config_file}.yml" \
    "TensorRT-LLM/tests/scripts/perf-sanity/${config_file}.yaml"; do
    if [[ -f "$path" ]]; then
        echo "æ‰¾åˆ°: $path"
    fi
done
```

#### 3. éªŒè¯ pytest å‘½ä»¤

```bash
# ä½¿ç”¨ --dry-run æŸ¥çœ‹å°†æ‰§è¡Œçš„ pytest å‘½ä»¤
./scripts/run_single_agg_test.sh \
    --config-file deepseek_r1_fp4_v2_grace_blackwell \
    --trtllm-dir /path/to/TensorRT-LLM \
    --dry-run

# è¾“å‡º:
# æ‰§è¡Œå‘½ä»¤:
#   cd /path/to/TensorRT-LLM
#   srun --mpi=pmix -N 1 -A coreai_comparch_trtllm -p batch \
#     --container-image=nvcr.io/nvidia/tensorrt-llm:latest \
#     --container-workdir=/path/to/TensorRT-LLM \
#     python3 -m pytest tests/integration/defs/perf/test_perf_sanity.py::test_e2e \
#       -k 'aggr_upload-deepseek_r1_fp4_v2_grace_blackwell' -v --timeout=7200
```

#### 4. æ‰‹åŠ¨è¿è¡Œå•ä¸ªæµ‹è¯•

```bash
# åœ¨é›†ç¾¤ä¸Šæ‰‹åŠ¨è¿è¡Œï¼ˆè·³è¿‡è„šæœ¬ï¼‰
cd /path/to/TensorRT-LLM

srun --mpi=pmix -N 1 -A coreai_comparch_trtllm -p batch \
  --container-image=nvcr.io/nvidia/tensorrt-llm:latest \
  --container-workdir=$(pwd) \
  --container-mounts=$(pwd):$(pwd),/lustre/fs1/portfolios/coreai/projects/coreai_comparch_trtllm/common:/lustre/fs1/portfolios/coreai/projects/coreai_comparch_trtllm/common \
  python3 -m pytest \
    tests/integration/defs/perf/test_perf_sanity.py::test_e2e \
    -k 'aggr_upload-deepseek_r1_fp4_v2_grace_blackwell and r1_fp4_v2_tp4_mtp3_1k1k' \
    -v
```

### æ·»åŠ æ–°æµ‹è¯•

#### æ·»åŠ åˆ°ç°æœ‰ TestList

```yaml
# ç¼–è¾‘ jenkins_test/testlists/single_agg/gb200_perf_sanity.yml
tests:
  # æ·»åŠ æ–°çš„æµ‹è¯•è¡Œ
  - perf/test_perf_sanity.py::test_e2e[aggr_upload-my_new_model-my_config_name] TIMEOUT (90)
```

#### åˆ›å»ºæ–°çš„é…ç½®æ–‡ä»¶

```yaml
# åˆ›å»º jenkins_test/configs/single_agg/my_new_model.yml
metadata:
  model_name: my_new_model
  supported_gpus:
  - GB200

server_configs:
  - name: "my_config_name"
    model_name: "my_new_model"
    tensor_parallel_size: 4
    # ... å®Œæ•´é…ç½®
```

#### åˆ›å»ºæ–°çš„ TestList

```yaml
# åˆ›å»º jenkins_test/testlists/single_agg/my_custom_suite.yml
version: 0.0.1
my_custom_test_suite:
- condition:
    ranges:
      system_gpu_count:
        gte: 4
        lte: 4
    wildcards:
      gpu:
      - '*gb200*'
    terms:
      stage: pre_merge
      backend: pytorch
  tests:
  - perf/test_perf_sanity.py::test_e2e[aggr_upload-my_new_model-my_config_name]
```

ç„¶ååœ¨ `Perf_Test.groovy` çš„ TESTLIST å‚æ•°ä¸­æ·»åŠ ï¼š

```groovy
choice(
    name: 'TESTLIST',
    choices: [
        'single_agg/gb200_perf_sanity',
        'single_agg/my_custom_suite',  // æ·»åŠ æ–°çš„é€‰é¡¹
        ...
    ]
)
```

### ä¼˜åŠ¿æ€»ç»“

âœ… **å…¼å®¹ç°æœ‰** - TestList æ ¼å¼ä¸ test-db å®Œå…¨å…¼å®¹  
âœ… **ç»Ÿä¸€ç®¡ç†** - æ‰€æœ‰é…ç½®åœ¨ `jenkins_test/` ä¸‹é›†ä¸­ç®¡ç†  
âœ… **æ˜“äºè°ƒè¯•** - æ”¯æŒæœ¬åœ°è¿è¡Œå’Œ dry-run æ¨¡å¼  
âœ… **æ‰¹é‡æ‰§è¡Œ** - ä¸€ä¸ª testlist ç®¡ç†å¤šä¸ªæµ‹è¯•  
âœ… **çµæ´»åˆ‡æ¢** - å¯ä»¥åœ¨ testlist æ¨¡å¼å’Œæ‰‹åŠ¨æ¨¡å¼é—´åˆ‡æ¢  
âœ… **æ¸…æ™°å±‚æ¬¡** - testlist (æµ‹ä»€ä¹ˆ) â†’ config (æ€ä¹ˆé…ç½®)  

---

## âš ï¸ å¾…ä¿®å¤çš„è®¾è®¡é—®é¢˜

### é—®é¢˜ï¼šPerf_Test.groovy çš„ NODE_LIST å‚æ•°è®¾è®¡ä¸åˆç†

**å½“å‰é”™è¯¯å®ç°**:
```groovy
// Perf_Test.groovy å½“å‰å‚æ•°
string(name: 'NODE_LIST', defaultValue: '', description: 'èŠ‚ç‚¹åˆ—è¡¨')

// ç”¨æˆ·éœ€è¦æ‰‹åŠ¨æŒ‡å®šï¼š
NODE_LIST: node1,node2,node3,node4

// ç„¶åéªŒè¯ (ç¬¬ 244-257 è¡Œ):
def providedNodes = NODE_LIST.split(',').size()  // 4
if (providedNodes != nodeInfo.total_nodes) {
    error "èŠ‚ç‚¹æ•°ä¸åŒ¹é…ï¼"
}
```

**ä¸ºä»€ä¹ˆè¿™æ˜¯é”™è¯¯çš„**:

1. âŒ **Slurm è‡ªåŠ¨åˆ†é…èŠ‚ç‚¹**
   - Slurm æ ¹æ®èµ„æºå¯ç”¨æ€§åŠ¨æ€åˆ†é…èŠ‚ç‚¹
   - ç”¨æˆ·æ— æ³•é¢„çŸ¥ä¼šåˆ†é…å“ªäº›èŠ‚ç‚¹
   - èŠ‚ç‚¹åç§°å¯èƒ½æ˜¯ `gpu-node-[05-08]` è€Œä¸æ˜¯ `node1,node2,node3,node4`

2. âŒ **submit.py å’Œ slurm_launch_draft.sh ä¸ä½¿ç”¨èŠ‚ç‚¹åç§°**
   - å®ƒä»¬é€šè¿‡ `srun -N <count>` æŒ‡å®šèŠ‚ç‚¹æ•°é‡
   - Slurm è‡ªåŠ¨åœ¨å·²åˆ†é…çš„èŠ‚ç‚¹æ± ä¸­é€‰æ‹©
   - ä¸éœ€è¦ä¹Ÿä¸ä½¿ç”¨ç”¨æˆ·æä¾›çš„èŠ‚ç‚¹åç§°åˆ—è¡¨

3. âŒ **é™åˆ¶è°ƒåº¦çµæ´»æ€§**
   - æ‰‹åŠ¨æŒ‡å®šèŠ‚ç‚¹å¯èƒ½å¯¼è‡´è¿™äº›èŠ‚ç‚¹ä¸å¯ç”¨
   - Slurm åº”è¯¥æœ‰è‡ªç”±é€‰æ‹©æœ€ä¼˜èŠ‚ç‚¹çš„èƒ½åŠ›

**æ­£ç¡®çš„å®ç°æ–¹å¼** (å‚è€ƒ L0_Test.groovy):

```groovy
// âœ… æ­£ç¡®çš„å‚æ•°å®šä¹‰
string(name: 'NODE_COUNT', defaultValue: '4', description: 'éœ€è¦çš„èŠ‚ç‚¹æ•°é‡')

// âœ… ç”Ÿæˆ SBATCH å‚æ•° (L0_Test.groovy ç¬¬ 783-798 è¡Œ)
def getNodeArgs(int nodeCount, int gpuCount, boolean setSegment = false) {
    int gpusPerNode = ((gpuCount / nodeCount) as BigDecimal).setScale(0, BigDecimal.ROUND_CEILING).intValue()
    return [
        "--nodes=${nodeCount}",          // â† åªæŒ‡å®šæ•°é‡
        "--ntasks=${gpuCount}",
        "--ntasks-per-node=${gpusPerNode}",
        "--gpus-per-node=${gpusPerNode}",
    ]
}

// âœ… åœ¨ sbatch è„šæœ¬ä¸­ (L0_Test.groovy ç¬¬ 1163-1168 è¡Œ)
#!/bin/bash
#SBATCH --nodes=4                        // â† å‘Šè¯‰ Slurm éœ€è¦ 4 ä¸ªèŠ‚ç‚¹
#SBATCH --ntasks=32
#SBATCH --ntasks-per-node=8
#SBATCH --gpus-per-node=8

echo "Starting Slurm job $SLURM_JOB_ID on $SLURM_NODELIST"  // â† è¿è¡Œæ—¶è·å–å®é™…èŠ‚ç‚¹

// âœ… éªŒè¯é€»è¾‘åº”è¯¥æ”¹ä¸º
if (params.NODE_COUNT != nodeInfo.total_nodes) {
    error """
èŠ‚ç‚¹æ•°ä¸åŒ¹é…ï¼
  é…ç½®è¦æ±‚: ${nodeInfo.total_nodes} ä¸ªèŠ‚ç‚¹
  ç”¨æˆ·æŒ‡å®š: ${params.NODE_COUNT} ä¸ªèŠ‚ç‚¹
"""
}
```

**Slurm èŠ‚ç‚¹åˆ†é…çš„å®é™…æµç¨‹**:

```bash
# æ­¥éª¤ 1: æäº¤ä½œä¸šï¼ŒåªæŒ‡å®šéœ€è¦çš„èŠ‚ç‚¹æ•°é‡
$ sbatch --nodes=4 my_job.sh
Submitted batch job 12345

# æ­¥éª¤ 2: Slurm è‡ªåŠ¨é€‰æ‹© 4 ä¸ªå¯ç”¨èŠ‚ç‚¹
# å‡è®¾é€‰ä¸­äº†: gpu-node-05, gpu-node-06, gpu-node-07, gpu-node-08

# æ­¥éª¤ 3: ä½œä¸šè¿è¡Œæ—¶ï¼Œé€šè¿‡ç¯å¢ƒå˜é‡è·å–å®é™…åˆ†é…çš„èŠ‚ç‚¹
$ echo $SLURM_NODELIST
gpu-node-[05-08]

$ echo $SLURM_JOB_NUM_NODES
4

# æ­¥éª¤ 4: ä½¿ç”¨ srun åœ¨å·²åˆ†é…çš„èŠ‚ç‚¹ä¸­æ‰§è¡Œä»»åŠ¡
$ srun -N 2 hostname     # ä» 4 ä¸ªèŠ‚ç‚¹ä¸­é€‰ 2 ä¸ª
gpu-node-05
gpu-node-06

$ srun -N 2 hostname     # å¯èƒ½é€‰æ‹©å¦å¤– 2 ä¸ª
gpu-node-07
gpu-node-08
```

**submit.py çš„å®é™…è¡Œä¸º**:

æŸ¥çœ‹ `jenkins/scripts/perf/disaggregated/slurm_launch_draft.sh` (ç¬¬ 19-31 è¡Œ):

```bash
# å¯åŠ¨ gen servers
for i in $(seq 0 $((numGenServers - 1))); do
    gen_world_size=$((nodesPerGenServer * gpusPerNode))
    export DISAGG_SERVING_TYPE="GEN_$i"
    export pytestCommand="$pytestCommandWorker"
    
    # âœ… åªæŒ‡å®šéœ€è¦çš„èŠ‚ç‚¹æ•°é‡ï¼Œä¸æŒ‡å®šå…·ä½“èŠ‚ç‚¹åç§°
    srun "${srunArgs[@]}" --kill-on-bad-exit=1 \
        -N $nodesPerGenServer \              # â† ä¾‹å¦‚: -N 2 (éœ€è¦2ä¸ªèŠ‚ç‚¹)
        --ntasks=$gen_world_size \
        --ntasks-per-node=$gpusPerNode \
        $runScript &> $jobWorkspace/gen_server_$i.log &
done

# Slurm ä¼šè‡ªåŠ¨ä» $SLURM_NODELIST ä¸­é€‰æ‹© 2 ä¸ªèŠ‚ç‚¹æ¥è¿è¡Œè¿™ä¸ªå‘½ä»¤
# ç”¨æˆ·æä¾›çš„ NODE_LIST å®Œå…¨æ²¡æœ‰è¢«ä½¿ç”¨ï¼
```

**ä¿®å¤å»ºè®®**:

1. **ä¿®æ”¹å‚æ•°å®šä¹‰** (Perf_Test.groovy ç¬¬ 7 è¡Œ):
   ```groovy
   // åˆ é™¤
   - string(name: 'NODE_LIST', defaultValue: '', description: 'èŠ‚ç‚¹åˆ—è¡¨')
   
   // æ·»åŠ 
   + string(name: 'NODE_COUNT', defaultValue: '', description: 'èŠ‚ç‚¹æ•°é‡ï¼ˆå¯é€‰ï¼Œdisaggæ¨¡å¼ä¼šè‡ªåŠ¨è®¡ç®—ï¼‰')
   ```

2. **ä¿®æ”¹éªŒè¯é€»è¾‘** (Perf_Test.groovy ç¬¬ 244-257 è¡Œ):
   ```groovy
   // åˆ é™¤
   - if (NODE_LIST) {
   -     def providedNodes = NODE_LIST.split(',').size()
   -     if (providedNodes != nodeInfo.total_nodes) {
   
   // æ·»åŠ 
   + if (params.NODE_COUNT) {
   +     def requestedNodes = params.NODE_COUNT.toInteger()
   +     if (requestedNodes != nodeInfo.total_nodes) {
           error """
   èŠ‚ç‚¹æ•°ä¸åŒ¹é…ï¼
     é…ç½®è¦æ±‚: ${nodeInfo.total_nodes} ä¸ªèŠ‚ç‚¹
   - å®é™…æä¾›: ${providedNodes} ä¸ªèŠ‚ç‚¹
   + ç”¨æˆ·æŒ‡å®š: ${requestedNodes} ä¸ªèŠ‚ç‚¹
   """
       }
       echo "âœ“ èŠ‚ç‚¹æ•°éªŒè¯é€šè¿‡"
   }
   ```

3. **åœ¨ sbatch å‘½ä»¤ä¸­ä½¿ç”¨** (éœ€è¦æ–°å¢):
   ```groovy
   // åœ¨ç”Ÿæˆ sbatch è„šæœ¬æ—¶æ·»åŠ 
   def sbatchScript = """#!/bin/bash
   #SBATCH --nodes=${nodeInfo.total_nodes}
   #SBATCH --ntasks=${nodeInfo.total_gpus}
   #SBATCH --ntasks-per-node=${nodeInfo.gpus_per_node}
   ...
   """
   ```

**æ€»ç»“**:

| ç»´åº¦ | å½“å‰é”™è¯¯å®ç° | æ­£ç¡®å®ç° (L0_Test.groovy) |
|------|-------------|--------------------------|
| **å‚æ•°ç±»å‹** | `NODE_LIST` (å­—ç¬¦ä¸²åˆ—è¡¨) | `NODE_COUNT` (æ•´æ•°) |
| **ç”¨æˆ·è¾“å…¥** | `node1,node2,node3,node4` | `4` |
| **éªŒè¯æ–¹å¼** | `NODE_LIST.split(',').size()` | `params.NODE_COUNT.toInteger()` |
| **sbatch å‚æ•°** | æœªä½¿ç”¨ | `--nodes=4` |
| **èŠ‚ç‚¹åˆ†é…** | å‡è£…ç”¨æˆ·çŸ¥é“èŠ‚ç‚¹åç§° | Slurm è‡ªåŠ¨åˆ†é… |
| **submit.py ä½¿ç”¨** | å®Œå…¨ä¸ä½¿ç”¨ NODE_LIST | ä½¿ç”¨ total_nodes æ•°é‡ |

---

## ğŸ¯ æ ¸å¿ƒç»“è®º

### æ‰€æœ‰æ€§èƒ½æµ‹è¯•ç»Ÿä¸€ä½¿ç”¨ test_perf_sanity.py

**é‡è¦å‘ç°**ï¼š
- âœ… **Single Node Agg** â†’ `test_perf_sanity.py::test_e2e`
- âœ… **Multi-Node Agg** â†’ `test_perf_sanity.py::test_e2e`
- âœ… **Multi-Node Disagg** â†’ `test_perf_sanity.py::test_e2e`
- âŒ **test_perf.py** â†’ æ—§æ–‡ä»¶ï¼Œå·²ä¸å†ç”¨äº perf sanity æµ‹è¯•

### å…³é”®é…ç½®è¯´æ˜

**Q1: `disagg_run_type` çš„é»˜è®¤å€¼æ˜¯ä»€ä¹ˆï¼Ÿ**

âœ… **ç­”æ¡ˆ**: é»˜è®¤å€¼æ˜¯ `"aggr"`

**è¯æ®**ï¼ˆ`test_perf_sanity.py` ç¬¬ 129 è¡Œï¼‰:
```python
self.disagg_run_type = server_config_data.get("disagg_run_type", "aggr")
                                                                   ^^^^^^
                                                                   é»˜è®¤å€¼
```

**è¯´æ˜**:
- å¦‚æœ `server_config` ä¸­æ²¡æœ‰ `disagg_run_type` å­—æ®µï¼Œé»˜è®¤ä¸º `"aggr"`
- Agg é…ç½®æ–‡ä»¶é€šå¸¸ä¸éœ€è¦æ˜¾å¼æŒ‡å®šï¼ˆå¯ä»¥çœç•¥ï¼‰
- Disagg é…ç½®æ–‡ä»¶æ ¹æœ¬ä¸ä½¿ç”¨ `server_config`ï¼Œè€Œæ˜¯ä½¿ç”¨ `hardware` + `worker_config`

**Q2: `jenkins/scripts/perf/disaggregated/submit.py` æ˜¯å¦åšäº†é€»è¾‘èŠ‚ç‚¹åˆ°ç¡¬ä»¶èŠ‚ç‚¹çš„è½¬æ¢ï¼Ÿ**

âœ… **ç­”æ¡ˆ**: æ˜¯çš„ï¼submit.py ç¡®å®åšäº†å®Œæ•´çš„è½¬æ¢

**è¯æ®**ï¼ˆ`submit.py` ç¬¬ 8-54 è¡Œï¼‰:

```python
def get_hardware_config(config, benchmark_mode):
    hardware = config.get("hardware", {})
    worker_config = config.get("worker_config", {})

    # 1. è¯»å–é€»è¾‘æœåŠ¡å™¨æ•°
    num_ctx_servers = hardware.get("num_ctx_servers")  # é€»è¾‘
    num_gen_servers = hardware.get("num_gen_servers")  # é€»è¾‘
    gpus_per_node = hardware.get("gpus_per_node")
    
    # 2. è®¡ç®—æ¯ä¸ªé€»è¾‘æœåŠ¡å™¨éœ€è¦çš„ GPU æ•°
    ctx_tp = ctx_config.get("tensor_parallel_size", 1)
    ctx_pp = ctx_config.get("pipeline_parallel_size", 1)
    ctx_cp = ctx_config.get("context_parallel_size", 1)
    gpus_per_ctx_server = ctx_tp * ctx_pp * ctx_cp  # æ¯ä¸ª CTX æœåŠ¡å™¨çš„ GPU æ•°
    
    gen_tp = gen_config.get("tensor_parallel_size", 1)
    gen_pp = gen_config.get("pipeline_parallel_size", 1)
    gen_cp = gen_config.get("context_parallel_size", 1)
    gpus_per_gen_server = gen_tp * gen_pp * gen_cp  # æ¯ä¸ª GEN æœåŠ¡å™¨çš„ GPU æ•°
    
    # 3. è®¡ç®—æ¯ä¸ªé€»è¾‘æœåŠ¡å™¨éœ€è¦çš„ç¡¬ä»¶èŠ‚ç‚¹æ•°ï¼ˆå‘ä¸Šå–æ•´ï¼‰
    nodes_per_ctx_server = (gpus_per_ctx_server + gpus_per_node - 1) // gpus_per_node
    nodes_per_gen_server = (gpus_per_gen_server + gpus_per_node - 1) // gpus_per_node
    
    # 4. è®¡ç®—æ€»ç¡¬ä»¶èŠ‚ç‚¹æ•°
    total_nodes = num_ctx_servers * nodes_per_ctx_server + num_gen_servers * nodes_per_gen_server
    total_gpus = total_nodes * gpus_per_node
    
    return {
        "num_ctx_servers": num_ctx_servers,           # é€»è¾‘
        "num_gen_servers": num_gen_servers,           # é€»è¾‘
        "nodes_per_ctx_server": nodes_per_ctx_server, # ç¡¬ä»¶
        "nodes_per_gen_server": nodes_per_gen_server, # ç¡¬ä»¶
        "total_nodes": total_nodes,                   # ç¡¬ä»¶
        "total_gpus": total_gpus,
    }
```

**è®¡ç®—ç¤ºä¾‹**:

å‡è®¾é…ç½®ä¸ºï¼š
```yaml
hardware:
  num_ctx_servers: 1    # 1 ä¸ªé€»è¾‘ CTX æœåŠ¡å™¨
  num_gen_servers: 1    # 1 ä¸ªé€»è¾‘ GEN æœåŠ¡å™¨
  gpus_per_node: 4
worker_config:
  ctx:
    tensor_parallel_size: 4  # CTX TP=4
  gen:
    tensor_parallel_size: 8  # GEN TP=8
```

è®¡ç®—è¿‡ç¨‹ï¼š
```python
# CTX è®¡ç®—
gpus_per_ctx_server = 4 Ã— 1 Ã— 1 = 4
nodes_per_ctx_server = (4 + 4 - 1) // 4 = 1  # æ¯ä¸ª CTX é€»è¾‘æœåŠ¡å™¨éœ€è¦ 1 ä¸ªç¡¬ä»¶èŠ‚ç‚¹
ctx_total_nodes = 1 Ã— 1 = 1                   # 1 ä¸ªé€»è¾‘æœåŠ¡å™¨ Ã— 1 èŠ‚ç‚¹/æœåŠ¡å™¨ = 1 ä¸ªç¡¬ä»¶èŠ‚ç‚¹

# GEN è®¡ç®—
gpus_per_gen_server = 8 Ã— 1 Ã— 1 = 8
nodes_per_gen_server = (8 + 4 - 1) // 4 = 2  # æ¯ä¸ª GEN é€»è¾‘æœåŠ¡å™¨éœ€è¦ 2 ä¸ªç¡¬ä»¶èŠ‚ç‚¹
gen_total_nodes = 1 Ã— 2 = 2                   # 1 ä¸ªé€»è¾‘æœåŠ¡å™¨ Ã— 2 èŠ‚ç‚¹/æœåŠ¡å™¨ = 2 ä¸ªç¡¬ä»¶èŠ‚ç‚¹

# æ€»è®¡
total_nodes = 1 + 2 = 3  # 3 ä¸ªç¡¬ä»¶èŠ‚ç‚¹
total_gpus = 3 Ã— 4 = 12  # 12 ä¸ª GPU
```

**å…³é”®åŒºåˆ«**:
- `jenkins_test/scripts/calculate_hardware_nodes.py` - æˆ‘ä»¬è‡ªå·±å†™çš„å·¥å…·ï¼Œç”¨äºéªŒè¯
- `jenkins/scripts/perf/disaggregated/submit.py` - L0 çš„è„šæœ¬ï¼Œ**ä¹Ÿåšäº†ç›¸åŒçš„è®¡ç®—**

ä¸¤è€…è®¡ç®—é€»è¾‘å®Œå…¨ä¸€è‡´ï¼

---

## ğŸ“Š æµ‹è¯•æ–‡ä»¶å¯¹æ¯”

### test_perf_sanity.py (å½“å‰ä½¿ç”¨)

**ä½ç½®**: `tests/integration/defs/perf/test_perf_sanity.py`

**ç”¨é€”**: æ‰€æœ‰ Perf Sanity æµ‹è¯•

**æµ‹è¯•å‡½æ•°**: `test_e2e`

**æ”¯æŒçš„æµ‹è¯•ç±»å‹**:
- Aggregated (å•èŠ‚ç‚¹/å¤šèŠ‚ç‚¹)
- Disaggregated (å¤šèŠ‚ç‚¹)

**é…ç½®æ ¼å¼**:
```yaml
# Agg é…ç½®
server_config:
  model_name: deepseek_r1_fp4
  tensor_parallel_size: 4
  disagg_run_type: aggr  # â† å…³é”®ï¼šaggr è¡¨ç¤ºèšåˆæ¨¡å¼

# Disagg é…ç½®
hardware:
  num_ctx_servers: 2  # é€»è¾‘ CTX æœåŠ¡å™¨æ•°
  num_gen_servers: 1  # é€»è¾‘ GEN æœåŠ¡å™¨æ•°
  gpus_per_node: 4
worker_config:
  ctx:
    tensor_parallel_size: 4
  gen:
    tensor_parallel_size: 8
```

### test_perf.py (å·²å¼ƒç”¨)

**ä½ç½®**: `tests/integration/defs/perf/test_perf.py`

**ç”¨é€”**: æ—§çš„æ€§èƒ½æµ‹è¯•æ¡†æ¶

**æµ‹è¯•å‡½æ•°**: `test_perf`

**çŠ¶æ€**: âš ï¸ å·²ä¸å†ç”¨äº Perf Sanity æµ‹è¯•ï¼Œåªåœ¨å®Œæ•´çš„ perf æµ‹è¯•ä¸­ä½¿ç”¨

---

## ğŸ” L0_Test.groovy æ‰§è¡Œæµç¨‹

### é…ç½®å®šä¹‰ï¼ˆç¬¬ 3349-3367 è¡Œï¼‰

```groovy
multiNodesSBSAConfigs = [
    // Multi-Node Agg: 8 GPUs, 2 Nodes
    "GB200-8_GPUs-2_Nodes-PyTorch-PerfSanity-Post-Merge-1": [
        "gb200-oci-trtllm",                              // å¹³å°
        "l0_gb200_multi_nodes_aggr_perf_sanity_2_nodes", // TestList
        1,  // splitId
        5,  // splits
        8,  // gpuCount (æ€» GPU æ•°)
        2   // nodeCount (ç¡¬ä»¶èŠ‚ç‚¹æ•°)
    ],
    
    // Multi-Node Disagg: 12 GPUs, 3 Nodes
    "GB200-12_GPUs-3_Nodes-PyTorch-PerfSanity-Disagg-Post-Merge-1": [
        "gb200-oci-trtllm",
        "l0_gb200_multi_nodes_disagg_perf_sanity_3_nodes", // TestList
        1,  // splitId
        1,  // splits
        12, // gpuCount (æ€» GPU æ•°)
        3   // nodeCount (ç¡¬ä»¶èŠ‚ç‚¹æ•°)
    ],
]
```

### æ‰§è¡Œæµç¨‹ï¼ˆç¬¬ 3397 è¡Œï¼‰

```groovy
runLLMTestlistOnSlurm(
    pipeline,
    values[0],  // platform: gb200-oci-trtllm
    values[1],  // testList: l0_gb200_multi_nodes_aggr_perf_sanity_2_nodes
    config,
    key.contains("-Perf-"),  // perfMode: true
    key,        // stageName
    values[2],  // splitId: 1
    values[3],  // splits: 5
    values[4],  // gpuCount: 8
    values[5],  // nodeCount: 2
    values[6]   // runWithSbatch: false
)
```

### runLLMTestlistOnSlurm å‡½æ•°ï¼ˆç¬¬ 1411-1419 è¡Œï¼‰

```groovy
def runLLMTestlistOnSlurm(..., nodeCount=1, ...) {
    echo "Run Slurm job with native sbatch: $runWithSbatch"
    if (nodeCount > 1 || runWithSbatch) {
        // å¤šèŠ‚ç‚¹ï¼šä½¿ç”¨ sbatch
        runLLMTestlistWithSbatch(...)
    } else {
        // å•èŠ‚ç‚¹ï¼šä½¿ç”¨ agent
        runLLMTestlistWithAgent(...)
    }
}
```

### runLLMTestlistWithSbatch å‡½æ•°ï¼ˆç¬¬ 913-1409 è¡Œï¼‰

**æ ¸å¿ƒé€»è¾‘**:

1. **åˆ¤æ–­æ˜¯å¦ä¸º Disagg æ¨¡å¼**ï¼ˆç¬¬ 921 è¡Œï¼‰ï¼š
   ```groovy
   def disaggMode = stageName.contains("PerfSanity-Disagg")
   ```

2. **Disagg æ¨¡å¼**ï¼šè°ƒç”¨ `submit.py`
   ```groovy
   if (disaggMode) {
       // ä½¿ç”¨ jenkins/scripts/perf/disaggregated/submit.py
       script = """
           cd ${workspace}
           python3 jenkins/scripts/perf/disaggregated/submit.py \\
               --config <config_file.yaml> \\
               --work-dir <output_dir>
       """
   }
   ```

3. **Agg æ¨¡å¼**ï¼šè°ƒç”¨ `pytest` with `--test-list`
   ```groovy
   else {
       // è¯»å– TestList æ–‡ä»¶
       testListPath = "tests/integration/test_lists/test-db/${testList}.yml"
       
       // æå–æµ‹è¯•ç”¨ä¾‹åˆ° test_list.txt
       python3 << 'EOF'
       import yaml
       with open('${testListPath}') as f:
           data = yaml.safe_load(f)
       # æå– tests åˆ—è¡¨
       for item in data[testlist_name]:
           if 'tests' in item:
               for test in item['tests']:
                   print(test)
       EOF
       
       // ä½¿ç”¨ srun è¿è¡Œ pytest
       script = """
           srun --nodes=${nodeCount} \\
               python3 -m pytest \\
                   --test-list=${testListTxt} \\
                   --splitting-algorithm least_duration \\
                   --splits ${splits} \\
                   --group ${splitId} \\
                   tests/integration/defs/
       """
   }
   ```

---

## ğŸ”§ Perf_Test.groovy æ‰§è¡Œæµç¨‹

### Single Node Agg

**Jenkins å‚æ•°**:
```groovy
TEST_MODE: single-agg
CONFIG_FILE: aggr_upload-deepseek_r1_fp4_v2_grace_blackwell-r1_fp4_v2_dep4_mtp1_1k1k
```

**æ‰§è¡Œå‘½ä»¤**ï¼ˆç¬¬ 265-280 è¡Œï¼‰:
```bash
cd ${TRTLLM_DIR}
python3 -m pytest \
    tests/integration/defs/perf/test_perf_sanity.py::test_e2e \
    -k 'aggr_upload-deepseek_r1_fp4_v2_grace_blackwell-r1_fp4_v2_dep4_mtp1_1k1k' \
    -v
```

**è¯´æ˜**:
- âœ… ç›´æ¥è¿è¡Œ pytest
- âœ… ä½¿ç”¨ `-k` å‚æ•°è¿‡æ»¤æµ‹è¯•
- âœ… è°ƒç”¨ `test_perf_sanity.py::test_e2e`

### Multi-Node Agg

**Jenkins å‚æ•°**:
```groovy
TEST_MODE: multi-agg
CONFIG_FILE: aggr_upload-k2_thinking_fp4_2_nodes_grace_blackwell-k2_thinking_fp4_tep8_32k8k
NODE_LIST: node1,node2
```

**æ‰§è¡Œå‘½ä»¤**ï¼ˆç¬¬ 282-306 è¡Œï¼‰:
```bash
ssh node1 'cd ${TRTLLM_DIR} && \
srun \
    --nodes=2 \
    --ntasks-per-node=1 \
    python3 -m pytest \
        tests/integration/defs/perf/test_perf_sanity.py::test_e2e \
        -k "aggr_upload-k2_thinking_fp4_2_nodes_grace_blackwell-k2_thinking_fp4_tep8_32k8k" \
        -v'
```

**è¯´æ˜**:
- âœ… ä½¿ç”¨ `srun` å¤šèŠ‚ç‚¹æ‰§è¡Œ
- âœ… ä½¿ç”¨ `-k` å‚æ•°è¿‡æ»¤æµ‹è¯•
- âœ… è°ƒç”¨ `test_perf_sanity.py::test_e2e`

### Multi-Node Disagg

**Jenkins å‚æ•°**:
```groovy
TEST_MODE: disagg
TESTLIST: l0_gb200_multi_nodes_disagg_perf_sanity_3_nodes
NODE_LIST: node1,node2,node3,node4
```

**æ‰§è¡Œæµç¨‹**ï¼ˆç¬¬ 139-238 è¡Œï¼‰:

1. **æå–é…ç½®æ–‡ä»¶**:
   ```bash
   # ä» TestList YAML æå–é…ç½®å
   python3 << 'EOF'
   import yaml, re
   with open('tests/integration/test_lists/test-db/l0_gb200_multi_nodes_disagg_perf_sanity_3_nodes.yml') as f:
       data = yaml.safe_load(f)
   # æå–: deepseek-r1-fp4_1k1k_ctx1_gen1_dep8_bs768_eplb0_mtp0_ccb-UCX
   EOF
   ```

2. **æŸ¥æ‰¾é…ç½®æ–‡ä»¶**:
   ```bash
   # åœ¨ä»¥ä¸‹è·¯å¾„æŸ¥æ‰¾:
   tests/integration/defs/perf/disagg/test_configs/disagg/perf/deepseek-r1-fp4_1k1k_ctx1_gen1_dep8_bs768_eplb0_mtp0_ccb-UCX.yaml
   tests/integration/defs/perf/disagg/test_configs/wideep/perf/deepseek-r1-fp4_1k1k_ctx1_gen1_dep8_bs768_eplb0_mtp0_ccb-UCX.yaml
   ```

3. **è®¡ç®—ç¡¬ä»¶èŠ‚ç‚¹**:
   ```bash
   python3 scripts/calculate_hardware_nodes.py \
       --config <config_file.yaml> \
       --json
   
   # è¾“å‡º:
   # {
   #   "num_ctx_servers": 1,  # é€»è¾‘
   #   "num_gen_servers": 1,
   #   "ctx_nodes": 2,        # ç¡¬ä»¶
   #   "gen_nodes": 2,
   #   "total_nodes": 4
   # }
   ```

4. **éªŒè¯èŠ‚ç‚¹æ•°**:
   ```bash
   # æä¾›çš„èŠ‚ç‚¹: 4 (node1,node2,node3,node4)
   # è¦æ±‚çš„èŠ‚ç‚¹: 4
   # âœ“ éªŒè¯é€šè¿‡
   ```
   
   **âš ï¸ å½“å‰å®ç°çš„éªŒè¯æµç¨‹** (å­˜åœ¨è®¾è®¡é—®é¢˜):
   ```groovy
   // æ­¥éª¤ 1: ä» NODE_LIST å‚æ•°è·å–ç”¨æˆ·æä¾›çš„èŠ‚ç‚¹æ•°
   if (NODE_LIST) {
       // NODE_LIST = "node1,node2,node3,node4"
       def providedNodes = NODE_LIST.split(',').size()
       echo "æä¾›çš„èŠ‚ç‚¹æ•°: ${providedNodes}"  // è¾“å‡º: 4
       
       // æ­¥éª¤ 2: ä»è®¡ç®—ç»“æœè·å–é…ç½®è¦æ±‚çš„èŠ‚ç‚¹æ•°
       // nodeInfo.total_nodes = 4 (æ¥è‡ª calculate_hardware_nodes.py)
       
       // æ­¥éª¤ 3: å¯¹æ¯”
       if (providedNodes != nodeInfo.total_nodes) {
           error """
èŠ‚ç‚¹æ•°ä¸åŒ¹é…ï¼
  é…ç½®è¦æ±‚: ${nodeInfo.total_nodes} ä¸ªèŠ‚ç‚¹
  å®é™…æä¾›: ${providedNodes} ä¸ªèŠ‚ç‚¹
"""
       }
       
       echo "âœ“ èŠ‚ç‚¹æ•°éªŒè¯é€šè¿‡"  // 4 == 4
   }
   ```
   
   **âŒ é—®é¢˜**:
   - å½“å‰è®¾è®¡è¦æ±‚ç”¨æˆ·æ‰‹åŠ¨æŒ‡å®šèŠ‚ç‚¹åç§°åˆ—è¡¨ (`NODE_LIST`)
   - è¿™æ˜¯ä¸åˆç†çš„ï¼Œå› ä¸ºï¼š
     1. ç”¨æˆ·æ— æ³•é¢„çŸ¥ Slurm ä¼šåˆ†é…å“ªäº›å…·ä½“èŠ‚ç‚¹
     2. Slurm ä¼šæ ¹æ®èµ„æºå¯ç”¨æ€§åŠ¨æ€åˆ†é…èŠ‚ç‚¹
     3. æ‰‹åŠ¨æŒ‡å®šèŠ‚ç‚¹ä¼šé™åˆ¶è°ƒåº¦çµæ´»æ€§
   
   **âœ… æ­£ç¡®çš„åšæ³•** (å‚è€ƒ L0_Test.groovy ç¬¬ 786-797 è¡Œ):
   ```groovy
   // L0_Test.groovy çš„åšæ³•:
   def getNodeArgs(int nodeCount, int gpuCount, boolean setSegment = false) {
       int gpusPerNode = ((gpuCount / nodeCount) as BigDecimal).setScale(0, BigDecimal.ROUND_CEILING).intValue()
       def args = nodeCount == 1 ? [
           "--nodes=${nodeCount}",
           "--gpus=${gpuCount}"
       ] : [
           "--nodes=${nodeCount}",          // â† åªæŒ‡å®šæ•°é‡ï¼Œä¸æŒ‡å®šåç§°
           "--ntasks=${gpuCount}",
           "--ntasks-per-node=${gpusPerNode}",
           "--gpus-per-node=${gpusPerNode}",
       ]
       return args
   }
   
   // åœ¨ sbatch è„šæœ¬ä¸­ (ç¬¬ 1163-1168 è¡Œ):
   #SBATCH --nodes=4                       // â† åªæŒ‡å®šéœ€è¦ 4 ä¸ªèŠ‚ç‚¹
   #SBATCH --ntasks=32
   #SBATCH --ntasks-per-node=8
   
   // è¿è¡Œæ—¶è‡ªåŠ¨è·å–åˆ†é…çš„èŠ‚ç‚¹ (ç¬¬ 1174 è¡Œ):
   echo "Starting Slurm job $SLURM_JOB_ID on $SLURM_NODELIST"  // â† Slurm è‡ªåŠ¨åˆ†é…
   ```
   
   **Slurm èŠ‚ç‚¹åˆ†é…æœºåˆ¶**:
   1. **æäº¤æ—¶**: ç”¨æˆ·é€šè¿‡ `sbatch --nodes=4` å‘Šè¯‰ Slurm éœ€è¦ 4 ä¸ªèŠ‚ç‚¹
   2. **è°ƒåº¦æ—¶**: Slurm æ ¹æ®èµ„æºå¯ç”¨æ€§è‡ªåŠ¨é€‰æ‹© 4 ä¸ªèŠ‚ç‚¹ï¼ˆä¾‹å¦‚ gpu-node-[05-08]ï¼‰
   3. **è¿è¡Œæ—¶**: é€šè¿‡ç¯å¢ƒå˜é‡è·å–å®é™…åˆ†é…çš„èŠ‚ç‚¹ï¼š
      - `$SLURM_NODELIST`: èŠ‚ç‚¹åˆ—è¡¨ï¼ˆä¾‹å¦‚: `gpu-node-[05-08]`ï¼‰
      - `$SLURM_JOB_NODELIST`: åŒ `$SLURM_NODELIST`
      - `scontrol show hostname $SLURM_NODELIST`: å±•å¼€ä¸ºå…·ä½“èŠ‚ç‚¹å
   
   **submit.py å’Œ slurm_launch_draft.sh çš„å®é™…è¡Œä¸º**:
   - âœ… **ä¸ä¾èµ–ç”¨æˆ·æŒ‡å®šçš„èŠ‚ç‚¹åç§°**
   - âœ… é€šè¿‡ `srun -N <num_nodes>` æŒ‡å®šèŠ‚ç‚¹æ•°é‡
   - âœ… Slurm è‡ªåŠ¨åœ¨å·²åˆ†é…çš„èŠ‚ç‚¹ä¸­é€‰æ‹©å¯¹åº”æ•°é‡çš„èŠ‚ç‚¹æ‰§è¡Œä»»åŠ¡
   
   **ç¤ºä¾‹ - slurm_launch_draft.sh ç¬¬ 19-31 è¡Œ**:
   ```bash
   # å¯åŠ¨ gen servers
   for i in $(seq 0 $((numGenServers - 1))); do
       gen_world_size=$((nodesPerGenServer * gpusPerNode))
       export DISAGG_SERVING_TYPE="GEN_$i"
       export pytestCommand="$pytestCommandWorker"
       srun "${srunArgs[@]}" --kill-on-bad-exit=1 \
           -N $nodesPerGenServer \              # â† åªæŒ‡å®šæ•°é‡ï¼šéœ€è¦ 2 ä¸ªèŠ‚ç‚¹
           --ntasks=$gen_world_size \
           --ntasks-per-node=$gpusPerNode \
           $runScript &> $jobWorkspace/gen_server_$i.log &
   done
   # Slurm ä¼šä» $SLURM_NODELIST ä¸­è‡ªåŠ¨é€‰æ‹© 2 ä¸ªèŠ‚ç‚¹æ¥å¯åŠ¨è¿™ä¸ª GEN server
   ```

5. **è°ƒç”¨ submit.py**:
   ```bash
   python3 ${TRTLLM_DIR}/jenkins/scripts/perf/disaggregated/submit.py \
       --config <config_file.yaml>
   ```

**è¯´æ˜**:
- âœ… é€šè¿‡ `submit.py` æäº¤
- âœ… `submit.py` å†…éƒ¨ä¼šè°ƒç”¨ `test_perf_sanity.py::test_e2e`
- âœ… **submit.py è‡ªå·±åšäº†é€»è¾‘èŠ‚ç‚¹â†’ç¡¬ä»¶èŠ‚ç‚¹çš„è½¬æ¢**ï¼ˆç¬¬ 8-54 è¡Œï¼‰
- âœ… æˆ‘ä»¬çš„ `calculate_hardware_nodes.py` åªæ˜¯ç”¨æ¥éªŒè¯ï¼Œç®—æ³•ä¸ submit.py ä¸€è‡´

---

## ğŸ“ TestList æ–‡ä»¶æ ¼å¼

### Agg TestList ç¤ºä¾‹

**æ–‡ä»¶**: `tests/integration/test_lists/test-db/l0_gb200_multi_nodes_aggr_perf_sanity_2_nodes.yml`

```yaml
version: 0.0.1
l0_gb200_multi_nodes_aggr_perf_sanity_2_nodes:
- condition:
    ranges:
      system_gpu_count:
        gte: 8
        lte: 8
    wildcards:
      gpu:
      - '*gb200*'
    terms:
      stage: post_merge
      backend: pytorch
  tests:
  # â† é‡ç‚¹ï¼šæ‰€æœ‰æµ‹è¯•éƒ½è°ƒç”¨ test_perf_sanity.py::test_e2e
  - perf/test_perf_sanity.py::test_e2e[aggr_upload-deepseek_r1_fp4_v2_2_nodes_grace_blackwell-r1_fp4_v2_dep8_mtp1_1k1k] TIMEOUT (90)
  - perf/test_perf_sanity.py::test_e2e[aggr_upload-k2_thinking_fp4_2_nodes_grace_blackwell-k2_thinking_fp4_tep8_32k8k] TIMEOUT (90)
```

### Disagg TestList ç¤ºä¾‹

**æ–‡ä»¶**: `tests/integration/test_lists/test-db/l0_gb200_multi_nodes_disagg_perf_sanity_3_nodes.yml`

```yaml
version: 0.0.1
l0_gb200_multi_nodes_disagg_perf_sanity_3_nodes:
- condition:
    ranges:
      system_gpu_count:
        gte: 12
        lte: 12
    wildcards:
      gpu:
      - '*gb200*'
    terms:
      stage: post_merge
      backend: pytorch
  tests:
  # â† é‡ç‚¹ï¼šDisagg ä¹Ÿè°ƒç”¨ test_perf_sanity.py::test_e2e
  - perf/test_perf_sanity.py::test_e2e[disagg_upload-deepseek-r1-fp4_1k1k_ctx1_gen1_dep8_bs768_eplb0_mtp0_ccb-UCX] TIMEOUT (90)
```

---

## ğŸ¨ test_perf_sanity.py::test_e2e è¯¦è§£

### æµ‹è¯•å…¥å£

**ä½ç½®**: `tests/integration/defs/perf/test_perf_sanity.py`

**å‡½æ•°ç­¾å**:
```python
@pytest.mark.parametrize("config_name", [...])
def test_e2e(config_name: str, request):
    """End-to-end performance test."""
    pass
```

### Agg æ¨¡å¼æµç¨‹

**å‚æ•°ç¤ºä¾‹**:
```
config_name = "aggr_upload-deepseek_r1_fp4_v2_grace_blackwell-r1_fp4_v2_dep4_mtp1_1k1k"
```

**æ‰§è¡Œæµç¨‹**:
```python
# 1. è§£æé…ç½®å
parts = config_name.split('-')
config_type = parts[0]  # "aggr_upload"
model_config = parts[1]  # "deepseek_r1_fp4_v2_grace_blackwell"
test_config = parts[2]   # "r1_fp4_v2_dep4_mtp1_1k1k"

# 2. æŸ¥æ‰¾é…ç½®æ–‡ä»¶
config_file = f"tests/scripts/perf-sanity/{model_config}.yaml"

# 3. è¯»å–é…ç½®
with open(config_file) as f:
    config = yaml.safe_load(f)

# 4. æå– server_config
server_config = config['server_config']
# server_config:
#   model_name: deepseek_r1_fp4
#   tensor_parallel_size: 4
#   disagg_run_type: aggr  # â† å…³é”®ï¼

# 5. å¯åŠ¨ trtllm-server
# æ³¨æ„: disagg_run_type é»˜è®¤å€¼æ˜¯ "aggr" (ç¬¬ 129 è¡Œ)
if server_config['disagg_run_type'] == 'aggr':
    # å•èŠ‚ç‚¹æˆ–å¤šèŠ‚ç‚¹èšåˆæ¨¡å¼
    start_aggregated_server(server_config)

# 6. è¿è¡Œ benchmark
run_benchmark(benchmark_config)

# 7. æ”¶é›†æ€§èƒ½æŒ‡æ ‡
metrics = parse_benchmark_output(output)

# 8. ä¸Šä¼ åˆ°æ•°æ®åº“
post_new_perf_data(metrics)
```

### Disagg æ¨¡å¼æµç¨‹

**å‚æ•°ç¤ºä¾‹**:
```
config_name = "disagg_upload-deepseek-r1-fp4_1k1k_ctx1_gen1_dep8_bs768_eplb0_mtp0_ccb-UCX"
```

**æ‰§è¡Œæµç¨‹**:
```python
# 1. è§£æé…ç½®å
config_type = "disagg_upload"

# 2. æŸ¥æ‰¾é…ç½®æ–‡ä»¶
config_file = "tests/integration/defs/perf/disagg/test_configs/disagg/perf/deepseek-r1-fp4_1k1k_ctx1_gen1_dep8_bs768_eplb0_mtp0_ccb-UCX.yaml"

# 3. è¯»å–é…ç½®
with open(config_file) as f:
    config = yaml.safe_load(f)

# 4. é…ç½®ç»“æ„
# hardware:
#   num_ctx_servers: 1  # é€»è¾‘æœåŠ¡å™¨æ•°
#   num_gen_servers: 1
#   gpus_per_node: 4
# worker_config:
#   ctx:
#     tensor_parallel_size: 4
#   gen:
#     tensor_parallel_size: 8

# 5. é€šè¿‡ submit.py å·²ç»å¯åŠ¨äº†å¤šä¸ªè¿›ç¨‹
# - CTX workers (2 ä¸ªç¡¬ä»¶èŠ‚ç‚¹)
# - GEN workers (2 ä¸ªç¡¬ä»¶èŠ‚ç‚¹)
# - Disagg server
# - Benchmark client

# 6. test_e2e åªéœ€è¦ç­‰å¾…å¹¶æ”¶é›†ç»“æœ
wait_for_disagg_test_complete()

# 7. æ”¶é›†æ€§èƒ½æŒ‡æ ‡
metrics = parse_disagg_benchmark_output(output)

# 8. ä¸Šä¼ åˆ°æ•°æ®åº“
post_new_perf_data(metrics)
```

---

## ğŸ”„ å®Œæ•´è°ƒç”¨é“¾å¯¹æ¯”

### L0_Test.groovy - Multi-Node Agg

```
L0_Test.groovy (ç¬¬ 3358 è¡Œ)
    â†“
multiNodesSBSAConfigs é…ç½®
    "GB200-8_GPUs-2_Nodes-PyTorch-PerfSanity-Post-Merge-1"
    â†“
runLLMTestlistOnSlurm (ç¬¬ 3397 è¡Œ)
    â†“
runLLMTestlistWithSbatch (ç¬¬ 913 è¡Œ)
    â†“
åˆ¤æ–­: disaggMode = false (ç¬¬ 921 è¡Œ)
    â†“
è¯»å– TestList YAML æ–‡ä»¶ (ç¬¬ 1066 è¡Œ)
    tests/integration/test_lists/test-db/l0_gb200_multi_nodes_aggr_perf_sanity_2_nodes.yml
    â†“
æå–æµ‹è¯•ç”¨ä¾‹åˆ—è¡¨åˆ° test_list.txt
    perf/test_perf_sanity.py::test_e2e[aggr_upload-deepseek_r1_fp4_v2_2_nodes_grace_blackwell-r1_fp4_v2_dep8_mtp1_1k1k]
    perf/test_perf_sanity.py::test_e2e[aggr_upload-k2_thinking_fp4_2_nodes_grace_blackwell-k2_thinking_fp4_tep8_32k8k]
    â†“
srun --nodes=2 python3 -m pytest \
    --test-list=test_list.txt \
    --splitting-algorithm least_duration \
    --splits 5 \
    --group 1 \
    tests/integration/defs/
    â†“
pytest å‘ç°å¹¶è¿è¡Œ
    tests/integration/defs/perf/test_perf_sanity.py::test_e2e
        â†“
        å¯åŠ¨ aggregated trtllm-server (2 èŠ‚ç‚¹, TP=8)
        â†“
        è¿è¡Œ benchmark
        â†“
        æ”¶é›†æ€§èƒ½æŒ‡æ ‡
        â†“
        ä¸Šä¼ åˆ°æ•°æ®åº“
```

### L0_Test.groovy - Multi-Node Disagg

```
L0_Test.groovy (ç¬¬ 3363 è¡Œ)
    â†“
multiNodesSBSAConfigs é…ç½®
    "GB200-12_GPUs-3_Nodes-PyTorch-PerfSanity-Disagg-Post-Merge-1"
    â†“
runLLMTestlistOnSlurm (ç¬¬ 3397 è¡Œ)
    â†“
runLLMTestlistWithSbatch (ç¬¬ 913 è¡Œ)
    â†“
åˆ¤æ–­: disaggMode = true (ç¬¬ 921 è¡Œ)
    â†“
è°ƒç”¨ jenkins/scripts/perf/disaggregated/submit.py
    â†“
submit.py è¯»å–é…ç½®
    tests/integration/defs/perf/disagg/test_configs/disagg/perf/deepseek-r1-fp4_1k1k_ctx1_gen1_dep8_bs768_eplb0_mtp0_ccb-UCX.yaml
    â†“
submit.py è®¡ç®—ç¡¬ä»¶èŠ‚ç‚¹
    num_ctx_servers: 1 (é€»è¾‘) â†’ ctx_nodes: 2 (ç¡¬ä»¶)
    num_gen_servers: 1 (é€»è¾‘) â†’ gen_nodes: 2 (ç¡¬ä»¶)
    total_nodes: 4
    â†“
submit.py ç”Ÿæˆ sbatch è„šæœ¬
    â†“
sbatch æäº¤å¤šä¸ªä»»åŠ¡
    â”œâ”€ CTX workers (node1, node2)
    â”œâ”€ GEN workers (node3, node4)
    â”œâ”€ Disagg server (node1)
    â””â”€ Benchmark client
        â†“
        å†…éƒ¨è°ƒç”¨ pytest tests/integration/defs/perf/test_perf_sanity.py::test_e2e
            â†“
            ç­‰å¾… disagg æµ‹è¯•å®Œæˆ
            â†“
            æ”¶é›†æ€§èƒ½æŒ‡æ ‡
            â†“
            ä¸Šä¼ åˆ°æ•°æ®åº“
```

### Perf_Test.groovy - Single Agg

```
Perf_Test.groovy
    â†“
Jenkins å‚æ•°
    TEST_MODE: single-agg
    CONFIG_FILE: aggr_upload-deepseek_r1_fp4_v2_grace_blackwell-r1_fp4_v2_dep4_mtp1_1k1k
    â†“
æ‹‰å– TensorRT-LLM (ç¬¬ 93 è¡Œ)
    â†“
å¤„ç†é…ç½® - Agg æ¨¡å¼ (ç¬¬ 240 è¡Œ)
    â†“
æŸ¥æ‰¾é…ç½®æ–‡ä»¶
    tests/scripts/perf-sanity/deepseek_r1_fp4_v2_grace_blackwell.yaml
    â†“
è¿è¡Œæµ‹è¯• (ç¬¬ 265 è¡Œ)
    â†“
python3 -m pytest \
    tests/integration/defs/perf/test_perf_sanity.py::test_e2e \
    -k 'aggr_upload-deepseek_r1_fp4_v2_grace_blackwell-r1_fp4_v2_dep4_mtp1_1k1k' \
    -v
    â†“
pytest è¿è¡Œ
    tests/integration/defs/perf/test_perf_sanity.py::test_e2e
        â†“
        å¯åŠ¨ aggregated trtllm-server (å•èŠ‚ç‚¹, TP=4)
        â†“
        è¿è¡Œ benchmark
        â†“
        æ”¶é›†æ€§èƒ½æŒ‡æ ‡
        â†“
        ä¸Šä¼ åˆ°æ•°æ®åº“
```

### Perf_Test.groovy - Multi-Node Agg

```
Perf_Test.groovy
    â†“
Jenkins å‚æ•°
    TEST_MODE: multi-agg
    CONFIG_FILE: aggr_upload-k2_thinking_fp4_2_nodes_grace_blackwell-k2_thinking_fp4_tep8_32k8k
    NODE_LIST: node1,node2
    â†“
æ‹‰å– TensorRT-LLM
    â†“
å¤„ç†é…ç½® - Agg æ¨¡å¼
    â†“
æŸ¥æ‰¾é…ç½®æ–‡ä»¶
    tests/scripts/perf-sanity/k2_thinking_fp4_2_nodes_grace_blackwell.yaml
    â†“
è¿è¡Œæµ‹è¯• (ç¬¬ 282 è¡Œ)
    â†“
ssh node1 'cd ${TRTLLM_DIR} && \
srun --nodes=2 --ntasks-per-node=1 \
    python3 -m pytest \
        tests/integration/defs/perf/test_perf_sanity.py::test_e2e \
        -k "aggr_upload-k2_thinking_fp4_2_nodes_grace_blackwell-k2_thinking_fp4_tep8_32k8k" \
        -v'
    â†“
srun å¤šèŠ‚ç‚¹è¿è¡Œ pytest
    tests/integration/defs/perf/test_perf_sanity.py::test_e2e
        â†“
        å¯åŠ¨ aggregated trtllm-server (2 èŠ‚ç‚¹, TP=8)
        â†“
        è¿è¡Œ benchmark
        â†“
        æ”¶é›†æ€§èƒ½æŒ‡æ ‡
        â†“
        ä¸Šä¼ åˆ°æ•°æ®åº“
```

### Perf_Test.groovy - Multi-Node Disagg

```
Perf_Test.groovy
    â†“
Jenkins å‚æ•°
    TEST_MODE: disagg
    TESTLIST: l0_gb200_multi_nodes_disagg_perf_sanity_3_nodes
    NODE_LIST: node1,node2,node3,node4
    â†“
æ‹‰å– TensorRT-LLM
    â†“
å¤„ç†é…ç½® - Disagg æ¨¡å¼ (ç¬¬ 139 è¡Œ)
    â†“
ä» TestList æå–é…ç½®å (ç¬¬ 164 è¡Œ)
    tests/integration/test_lists/test-db/l0_gb200_multi_nodes_disagg_perf_sanity_3_nodes.yml
    â†“
    æå–: deepseek-r1-fp4_1k1k_ctx1_gen1_dep8_bs768_eplb0_mtp0_ccb-UCX
    â†“
æŸ¥æ‰¾é…ç½®æ–‡ä»¶ (ç¬¬ 204 è¡Œ)
    tests/integration/defs/perf/disagg/test_configs/disagg/perf/deepseek-r1-fp4_1k1k_ctx1_gen1_dep8_bs768_eplb0_mtp0_ccb-UCX.yaml
    â†“
è®¡ç®—ç¡¬ä»¶èŠ‚ç‚¹ (ç¬¬ 217 è¡Œ)
    scripts/calculate_hardware_nodes.py --config <config>.yaml --json
    â†“
    è¾“å‡º: {total_nodes: 4, ctx_nodes: 2, gen_nodes: 2}
    â†“
éªŒè¯èŠ‚ç‚¹æ•° (ç¬¬ 228 è¡Œ)
    # ç”¨æˆ·åœ¨ Jenkins å‚æ•°ä¸­æä¾›çš„èŠ‚ç‚¹åˆ—è¡¨
    NODE_LIST: node1,node2,node3,node4  â† æä¾›äº† 4 ä¸ªèŠ‚ç‚¹
    
    # ä»é…ç½®æ–‡ä»¶è®¡ç®—å‡ºçš„è¦æ±‚
    total_nodes: 4  â† é…ç½®è¦æ±‚ 4 ä¸ªèŠ‚ç‚¹
    
    # éªŒè¯: æä¾›çš„èŠ‚ç‚¹æ•° == è¦æ±‚çš„èŠ‚ç‚¹æ•°
    æä¾›: 4 (ä» NODE_LIST.split(',').size())
    è¦æ±‚: 4 (ä» calculate_hardware_nodes.py è®¡ç®—)
    â†’ âœ“ é€šè¿‡ (4 == 4)
    â†“
æäº¤ä»»åŠ¡ (ç¬¬ 313 è¡Œ)
    â†“
python3 ${TRTLLM_DIR}/jenkins/scripts/perf/disaggregated/submit.py \
    --config <config>.yaml
    â†“
submit.py æ‰§è¡Œ
    â†“
    ç”Ÿæˆ sbatch è„šæœ¬
    â†“
    sbatch æäº¤å¤šä¸ªä»»åŠ¡
        â”œâ”€ CTX workers (node1, node2)
        â”œâ”€ GEN workers (node3, node4)
        â”œâ”€ Disagg server (node1)
        â””â”€ Benchmark client
            â†“
            å†…éƒ¨è°ƒç”¨ pytest tests/integration/defs/perf/test_perf_sanity.py::test_e2e
                â†“
                ç­‰å¾… disagg æµ‹è¯•å®Œæˆ
                â†“
                æ”¶é›†æ€§èƒ½æŒ‡æ ‡
                â†“
                ä¸Šä¼ åˆ°æ•°æ®åº“
```

---

## ğŸ” å…³é”®é—®é¢˜è§£ç­”

### Q1: disagg_run_type çš„é»˜è®¤å€¼æ˜¯ä»€ä¹ˆï¼Ÿ

**ç­”æ¡ˆ**: é»˜è®¤å€¼æ˜¯ `"aggr"`

**ä»£ç ä½ç½®**: `tests/integration/defs/perf/test_perf_sanity.py` ç¬¬ 129 è¡Œ

```python
class ServerConfig:
    def __init__(self, server_config_data: dict, env_vars: str = ""):
        self.disagg_run_type = server_config_data.get("disagg_run_type", "aggr")
                                                                         ^^^^^^
                                                                         é»˜è®¤å€¼
```

**å®é™…å½±å“**:
- âœ… Agg é…ç½®æ–‡ä»¶å¯ä»¥çœç•¥ `disagg_run_type` å­—æ®µ
- âœ… æœªæŒ‡å®šæ—¶è‡ªåŠ¨æŒ‰ aggregated æ¨¡å¼è¿è¡Œ
- âš ï¸ Disagg é…ç½®ä¸ä½¿ç”¨ `server_config`ï¼Œç›´æ¥ç”¨ `hardware` + `worker_config`

**ç¤ºä¾‹é…ç½®**:

```yaml
# Agg é…ç½® (å¯ä»¥çœç•¥ disagg_run_type)
server_config:
  model_name: deepseek_r1_fp4
  tensor_parallel_size: 4
  # disagg_run_type: aggr  â† å¯ä»¥çœç•¥ï¼Œé»˜è®¤å°±æ˜¯ aggr

# Disagg é…ç½® (æ ¹æœ¬ä¸ç”¨ server_config)
hardware:
  num_ctx_servers: 1
  num_gen_servers: 1
  gpus_per_node: 4
worker_config:
  ctx:
    tensor_parallel_size: 4
  gen:
    tensor_parallel_size: 8
```

---

### Q2: submit.py æ˜¯å¦åšäº†é€»è¾‘èŠ‚ç‚¹â†’ç¡¬ä»¶èŠ‚ç‚¹çš„è½¬æ¢ï¼Ÿ

**ç­”æ¡ˆ**: âœ… æ˜¯çš„ï¼submit.py ç¡®å®åšäº†å®Œæ•´çš„è½¬æ¢

**ä»£ç ä½ç½®**: `jenkins/scripts/perf/disaggregated/submit.py` ç¬¬ 8-54 è¡Œ

**å®Œæ•´è½¬æ¢é€»è¾‘**:

```python
def get_hardware_config(config, benchmark_mode):
    hardware = config.get("hardware", {})
    worker_config = config.get("worker_config", {})

    # ========================================
    # æ­¥éª¤ 1: è¯»å–é€»è¾‘é…ç½®
    # ========================================
    num_ctx_servers = hardware.get("num_ctx_servers")  # é€»è¾‘æœåŠ¡å™¨æ•°
    num_gen_servers = hardware.get("num_gen_servers")  # é€»è¾‘æœåŠ¡å™¨æ•°
    gpus_per_node = hardware.get("gpus_per_node")      # ç¡¬ä»¶é…ç½®
    
    # ========================================
    # æ­¥éª¤ 2: è®¡ç®—æ¯ä¸ªé€»è¾‘æœåŠ¡å™¨éœ€è¦çš„ GPU æ•°
    # ========================================
    ctx_config = worker_config.get("ctx", {})
    ctx_tp = ctx_config.get("tensor_parallel_size", 1)
    ctx_pp = ctx_config.get("pipeline_parallel_size", 1)
    ctx_cp = ctx_config.get("context_parallel_size", 1)
    gpus_per_ctx_server = ctx_tp * ctx_pp * ctx_cp  # CTX æœåŠ¡å™¨éœ€è¦çš„ GPU æ•°
    
    gen_config = worker_config.get("gen", {})
    gen_tp = gen_config.get("tensor_parallel_size", 1)
    gen_pp = gen_config.get("pipeline_parallel_size", 1)
    gen_cp = gen_config.get("context_parallel_size", 1)
    gpus_per_gen_server = gen_tp * gen_pp * gen_cp  # GEN æœåŠ¡å™¨éœ€è¦çš„ GPU æ•°
    
    # ========================================
    # æ­¥éª¤ 3: è®¡ç®—æ¯ä¸ªé€»è¾‘æœåŠ¡å™¨éœ€è¦çš„ç¡¬ä»¶èŠ‚ç‚¹æ•°ï¼ˆå‘ä¸Šå–æ•´ï¼‰
    # ========================================
    nodes_per_ctx_server = (gpus_per_ctx_server + gpus_per_node - 1) // gpus_per_node
    nodes_per_gen_server = (gpus_per_gen_server + gpus_per_node - 1) // gpus_per_node
    
    # ========================================
    # æ­¥éª¤ 4: è®¡ç®—æ€»ç¡¬ä»¶èŠ‚ç‚¹æ•°
    # ========================================
    total_nodes = num_ctx_servers * nodes_per_ctx_server + \
                  num_gen_servers * nodes_per_gen_server
    total_gpus = total_nodes * gpus_per_node
    
    return {
        "num_ctx_servers": num_ctx_servers,           # é€»è¾‘
        "num_gen_servers": num_gen_servers,           # é€»è¾‘
        "gpus_per_ctx_server": gpus_per_ctx_server,   # æ¯ä¸ªé€»è¾‘æœåŠ¡å™¨çš„ GPU
        "gpus_per_gen_server": gpus_per_gen_server,   # æ¯ä¸ªé€»è¾‘æœåŠ¡å™¨çš„ GPU
        "nodes_per_ctx_server": nodes_per_ctx_server, # æ¯ä¸ªé€»è¾‘æœåŠ¡å™¨çš„ç¡¬ä»¶èŠ‚ç‚¹
        "nodes_per_gen_server": nodes_per_gen_server, # æ¯ä¸ªé€»è¾‘æœåŠ¡å™¨çš„ç¡¬ä»¶èŠ‚ç‚¹
        "total_nodes": total_nodes,                   # æ€»ç¡¬ä»¶èŠ‚ç‚¹æ•° â­
        "total_gpus": total_gpus,                     # æ€» GPU æ•°
    }
```

**è®¡ç®—ç¤ºä¾‹ 1 - ç®€å•é…ç½®**:

```yaml
hardware:
  num_ctx_servers: 1    # 1 ä¸ªé€»è¾‘ CTX æœåŠ¡å™¨
  num_gen_servers: 1    # 1 ä¸ªé€»è¾‘ GEN æœåŠ¡å™¨
  gpus_per_node: 4      # æ¯ä¸ªç¡¬ä»¶èŠ‚ç‚¹ 4 ä¸ª GPU
worker_config:
  ctx:
    tensor_parallel_size: 4   # CTX TP=4
    pipeline_parallel_size: 1
    context_parallel_size: 1
  gen:
    tensor_parallel_size: 8   # GEN TP=8
    pipeline_parallel_size: 1
    context_parallel_size: 1
```

**è®¡ç®—è¿‡ç¨‹**:
```python
# CTX è®¡ç®—
gpus_per_ctx_server = 4 Ã— 1 Ã— 1 = 4 GPU
nodes_per_ctx_server = ceil(4 / 4) = 1 ç¡¬ä»¶èŠ‚ç‚¹
ctx_total_nodes = 1 é€»è¾‘æœåŠ¡å™¨ Ã— 1 èŠ‚ç‚¹/æœåŠ¡å™¨ = 1 ç¡¬ä»¶èŠ‚ç‚¹

# GEN è®¡ç®—
gpus_per_gen_server = 8 Ã— 1 Ã— 1 = 8 GPU
nodes_per_gen_server = ceil(8 / 4) = 2 ç¡¬ä»¶èŠ‚ç‚¹
gen_total_nodes = 1 é€»è¾‘æœåŠ¡å™¨ Ã— 2 èŠ‚ç‚¹/æœåŠ¡å™¨ = 2 ç¡¬ä»¶èŠ‚ç‚¹

# æ€»è®¡
total_nodes = 1 + 2 = 3 ç¡¬ä»¶èŠ‚ç‚¹
total_gpus = 3 Ã— 4 = 12 GPU
```

**ä½†å®é™… TestList ä¸­æ˜¯ 4 ä¸ªèŠ‚ç‚¹ï¼ä¸ºä»€ä¹ˆï¼Ÿ**

æŸ¥çœ‹å®é™…é…ç½®æ–‡ä»¶ä¼šå‘ç° CTX æˆ– GEN çš„é…ç½®ä¸åŒï¼Œä¾‹å¦‚ï¼š
```yaml
# å®é™…é…ç½®å¯èƒ½æ˜¯
worker_config:
  ctx:
    tensor_parallel_size: 8  # â† CTX ä¹Ÿæ˜¯ TP=8
  gen:
    tensor_parallel_size: 8
```

è¿™æ ·è®¡ç®—ï¼š
```python
ctx_total_nodes = 1 Ã— ceil(8/4) = 1 Ã— 2 = 2 ç¡¬ä»¶èŠ‚ç‚¹
gen_total_nodes = 1 Ã— ceil(8/4) = 1 Ã— 2 = 2 ç¡¬ä»¶èŠ‚ç‚¹
total_nodes = 2 + 2 = 4 ç¡¬ä»¶èŠ‚ç‚¹  # âœ“ æ­£ç¡®ï¼
```

**è®¡ç®—ç¤ºä¾‹ 2 - å¤æ‚é…ç½®**:

```yaml
hardware:
  num_ctx_servers: 2    # 2 ä¸ªé€»è¾‘ CTX æœåŠ¡å™¨
  num_gen_servers: 1    # 1 ä¸ªé€»è¾‘ GEN æœåŠ¡å™¨
  gpus_per_node: 4
worker_config:
  ctx:
    tensor_parallel_size: 4
  gen:
    tensor_parallel_size: 8
```

**è®¡ç®—è¿‡ç¨‹**:
```python
# CTX è®¡ç®—
gpus_per_ctx_server = 4
nodes_per_ctx_server = 1
ctx_total_nodes = 2 é€»è¾‘æœåŠ¡å™¨ Ã— 1 èŠ‚ç‚¹/æœåŠ¡å™¨ = 2 ç¡¬ä»¶èŠ‚ç‚¹

# GEN è®¡ç®—
gpus_per_gen_server = 8
nodes_per_gen_server = 2
gen_total_nodes = 1 é€»è¾‘æœåŠ¡å™¨ Ã— 2 èŠ‚ç‚¹/æœåŠ¡å™¨ = 2 ç¡¬ä»¶èŠ‚ç‚¹

# æ€»è®¡
total_nodes = 2 + 2 = 4 ç¡¬ä»¶èŠ‚ç‚¹
total_gpus = 4 Ã— 4 = 16 GPU
```

---

### Q3: calculate_hardware_nodes.py å’Œ submit.py çš„å…³ç³»ï¼Ÿ

**ç­”æ¡ˆ**: ä¸¤è€…ç®—æ³•å®Œå…¨ä¸€è‡´ï¼Œåªæ˜¯è°ƒç”¨æ—¶æœºä¸åŒ

**å¯¹æ¯”**:

| ç‰¹æ€§ | calculate_hardware_nodes.py | submit.py (get_hardware_config) |
|------|----------------------------|----------------------------------|
| **ä½ç½®** | `jenkins_test/scripts/` | `jenkins/scripts/perf/disaggregated/` |
| **ç”¨é€”** | Perf_Test.groovy ç”¨æ¥éªŒè¯ | L0 submit.py çš„å†…éƒ¨å‡½æ•° |
| **è°ƒç”¨æ—¶æœº** | Pipeline ä¸­ï¼Œæäº¤å‰éªŒè¯ | submit.py å†…éƒ¨ï¼Œç”Ÿæˆè„šæœ¬æ—¶ |
| **ç®—æ³•** | âœ… ç›¸åŒ | âœ… ç›¸åŒ |
| **å…¬å¼** | `ceil(world_size Ã— num_servers / gpus_per_node)` | `(gpus_per_server + gpus_per_node - 1) // gpus_per_node` â­ |

**å…¬å¼ç­‰ä»·æ€§è¯æ˜**:

```python
# æ–¹å¼ 1 (calculate_hardware_nodes.py)
import math
result1 = math.ceil(world_size * num_servers / gpus_per_node)

# æ–¹å¼ 2 (submit.py)
result2 = (world_size * num_servers + gpus_per_node - 1) // gpus_per_node

# è¯æ˜ç­‰ä»·
# ceil(a/b) = floor((a + b - 1) / b) = (a + b - 1) // b
# ç¤ºä¾‹: ceil(8/4) = floor((8+4-1)/4) = floor(11/4) = 2
```

**ä¸ºä»€ä¹ˆéœ€è¦ä¸¤ä»½ï¼Ÿ**

1. **calculate_hardware_nodes.py**:
   - Perf_Test.groovy ç”¨æ¥**æå‰éªŒè¯**èŠ‚ç‚¹æ•°
   - åœ¨æäº¤ä»»åŠ¡**ä¹‹å‰**æ£€æŸ¥ï¼Œé¿å…æµªè´¹èµ„æº
   - å¯ä»¥ç‹¬ç«‹ä½¿ç”¨ï¼Œæ–¹ä¾¿è°ƒè¯•

2. **submit.py (get_hardware_config)**:
   - L0 submit.py å†…éƒ¨ä½¿ç”¨
   - ç”Ÿæˆ Slurm è„šæœ¬æ—¶è®¡ç®—
   - æ˜¯ L0 çš„æ ¸å¿ƒé€»è¾‘ï¼Œä¸èƒ½ä¿®æ”¹

**ç»“è®º**: æˆ‘ä»¬çš„ `calculate_hardware_nodes.py` æ˜¯æ­£ç¡®çš„ï¼Œä¸ L0 å®Œå…¨ä¸€è‡´ï¼

---

## ğŸ“ˆ æ€§èƒ½æŒ‡æ ‡æ”¶é›†

### ç»Ÿä¸€çš„æŒ‡æ ‡æ”¶é›†ï¼ˆtest_perf_sanity.pyï¼‰

**ä½ç½®**: `test_perf_sanity.py` ç¬¬ 73-90 è¡Œ

```python
PERF_METRIC_LOG_QUERIES = {
    "seq_throughput": re.compile(r"Request throughput \(req\/s\):\s+(-?[\d\.]+)"),
    "token_throughput": re.compile(r"Output token throughput \(tok\/s\):\s+(-?[\d\.]+)"),
    "total_token_throughput": re.compile(r"Total Token throughput \(tok\/s\):\s+(-?[\d\.]+)"),
    "user_throughput": re.compile(r"User throughput \(tok\/s\):\s+(-?[\d\.]+)"),
    "mean_ttft": re.compile(r"Mean TTFT \(ms\):\s+(-?[\d\.]+)"),
    "median_ttft": re.compile(r"Median TTFT \(ms\):\s+(-?[\d\.]+)"),
    "p99_ttft": re.compile(r"P99 TTFT \(ms\):\s+(-?[\d\.]+)"),
    "mean_itl": re.compile(r"Mean ITL \(ms\):\s+(-?[\d\.]+)"),
    "median_itl": re.compile(r"Median ITL \(ms\):\s+(-?[\d\.]+)"),
    "p99_itl": re.compile(r"P99 ITL \(ms\):\s+(-?[\d\.]+)"),
    "mean_tpot": re.compile(r"Mean TPOT \(ms\):\s+(-?[\d\.]+)"),
    "median_tpot": re.compile(r"Median TPOT \(ms\):\s+(-?[\d\.]+)"),
    "p99_tpot": re.compile(r"P99 TPOT \(ms\):\s+(-?[\d\.]+)"),
    "mean_e2el": re.compile(r"Mean E2EL \(ms\):\s+(-?[\d\.]+)"),
    "median_e2el": re.compile(r"Median E2EL \(ms\):\s+(-?[\d\.]+)"),
    "p99_e2el": re.compile(r"P99 E2EL \(ms\):\s+(-?[\d\.]+)"),
}
```

**è¯´æ˜**:
- âœ… Agg å’Œ Disagg ä½¿ç”¨ç›¸åŒçš„æŒ‡æ ‡å®šä¹‰
- âœ… ä» benchmark è¾“å‡ºè§£ææ€§èƒ½æ•°æ®
- âœ… ä¸Šä¼ åˆ° OpenSearch æ•°æ®åº“

---

## âŒ test_perf.py å·²ä¸å†ä½¿ç”¨

### ä¸ºä»€ä¹ˆå¼ƒç”¨ï¼Ÿ

1. **æ—§çš„æµ‹è¯•æ¡†æ¶**:
   - `test_perf.py` æ˜¯æ—©æœŸçš„æ€§èƒ½æµ‹è¯•æ¡†æ¶
   - ä½¿ç”¨ä¸åŒçš„é…ç½®æ ¼å¼
   - ä¸æ”¯æŒ Disagg æ¨¡å¼

2. **ç°çŠ¶**:
   - âš ï¸ åœ¨ TestList ä¸­å·²æ‰¾ä¸åˆ° `perf/test_perf.py::test_perf` çš„å¼•ç”¨
   - âš ï¸ æ‰€æœ‰ Perf Sanity æµ‹è¯•éƒ½ä½¿ç”¨ `test_perf_sanity.py::test_e2e`
   - âš ï¸ `test_perf.py` å¯èƒ½ä»ç”¨äºå®Œæ•´çš„ perf æµ‹è¯•ï¼ˆé sanityï¼‰

### æœç´¢ç»“æœéªŒè¯

```bash
# åœ¨ test_lists ä¸­æœç´¢ test_perf.py
grep -r "test_perf.py" tests/integration/test_lists/

# ç»“æœï¼šåªåœ¨ waives.txt ä¸­å‡ºç°ï¼ˆè·³è¿‡çš„æµ‹è¯•ï¼‰
tests/integration/test_lists/waives.txt:
  - perf/test_perf.py::test_perf[...] SKIP
  - perf/test_perf.py::test_perf[...] SKIP
  ...

# åœ¨ test_lists ä¸­æœç´¢ test_perf_sanity.py
grep -r "test_perf_sanity.py" tests/integration/test_lists/

# ç»“æœï¼šå¤§é‡ä½¿ç”¨
tests/integration/test_lists/test-db/l0_gb200_multi_nodes_aggr_perf_sanity_2_nodes.yml:
  - perf/test_perf_sanity.py::test_e2e[...]
tests/integration/test_lists/test-db/l0_gb200_multi_nodes_disagg_perf_sanity_3_nodes.yml:
  - perf/test_perf_sanity.py::test_e2e[...]
tests/integration/test_lists/test-db/l0_gb200_multi_gpus_perf_sanity.yml:
  - perf/test_perf_sanity.py::test_e2e[...]
```

---

## ğŸ¯ æ€»ç»“

### æ ¸å¿ƒå‘ç°

1. **ç»Ÿä¸€æµ‹è¯•æ¡†æ¶**:
   - âœ… æ‰€æœ‰ Perf Sanity æµ‹è¯•ä½¿ç”¨ `test_perf_sanity.py::test_e2e`
   - âœ… Single Agg, Multi-Node Agg, Multi-Node Disagg éƒ½ç”¨åŒä¸€ä¸ªæµ‹è¯•

2. **æµ‹è¯•ç±»å‹åŒºåˆ†**:
   - **Agg**: é€šè¿‡é…ç½®ä¸­çš„ `disagg_run_type: aggr` åŒºåˆ†
   - **Disagg**: é€šè¿‡é…ç½®ä¸­çš„ `hardware` å’Œ `worker_config` åŒºåˆ†

3. **æ‰§è¡Œæ–¹å¼**:
   - **Single Agg**: ç›´æ¥ `pytest`
   - **Multi-Node Agg**: `srun` + `pytest` + `--test-list`
   - **Multi-Node Disagg**: `submit.py` â†’ `sbatch` â†’ pytest

4. **L0 vs Perf_Test**:
   - **L0_Test.groovy**: ä½¿ç”¨ TestList + å¤æ‚çš„ Slurm é€»è¾‘
   - **Perf_Test.groovy**: ç®€åŒ–ç‰ˆï¼Œç›´æ¥è°ƒç”¨ pytest æˆ– submit.py

5. **test_perf.py çŠ¶æ€**:
   - âŒ å·²ä¸å†ç”¨äº Perf Sanity æµ‹è¯•
   - âš ï¸ å¯èƒ½ä»ç”¨äºå…¶ä»–å®Œæ•´ perf æµ‹è¯•

### æ¨èä½¿ç”¨

**å¯¹äº Perf Sanity æµ‹è¯•**:
- âœ… ä½¿ç”¨ `Perf_Test.groovy` (ç®€åŒ–ç‰ˆ)
- âœ… ç›´æ¥è°ƒç”¨ `test_perf_sanity.py::test_e2e`
- âœ… æ”¯æŒä¸‰ç§æµ‹è¯•æ¨¡å¼

**å¯¹äº L0 æµ‹è¯•**:
- âœ… ç»§ç»­ä½¿ç”¨ `L0_Test.groovy`
- âœ… åŒ…å« Perf Sanity å’Œå…¶ä»–æ‰€æœ‰æµ‹è¯•

---

**æ–‡æ¡£å®Œæˆæ—¶é—´**: 2026-01-31  
**åˆ†æçš„ä»£ç ç‰ˆæœ¬**: TensorRT-LLM main åˆ†æ”¯
