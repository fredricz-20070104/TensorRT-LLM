# TEST_PERF_ADVANCED.md - é€šè¿‡ç»§æ‰¿å®ç° Disagg å’Œ WideEP åŒæ¨¡å¼æ”¯æŒ

> **ç›®æ ‡ï¼šé€šè¿‡ç»§æ‰¿ test_perf_sanity.py åˆ›å»ºæ–°çš„æµ‹è¯•æ¨¡å—ï¼Œæ”¯æŒ Disaggï¼ˆNV SA benchmarkï¼‰å’Œ WideEPï¼ˆæ ‡å‡† benchmarkï¼‰ä¸¤ç§æ¨¡å¼**

---

## ğŸ“‹ èƒŒæ™¯è¯´æ˜

### é—®é¢˜æè¿°

å½“å‰ `test_perf_sanity.py` åªæ”¯æŒä¸€ç§ benchmark æ¨¡å¼ï¼Œæ— æ³•åŒæ—¶å¤„ç†ä¸¤ç§é…ç½®ï¼š

| é…ç½®ç±»å‹ | è·¯å¾„ | Benchmark è„šæœ¬ | æ•°æ®é›†ç±»å‹ | å…³é”®å‚æ•° |
|---------|------|---------------|-----------|---------|
| **Disagg** | `test_configs/disagg/perf/` | `run_benchmark_nv_sa.sh` | éšæœºç”Ÿæˆ | `--dataset-name random` |
| **WideEP** | `test_configs/wideep/perf/` | `run_benchmark.sh` | çœŸå®æ•°æ®é›† | `--dataset-name trtllm_custom` |

### YAML é…ç½®å·®å¼‚

#### Disagg é…ç½®ç¤ºä¾‹

```yaml
benchmark:
  mode: e2e
  use_nv_sa_benchmark: true  # â† å…³é”®æ ‡å¿—
  multi_round: 8
  benchmark_ratio: 0.8
  streaming: true
  concurrency_list: '1024'
  input_length: 1024
  output_length: 1024
  dataset_file: <dataset_file>  # â† ä¸ä½¿ç”¨
```

#### WideEP é…ç½®ç¤ºä¾‹

```yaml
benchmark:
  mode: gen_only
  use_nv_sa_benchmark: false  # â† å…³é”®æ ‡å¿—
  multi_round: 8
  benchmark_ratio: 0.8
  streaming: true
  concurrency_list: '1024'
  input_length: 1024
  output_length: 1024
  dataset_file: <dataset_file>  # â† ä½¿ç”¨çœŸå®æ•°æ®é›†
```

---

## ğŸ¯ æ¨èæ–¹æ¡ˆï¼šç»§æ‰¿æ–¹å¼å®ç°ï¼ˆæœ€ä½³å®è·µï¼‰âœ…

### æ–¹æ¡ˆä¼˜åŠ¿

**ä¼˜ç‚¹ï¼š**
- âœ… **ä¸ä¿®æ”¹åŸå§‹æ–‡ä»¶**ï¼Œå®Œå…¨æ— é£é™©
- âœ… **ä»£ç å¤ç”¨**ï¼Œç»§æ‰¿å¤§éƒ¨åˆ†åŠŸèƒ½
- âœ… **èŒè´£æ¸…æ™°**ï¼ŒåŸæœ‰æµ‹è¯•ä¿æŒä¸å˜
- âœ… **æ˜“äºç»´æŠ¤**ï¼Œåªéœ€ç»´æŠ¤å·®å¼‚éƒ¨åˆ†
- âœ… **çµæ´»åˆ‡æ¢**ï¼Œé€šè¿‡ç¯å¢ƒå˜é‡é€‰æ‹©
- âœ… **å‘åå…¼å®¹**ï¼Œä¸å½±å“ç°æœ‰ CI

**ç¼ºç‚¹ï¼š**
- âš ï¸ éœ€è¦ä¿®æ”¹ Jenkins pipelineï¼ˆæ·»åŠ å‚æ•°é€‰æ‹©ï¼‰
- âš ï¸ éœ€è¦ç†è§£ç»§æ‰¿æœºåˆ¶

---

## ğŸ“ å®Œæ•´å®ç°æ­¥éª¤

### æ­¥éª¤ 1: åˆ›å»ºæ–°çš„æµ‹è¯•æ–‡ä»¶

**æ–‡ä»¶è·¯å¾„ï¼š** `tests/integration/defs/perf/test_perf_advanced.py`

æˆ–

**æ–‡ä»¶è·¯å¾„ï¼š** `tests/integration/defs/perf/test_perf_qa.py`

> å»ºè®®ä½¿ç”¨ `test_perf_advanced.py`ï¼Œå‘½åæ›´æ¸…æ™°

---

### æ–‡ä»¶ç»“æ„

```
tests/integration/defs/perf/
â”œâ”€â”€ test_perf_sanity.py          # åŸå§‹æ–‡ä»¶ï¼ˆä¸ä¿®æ”¹ï¼‰
â”œâ”€â”€ test_perf_advanced.py        # âœ… æ–°æ–‡ä»¶ï¼ˆç»§æ‰¿å®ç°ï¼‰
â””â”€â”€ __init__.py
```

---

### æ­¥éª¤ 2: å®Œæ•´çš„ test_perf_advanced.py å®ç°

**åˆ›å»ºæ–‡ä»¶ï¼š** `tests/integration/defs/perf/test_perf_advanced.py`

```python
"""
Advanced performance test with support for both Disagg and WideEP benchmark modes.

This module extends test_perf_sanity.py to support:
- NV SA Benchmark mode (random dataset) - for Disagg configs
- Standard Benchmark mode (real dataset) - for WideEP configs

Usage:
    # Use via environment variable
    export PERF_TEST_MODULE="perf/test_perf_advanced.py"
    
    # Or specify in pytest command
    pytest perf/test_perf_advanced.py::test_e2e[test_case_name]
"""

import os
import sys
from typing import List, Dict

# å¯¼å…¥åŸå§‹ç±»å’Œå‡½æ•°
from .test_perf_sanity import (
    PerfSanityTestConfig,
    ClientConfig as BaseClientConfig,
    ServerConfig,
    AggrTestCmds,
    DisaggTestCmds,
    DisaggConfig,
    get_model_dir,
    get_dataset_path,
    print_info,
    to_env_dict,
)


# ============================================
# ç»§æ‰¿å¹¶æ‰©å±• ClientConfig
# ============================================
class AdvancedClientConfig(BaseClientConfig):
    """
    Enhanced client config with dual benchmark mode support.
    
    Extends the base ClientConfig to support both:
    - NV SA benchmark (random dataset)
    - Standard benchmark (real dataset from file)
    """
    
    def __init__(
        self, 
        client_config_data: dict, 
        model_name: str, 
        env_vars: str = "",
        use_nv_sa_benchmark: bool = True  # æ–°å¢å‚æ•°
    ):
        """
        Initialize advanced client config.
        
        Args:
            client_config_data: Client configuration dict from YAML
            model_name: Model name
            env_vars: Environment variables string
            use_nv_sa_benchmark: If True, use NV SA benchmark (random);
                               If False, use standard benchmark (dataset file)
        """
        # è°ƒç”¨çˆ¶ç±»æ„é€ å‡½æ•°
        super().__init__(client_config_data, model_name, env_vars)
        
        # æ–°å¢å±æ€§
        self.use_nv_sa_benchmark = use_nv_sa_benchmark
        
        print_info(
            f"[Advanced] Client initialized with benchmark mode: "
            f"{'NV SA (random)' if use_nv_sa_benchmark else 'Standard (dataset)'}"
        )
    
    def to_cmd(self) -> List[str]:
        """
        Generate benchmark command with dual mode support.
        
        Returns:
            List[str]: Benchmark command arguments
        """
        model_dir = get_model_dir(self.model_name)
        self.model_path = model_dir if os.path.exists(model_dir) else self.model_name
        dataset_path = get_dataset_path()
        
        # åŸºç¡€å‘½ä»¤
        benchmark_cmd = [
            "python",
            "-m",
            "tensorrt_llm.serve.scripts.benchmark_serving",
            "--model",
            self.model_path,
            "--tokenizer",
            self.model_path,
        ]
        
        # ========================================
        # æ ¹æ®æ¨¡å¼é€‰æ‹©æ•°æ®é›†é…ç½®
        # ========================================
        if self.use_nv_sa_benchmark:
            # ========================================
            # NV SA Benchmark æ¨¡å¼ï¼ˆDisaggï¼‰
            # ========================================
            # ä½¿ç”¨éšæœºç”Ÿæˆçš„æ•°æ®ï¼Œä¸ä¾èµ–çœŸå®æ•°æ®é›†æ–‡ä»¶
            # å¯¹åº” examples/disaggregated/slurm/benchmark/run_benchmark_nv_sa.sh
            benchmark_cmd.extend([
                "--dataset-name",
                "random",
                "--random-ids",
                "--random-input-len",
                str(self.isl),
                "--random-output-len",
                str(self.osl),
                "--random-range-ratio",
                str(self.random_range_ratio),
            ])
            print_info(
                f"[Advanced] Using NV SA benchmark mode:\n"
                f"  - Dataset: random\n"
                f"  - Input length: {self.isl}\n"
                f"  - Output length: {self.osl}\n"
                f"  - Range ratio: {self.random_range_ratio}"
            )
        else:
            # ========================================
            # æ ‡å‡† Benchmark æ¨¡å¼ï¼ˆWideEPï¼‰
            # ========================================
            # ä½¿ç”¨çœŸå®æ•°æ®é›†æ–‡ä»¶
            # å¯¹åº” examples/disaggregated/slurm/benchmark/run_benchmark.sh
            benchmark_cmd.extend([
                "--dataset-name",
                "trtllm_custom",
            ])
            
            if dataset_path and os.path.exists(dataset_path):
                benchmark_cmd.extend([
                    "--dataset-path",
                    dataset_path,
                ])
                print_info(
                    f"[Advanced] Using standard benchmark mode:\n"
                    f"  - Dataset: trtllm_custom\n"
                    f"  - Dataset path: {dataset_path}"
                )
            else:
                print(
                    f"[Advanced WARNING] Dataset path not found or invalid: {dataset_path}\n"
                    f"Falling back to random mode with specified lengths",
                    file=sys.stderr
                )
                # å›é€€åˆ°éšæœºæ¨¡å¼ï¼ˆä¿æŒ trtllm_custom ä½†æ·»åŠ éšæœºå‚æ•°ï¼‰
                benchmark_cmd.extend([
                    "--random-input-len",
                    str(self.isl),
                    "--random-output-len",
                    str(self.osl),
                ])
        
        # ========================================
        # å…±åŒå‚æ•°ï¼ˆä¸¤ç§æ¨¡å¼éƒ½éœ€è¦ï¼‰
        # ========================================
        benchmark_cmd.extend([
            "--num-prompts",
            str(self.concurrency * self.iterations),
            "--max-concurrency",
            str(self.concurrency),
            "--ignore-eos",
            "--percentile-metrics",
            "ttft,tpot,itl,e2el",
        ])
        
        # å¯é€‰å‚æ•°
        if self.backend:
            benchmark_cmd.extend(["--backend", self.backend])
        if self.use_chat_template:
            benchmark_cmd.append("--use-chat-template")
        if not self.streaming:
            benchmark_cmd.append("--non-streaming")
        if self.trust_remote_code:
            benchmark_cmd.append("--trust-remote-code")
        
        return benchmark_cmd


# ============================================
# ç»§æ‰¿å¹¶æ‰©å±• PerfSanityTestConfig
# ============================================
class AdvancedPerfTestConfig(PerfSanityTestConfig):
    """
    Enhanced performance test config with dual mode support.
    
    Extends PerfSanityTestConfig to create AdvancedClientConfig instances
    instead of base ClientConfig instances.
    """
    
    def _parse_disagg_config_file(self, config_file_path: str, config_file: str):
        """
        Parse YAML config file for disaggregated server with enhanced benchmark support.
        
        This method overrides the parent to:
        1. Read the use_nv_sa_benchmark flag from YAML
        2. Create AdvancedClientConfig instances instead of base ClientConfig
        3. Pass the benchmark mode flag to clients
        """
        import yaml
        import socket
        
        disagg_serving_type = os.environ.get("DISAGG_SERVING_TYPE", "BENCHMARK")
        
        with open(config_file_path) as f:
            config = yaml.safe_load(f)
        
        # æå–å„éƒ¨åˆ†é…ç½®
        metadata = config.get("metadata", {})
        hardware = config.get("hardware", {})
        benchmark = config.get("benchmark", {})
        environment = config.get("environment", {})
        worker_config = config.get("worker_config", {})
        
        # âœ… å…³é”®ï¼šè¯»å– use_nv_sa_benchmark æ ‡å¿—
        use_nv_sa_benchmark = benchmark.get("use_nv_sa_benchmark", True)  # é»˜è®¤ Trueï¼ˆå‘åå…¼å®¹ï¼‰
        
        print_info(
            f"[Advanced] Disagg config parsed:\n"
            f"  - Config file: {config_file}\n"
            f"  - Benchmark mode: {'NV SA (random)' if use_nv_sa_benchmark else 'Standard (dataset)'}\n"
            f"  - Serving type: {disagg_serving_type}"
        )
        
        # æå–å…¶ä»–é…ç½®
        model_name = metadata.get("model_name", "")
        gpus_per_node = hardware.get("gpus_per_node", 0)
        
        worker_env_var = environment.get("worker_env_var", "")
        server_env_var = environment.get("server_env_var", "")
        client_env_var = environment.get("client_env_var", "")
        
        # è§£æ concurrency_list
        concurrency_str = benchmark.get("concurrency_list", "1")
        if isinstance(concurrency_str, str):
            concurrency_values = [int(x) for x in concurrency_str.split()]
        elif isinstance(concurrency_str, list):
            concurrency_values = [int(x) for x in concurrency_str]
        else:
            concurrency_values = [int(concurrency_str)]
        
        # åˆ›å»º server configsï¼ˆå¤ç”¨çˆ¶ç±»é€»è¾‘ï¼‰
        config_file_base_name = os.path.splitext(os.path.basename(config_file))[0]
        
        ctx_server_config_data = {
            "concurrency": max(concurrency_values),
            "name": config_file_base_name,
            "model_name": model_name,
            "gpus_per_node": gpus_per_node,
            "disagg_run_type": "ctx",
            **worker_config.get("ctx", {}),
        }
        
        gen_server_config_data = {
            "concurrency": max(concurrency_values),
            "name": config_file_base_name,
            "model_name": model_name,
            "gpus_per_node": gpus_per_node,
            "disagg_run_type": "gen",
            **worker_config.get("gen", {}),
        }
        
        ctx_server_config = ServerConfig(ctx_server_config_data, worker_env_var)
        gen_server_config = ServerConfig(gen_server_config_data, worker_env_var)
        
        # âœ… åˆ›å»º client configsï¼ˆä½¿ç”¨å¢å¼ºç‰ˆï¼‰
        client_configs = []
        for concurrency in concurrency_values:
            client_config_data = {
                "concurrency": concurrency,
                "iterations": benchmark.get("multi_round", 8),
                "isl": benchmark.get("input_length", 1024),
                "osl": benchmark.get("output_length", 1024),
                "random_range_ratio": benchmark.get("benchmark_ratio", 0.8),
                "backend": "openai",
                "use_chat_template": False,
                "streaming": benchmark.get("streaming", True),
                "trust_remote_code": True,
            }
            
            # âœ… ä½¿ç”¨ AdvancedClientConfig å¹¶ä¼ é€’ use_nv_sa_benchmark
            client_config = AdvancedClientConfig(
                client_config_data,
                model_name,
                client_env_var,
                use_nv_sa_benchmark=use_nv_sa_benchmark  # â† ä¼ é€’æ ‡å¿—
            )
            client_configs.append(client_config)
        
        # åˆ›å»º disagg config
        disagg_config = DisaggConfig(
            name=config_file_base_name,
            disagg_serving_type=disagg_serving_type,
            hostname=socket.gethostname(),
            numa_bind=benchmark.get("numa_bind", False),
            timeout=benchmark.get("timeout", 600),
            benchmark_mode=benchmark.get("mode", "e2e"),
            model_name=model_name,
            hardware=hardware,
            server_env_var=server_env_var,
        )
        
        # åˆ›å»ºå‘½ä»¤å…ƒç»„åˆ—è¡¨
        server_cmds = []
        client_cmds = {}
        
        for server_idx in range(1):  # ç®€åŒ–ï¼šåªæ”¯æŒå•ä¸ª server ç»„åˆ
            ctx_cmd = ctx_server_config.to_cmd(self.perf_sanity_output_dir, benchmark.get("numa_bind", False), "CTX")
            gen_cmd = gen_server_config.to_cmd(self.perf_sanity_output_dir, benchmark.get("numa_bind", False), "GEN")
            
            # Disagg coordinator å‘½ä»¤
            disagg_cmd = [
                "trtllm-serve-coordinator",
                "--config", os.path.join(self.perf_sanity_output_dir, f"server_config.{server_idx}.yaml"),
            ]
            
            server_cmds.append((ctx_cmd, gen_cmd, disagg_cmd))
            client_cmds[server_idx] = [client.to_cmd() for client in client_configs]
        
        return DisaggTestCmds(
            server_cmds=server_cmds,
            client_cmds=client_cmds,
            timeout=disagg_config.timeout,
            hostname=disagg_config.hostname,
            disagg_serving_type=disagg_config.disagg_serving_type,
            num_ctx_servers=hardware.get("num_ctx_servers", 1),
            num_gen_servers=hardware.get("num_gen_servers", 1),
            output_dir=self.perf_sanity_output_dir,
        )
    
    def _parse_aggr_config_file(self, config_file_path: str, config_file: str, selected_server_names=None):
        """
        Parse YAML config file for aggregated server with enhanced benchmark support.
        
        Overrides parent method to use AdvancedClientConfig.
        """
        import yaml
        
        with open(config_file_path) as f:
            config = yaml.safe_load(f)
        
        # æå–é…ç½®
        metadata = config.get("metadata", {})
        hardware = config.get("hardware", {})
        benchmark = config.get("benchmark", {})
        environment = config.get("environment", {})
        
        # âœ… è¯»å– use_nv_sa_benchmark æ ‡å¿—
        use_nv_sa_benchmark = benchmark.get("use_nv_sa_benchmark", True)
        
        print_info(
            f"[Advanced] Aggr config parsed:\n"
            f"  - Config file: {config_file}\n"
            f"  - Benchmark mode: {'NV SA (random)' if use_nv_sa_benchmark else 'Standard (dataset)'}"
        )
        
        model_name = metadata.get("model_name", "")
        gpus_per_node = hardware.get("gpus_per_node", 0)
        server_env_var = environment.get("server_env_var", "")
        client_env_var = environment.get("client_env_var", "")
        
        # åˆ›å»º server configs
        server_configs = []
        server_client_configs = {}
        
        for server_idx, server_config_data in enumerate(config["server_configs"]):
            # æ£€æŸ¥æ˜¯å¦åº”è¯¥åŒ…å«æ­¤ server
            if (
                selected_server_names is not None
                and server_config_data.get("name") not in selected_server_names
            ):
                continue
            
            server_config_data["model_name"] = (
                model_name
                if "model_name" not in server_config_data
                else server_config_data["model_name"]
            )
            server_config_data["gpus_per_node"] = gpus_per_node
            
            server_config = ServerConfig(server_config_data, server_env_var)
            server_id = len(server_configs)
            server_configs.append(server_config)
            
            # âœ… åˆ›å»º client configsï¼ˆä½¿ç”¨å¢å¼ºç‰ˆï¼‰
            client_configs = []
            for client_config_data in server_config_data["client_configs"]:
                client_config = AdvancedClientConfig(
                    client_config_data,
                    server_config_data["model_name"],
                    client_env_var,
                    use_nv_sa_benchmark=use_nv_sa_benchmark  # â† ä¼ é€’æ ‡å¿—
                )
                client_configs.append(client_config)
            
            server_client_configs[server_id] = client_configs
        
        self.server_configs = server_configs
        self.server_client_configs = server_client_configs


# ============================================
# Pytest å…¥å£å‡½æ•°
# ============================================
def test_e2e(test_case_name, request):
    """
    Advanced E2E performance test with dual benchmark mode support.
    
    This test function uses AdvancedPerfTestConfig which automatically
    selects the appropriate benchmark mode based on YAML configuration.
    
    Args:
        test_case_name: Test case name in format "prefix-config_name"
        request: Pytest request fixture
    """
    print_info(
        f"========================================\n"
        f"[Advanced] Starting enhanced performance test\n"
        f"  Test case: {test_case_name}\n"
        f"========================================"
    )
    
    # ä½¿ç”¨å¢å¼ºç‰ˆé…ç½®
    config = AdvancedPerfTestConfig(
        test_case_name=test_case_name,
        output_dir=request.config.getoption("--output-dir"),
        perf_sanity_test_prefix=request.config.getoption("--test-prefix"),
    )
    
    # æ‰§è¡Œæµ‹è¯•ï¼ˆå¤ç”¨çˆ¶ç±»é€»è¾‘ï¼‰
    config.parse_config_file()
    commands = config.get_commands()
    outputs = config.run_ex(commands)
    
    # åªæœ‰ BENCHMARK èŠ‚ç‚¹æ‰æ”¶é›†æ€§èƒ½æ•°æ®
    disagg_serving_type = os.environ.get("DISAGG_SERVING_TYPE", "BENCHMARK")
    if disagg_serving_type == "BENCHMARK":
        config.get_perf_result(outputs)
        config.upload_test_results_to_database()
    
    print_info(
        f"========================================\n"
        f"[Advanced] Performance test completed\n"
        f"  Test case: {test_case_name}\n"
        f"========================================"
    )
```

---

### æ­¥éª¤ 3: ä¿®æ”¹ run_disagg_test.sh æ”¯æŒæ¨¡å—é€‰æ‹©

**æ–‡ä»¶ï¼š** `jenkins_test/scripts/run_disagg_test.sh`

**ä¿®æ”¹ä½ç½®ï¼š** æ­¥éª¤ 4ï¼ˆçº¦ 250 è¡Œï¼‰

**å·²ç»å®Œæˆçš„ä¿®æ”¹ï¼š**

```bash
# ä»ç¯å¢ƒå˜é‡è¯»å–è‡ªå®šä¹‰æµ‹è¯•æ¨¡å—é…ç½®ï¼ˆå¯é€‰ï¼‰
PERF_TEST_MODULE="${PERF_TEST_MODULE:-perf/test_perf_sanity.py}"
PERF_TEST_FUNCTION="${PERF_TEST_FUNCTION:-test_e2e}"
PERF_TEST_PREFIX="${PERF_TEST_PREFIX:-disagg_upload}"

echo "æµ‹è¯•æ¨¡å—é…ç½®:"
echo "  æµ‹è¯•æ¨¡å—: $PERF_TEST_MODULE"
echo "  æµ‹è¯•å‡½æ•°: $PERF_TEST_FUNCTION"
echo "  æµ‹è¯•å‰ç¼€: $PERF_TEST_PREFIX"
```

**âœ… æ— éœ€é¢å¤–ä¿®æ”¹**ï¼Œå› ä¸ºæ¨¡å—è·¯å¾„ä¼šè‡ªåŠ¨ä¼ é€’ç»™ pytestã€‚

---

### æ­¥éª¤ 4: ä¿®æ”¹ Jenkins Pipeline æ·»åŠ æ¨¡å—é€‰æ‹©

**æ–‡ä»¶ï¼š** `jenkins_test/Perf_Test.groovy`

**æ·»åŠ å‚æ•°é€‰æ‹©ï¼š**

```groovy
parameters {
    // ... å…¶ä»–ç°æœ‰å‚æ•° ...
    
    choice(
        name: 'PERF_TEST_MODULE',
        choices: [
            'perf/test_perf_sanity.py',     // é»˜è®¤ï¼šåŸå§‹å®ç°
            'perf/test_perf_advanced.py',   // å¢å¼ºï¼šæ”¯æŒåŒæ¨¡å¼
            'perf/test_perf_qa.py'          // QAï¼šå¯é€‰çš„å¦ä¸€ä¸ªå®ç°
        ],
        description: 'æ€§èƒ½æµ‹è¯•æ¨¡å—é€‰æ‹©ï¼ˆadvanced æ”¯æŒ Disagg å’Œ WideEP åŒæ¨¡å¼ï¼‰'
    )
    
    string(
        name: 'PERF_TEST_FUNCTION',
        defaultValue: 'test_e2e',
        description: 'æ€§èƒ½æµ‹è¯•å‡½æ•°å'
    )
    
    string(
        name: 'PERF_TEST_PREFIX',
        defaultValue: 'disagg_upload',
        description: 'æµ‹è¯•åç§°å‰ç¼€'
    )
}

environment {
    // ... å…¶ä»–ç¯å¢ƒå˜é‡ ...
    
    PERF_TEST_MODULE = "${params.PERF_TEST_MODULE ?: 'perf/test_perf_sanity.py'}"
    PERF_TEST_FUNCTION = "${params.PERF_TEST_FUNCTION ?: 'test_e2e'}"
    PERF_TEST_PREFIX = "${params.PERF_TEST_PREFIX ?: 'disagg_upload'}"
}

// åœ¨æ‰§è¡Œé˜¶æ®µå¯¼å‡ºç¯å¢ƒå˜é‡ï¼ˆsync_and_run.sh éƒ¨åˆ†ï¼‰
stage('Run Tests') {
    steps {
        script {
            sh """
                export PERF_TEST_MODULE='${env.PERF_TEST_MODULE}'
                export PERF_TEST_FUNCTION='${env.PERF_TEST_FUNCTION}'
                export PERF_TEST_PREFIX='${env.PERF_TEST_PREFIX}'
                
                # è°ƒç”¨ sync_and_run.sh
                ${WORKSPACE}/jenkins_test/scripts/sync_and_run.sh \\
                    --cluster ${CLUSTER} \\
                    --trtllm-dir ${TRTLLM_DIR} \\
                    --testlist ${TESTLIST}
            """
        }
    }
}
```

---

### æ­¥éª¤ 5: æµ‹è¯•å’ŒéªŒè¯

#### 5.1 æœ¬åœ°æµ‹è¯•

**æµ‹è¯• Disagg é…ç½®ï¼ˆNV SA æ¨¡å¼ï¼‰ï¼š**

```bash
# è®¾ç½®ç¯å¢ƒå˜é‡ä½¿ç”¨å¢å¼ºç‰ˆ
export PERF_TEST_MODULE="perf/test_perf_advanced.py"
export PERF_TEST_FUNCTION="test_e2e"
export PERF_TEST_PREFIX="disagg_upload"

# è¿è¡Œ Disagg é…ç½®
pytest tests/integration/defs/perf/test_perf_advanced.py::test_e2e \
    -k "disagg_upload-deepseek-r1-fp4_1k1k_ctx1_gen1_dep8_bs768_eplb0_mtp0_ccb-UCX" \
    -vv

# æ£€æŸ¥æ—¥å¿—ï¼Œåº”è¯¥çœ‹åˆ°ï¼š
# [Advanced] Benchmark mode: NV SA (random)
# [Advanced] Using NV SA benchmark mode:
#   - Dataset: random
#   - Input length: 1024
#   - Output length: 1024
```

**æµ‹è¯• WideEP é…ç½®ï¼ˆæ ‡å‡†æ¨¡å¼ï¼‰ï¼š**

```bash
# è¿è¡Œ WideEP é…ç½®
pytest tests/integration/defs/perf/test_perf_advanced.py::test_e2e \
    -k "disagg_upload-deepseek-r1-fp4_1k1k_ctx1_gen1_dep32_bs32_eplb288_mtp0_ccb-UCX" \
    -vv

# æ£€æŸ¥æ—¥å¿—ï¼Œåº”è¯¥çœ‹åˆ°ï¼š
# [Advanced] Benchmark mode: Standard (dataset)
# [Advanced] Using standard benchmark mode:
#   - Dataset: trtllm_custom
#   - Dataset path: /path/to/dataset.json
```

---

#### 5.2 é€šè¿‡ run_disagg_test.sh æµ‹è¯•

```bash
# æµ‹è¯• Disagg é…ç½®
export PERF_TEST_MODULE="perf/test_perf_advanced.py"

./jenkins_test/scripts/run_disagg_test.sh \
    --trtllm-dir /path/to/TensorRT-LLM \
    --config-file deepseek-r1-fp4_1k1k_ctx1_gen1_dep8_bs768_eplb0_mtp0_ccb-UCX \
    --workspace /tmp/test_disagg \
    --dry-run

# æ£€æŸ¥ç”Ÿæˆçš„å‘½ä»¤
cat /tmp/test_disagg/slurm_launch_prefix.sh | grep pytestCommand
# åº”è¯¥åŒ…å« perf/test_perf_advanced.py
```

```bash
# æµ‹è¯• WideEP é…ç½®
export PERF_TEST_MODULE="perf/test_perf_advanced.py"

./jenkins_test/scripts/run_disagg_test.sh \
    --trtllm-dir /path/to/TensorRT-LLM \
    --config-file deepseek-r1-fp4_1k1k_ctx1_gen1_dep32_bs32_eplb288_mtp0_ccb-UCX \
    --workspace /tmp/test_wideep \
    --dry-run

# æ£€æŸ¥ç”Ÿæˆçš„å‘½ä»¤
cat /tmp/test_wideep/slurm_launch_prefix.sh | grep pytestCommand
```

---

#### 5.3 CI ç¯å¢ƒæµ‹è¯•

**åœ¨ Jenkins ä¸­é€‰æ‹©å‚æ•°ï¼š**

| å‚æ•° | å€¼ | è¯´æ˜ |
|------|-----|------|
| `PERF_TEST_MODULE` | `perf/test_perf_advanced.py` | ä½¿ç”¨å¢å¼ºç‰ˆ |
| `PERF_TEST_FUNCTION` | `test_e2e` | æµ‹è¯•å‡½æ•° |
| `PERF_TEST_PREFIX` | `disagg_upload` | æµ‹è¯•å‰ç¼€ |
| `TESTLIST` | ä½ çš„ testlist åç§° | æµ‹è¯•åˆ—è¡¨ |

---

### æ­¥éª¤ 6: éªŒè¯æ¸…å•

#### åŠŸèƒ½éªŒè¯

- [ ] **Disagg é…ç½®**ä½¿ç”¨ `--dataset-name random`
- [ ] **WideEP é…ç½®**ä½¿ç”¨ `--dataset-name trtllm_custom`
- [ ] æ—¥å¿—æ­£ç¡®æ˜¾ç¤ºä½¿ç”¨çš„ benchmark æ¨¡å¼
- [ ] ä¸¤ç§æ¨¡å¼éƒ½èƒ½æ­£ç¡®æ”¶é›†æ€§èƒ½æ•°æ®
- [ ] æ€§èƒ½æ•°æ®æ­£ç¡®ä¸Šä¼ åˆ° OpenSearch

#### å‘åå…¼å®¹éªŒè¯

- [ ] ä¸è®¾ç½® `PERF_TEST_MODULE` æ—¶ï¼Œé»˜è®¤ä½¿ç”¨ `test_perf_sanity.py`
- [ ] åŸæœ‰çš„ CI ä½œä¸šä¸å—å½±å“
- [ ] ç°æœ‰çš„ Disagg æµ‹è¯•ä»ç„¶æ­£å¸¸å·¥ä½œ

#### é”™è¯¯å¤„ç†

- [ ] WideEP é…ç½®ç¼ºå°‘æ•°æ®é›†æ–‡ä»¶æ—¶æœ‰æ¸…æ™°æç¤º
- [ ] è‡ªåŠ¨å›é€€åˆ°éšæœºæ¨¡å¼ï¼ˆå¦‚æœé…ç½®ï¼‰
- [ ] YAML é…ç½®ç¼ºå°‘ `use_nv_sa_benchmark` æ—¶ä½¿ç”¨é»˜è®¤å€¼ï¼ˆTrueï¼‰

---

## ğŸ“Š ç»§æ‰¿æ–¹æ¡ˆ vs ç›´æ¥ä¿®æ”¹å¯¹æ¯”

| å¯¹æ¯”é¡¹ | ç»§æ‰¿æ–¹æ¡ˆï¼ˆæœ¬æ–¹æ¡ˆï¼‰âœ… | ç›´æ¥ä¿®æ”¹ test_perf_sanity.py |
|--------|-------------------|----------------------------|
| **ä¿®æ”¹åŸæ–‡ä»¶** | âŒ ä¸ä¿®æ”¹ | âœ… éœ€è¦ä¿®æ”¹ |
| **é£é™©** | âœ… ä½ï¼ˆå®Œå…¨ç‹¬ç«‹ï¼‰ | âš ï¸ ä¸­ï¼ˆå¯èƒ½å½±å“ç°æœ‰åŠŸèƒ½ï¼‰ |
| **ä»£ç å¤ç”¨** | âœ… é«˜ï¼ˆç»§æ‰¿å¤§éƒ¨åˆ†ï¼‰ | âœ… é«˜ï¼ˆå…±äº«ä»£ç ï¼‰ |
| **ç»´æŠ¤æˆæœ¬** | âš ï¸ ä¸­ï¼ˆä¸¤ä¸ªæ–‡ä»¶ï¼‰ | âœ… ä½ï¼ˆä¸€ä¸ªæ–‡ä»¶ï¼‰ |
| **çµæ´»æ€§** | âœ… é«˜ï¼ˆå¯å¹¶å­˜ï¼‰ | âš ï¸ ä¸­ï¼ˆå¿…é¡»å…¼å®¹æ‰€æœ‰ï¼‰ |
| **æµ‹è¯•å¤æ‚åº¦** | âœ… ä½ï¼ˆç‹¬ç«‹æµ‹è¯•ï¼‰ | âš ï¸ é«˜ï¼ˆéœ€è¦å…¨é¢å›å½’ï¼‰ |
| **Jenkins ä¿®æ”¹** | âš ï¸ éœ€è¦ï¼ˆæ·»åŠ å‚æ•°ï¼‰ | âœ… ä¸éœ€è¦ |
| **æ¨èåº¦** | â­â­â­â­â­ | â­â­â­ |

---

## ğŸ¯ å®æ–½è·¯çº¿å›¾

### é˜¶æ®µ 1: åˆ›å»ºæ–°æ–‡ä»¶ï¼ˆ1 å¤©ï¼‰

1. âœ… åˆ›å»º `test_perf_advanced.py`
2. âœ… å®ç° `AdvancedClientConfig` ç±»
3. âœ… å®ç° `AdvancedPerfTestConfig` ç±»
4. âœ… å®ç° `test_e2e()` å‡½æ•°
5. âœ… æœ¬åœ°å•å…ƒæµ‹è¯•

### é˜¶æ®µ 2: é›†æˆæµ‹è¯•ï¼ˆ1 å¤©ï¼‰

1. âœ… ä¿®æ”¹ `run_disagg_test.sh` æ”¯æŒæ¨¡å—é€‰æ‹©ï¼ˆå·²å®Œæˆï¼‰
2. âœ… æµ‹è¯• Disagg é…ç½®
3. âœ… æµ‹è¯• WideEP é…ç½®
4. âœ… éªŒè¯å‘åå…¼å®¹æ€§

### é˜¶æ®µ 3: Jenkins é›†æˆï¼ˆ0.5 å¤©ï¼‰

1. âœ… ä¿®æ”¹ `Perf_Test.groovy` æ·»åŠ å‚æ•°
2. âœ… æµ‹è¯• Jenkins ä½œä¸š
3. âœ… éªŒè¯ç¯å¢ƒå˜é‡ä¼ é€’

### é˜¶æ®µ 4: æ–‡æ¡£å’Œéƒ¨ç½²ï¼ˆ0.5 å¤©ï¼‰

1. âœ… æ›´æ–°ä½¿ç”¨æ–‡æ¡£
2. âœ… åˆ›å»ºç¤ºä¾‹é…ç½®
3. âœ… åŸ¹è®­å›¢é˜Ÿæˆå‘˜

**æ€»æ—¶é—´ï¼š** çº¦ 3 å¤©

---

## ğŸ“š ä½¿ç”¨æŒ‡å—

### åœºæ™¯ 1: ä½¿ç”¨åŸå§‹å®ç°ï¼ˆé»˜è®¤ï¼‰

```bash
# ä¸è®¾ç½®ä»»ä½•ç¯å¢ƒå˜é‡ï¼Œé»˜è®¤ä½¿ç”¨ test_perf_sanity.py
./jenkins_test/scripts/run_disagg_test.sh \
    --trtllm-dir /path/to/TensorRT-LLM \
    --config-file your-config \
    --workspace /tmp/test
```

### åœºæ™¯ 2: ä½¿ç”¨å¢å¼ºç‰ˆï¼ˆDisagg é…ç½®ï¼‰

```bash
# è®¾ç½®ç¯å¢ƒå˜é‡ä½¿ç”¨ test_perf_advanced.py
export PERF_TEST_MODULE="perf/test_perf_advanced.py"

./jenkins_test/scripts/run_disagg_test.sh \
    --trtllm-dir /path/to/TensorRT-LLM \
    --config-file deepseek-r1-fp4_1k1k_ctx1_gen1_dep8_bs768_eplb0_mtp0_ccb-UCX \
    --workspace /tmp/test

# YAML é…ç½®ä¸­ use_nv_sa_benchmark: true
# è‡ªåŠ¨ä½¿ç”¨éšæœºæ•°æ®é›†
```

### åœºæ™¯ 3: ä½¿ç”¨å¢å¼ºç‰ˆï¼ˆWideEP é…ç½®ï¼‰

```bash
# è®¾ç½®ç¯å¢ƒå˜é‡ä½¿ç”¨ test_perf_advanced.py
export PERF_TEST_MODULE="perf/test_perf_advanced.py"

./jenkins_test/scripts/run_disagg_test.sh \
    --trtllm-dir /path/to/TensorRT-LLM \
    --config-file deepseek-r1-fp4_1k1k_ctx1_gen1_dep32_bs32_eplb288_mtp0_ccb-UCX \
    --workspace /tmp/test

# YAML é…ç½®ä¸­ use_nv_sa_benchmark: false
# è‡ªåŠ¨ä½¿ç”¨çœŸå®æ•°æ®é›†
```

### åœºæ™¯ 4: Jenkins Pipeline ä½¿ç”¨

åœ¨ Jenkins æ„å»ºæ—¶é€‰æ‹©å‚æ•°ï¼š

1. é€‰æ‹© `PERF_TEST_MODULE` = `perf/test_perf_advanced.py`
2. é€‰æ‹©ä½ çš„ `TESTLIST`
3. ç‚¹å‡»æ„å»º

Jenkins ä¼šè‡ªåŠ¨è®¾ç½®ç¯å¢ƒå˜é‡å¹¶è°ƒç”¨æ­£ç¡®çš„æµ‹è¯•æ¨¡å—ã€‚

---

## ğŸ” å¸¸è§é—®é¢˜

### Q1: ä¸ºä»€ä¹ˆé€‰æ‹©ç»§æ‰¿è€Œä¸æ˜¯ç›´æ¥ä¿®æ”¹ï¼Ÿ

**A:** ç»§æ‰¿æ–¹æ¡ˆæœ‰ä»¥ä¸‹ä¼˜åŠ¿ï¼š
- âœ… é›¶é£é™©ï¼šä¸å½±å“ç°æœ‰åŠŸèƒ½
- âœ… çµæ´»ï¼šå¯ä»¥åŒæ—¶ä¿ç•™ä¸¤ç§å®ç°
- âœ… æ¸è¿›ï¼šå¯ä»¥é€æ­¥è¿ç§»æµ‹è¯•
- âœ… æ¸…æ™°ï¼šèŒè´£åˆ†ç¦»ï¼Œä»£ç æ˜“è¯»

### Q2: test_perf_advanced.py å’Œ test_perf_sanity.py å¯ä»¥å¹¶å­˜å—ï¼Ÿ

**A:** å¯ä»¥ï¼è¿™æ˜¯ç»§æ‰¿æ–¹æ¡ˆçš„ä¼˜åŠ¿ï¼š
- åŸæœ‰æµ‹è¯•ç»§ç»­ä½¿ç”¨ `test_perf_sanity.py`
- æ–°æµ‹è¯•æˆ–éœ€è¦åŒæ¨¡å¼çš„æµ‹è¯•ä½¿ç”¨ `test_perf_advanced.py`
- é€šè¿‡ç¯å¢ƒå˜é‡æˆ– Jenkins å‚æ•°é€‰æ‹©

### Q3: å¦‚ä½•ç¡®ä¿ test_perf_advanced.py ä¸ test_perf_sanity.py ä¿æŒåŒæ­¥ï¼Ÿ

**A:** ç”±äºä½¿ç”¨ç»§æ‰¿ï¼Œå¤§éƒ¨åˆ†ä»£ç è‡ªåŠ¨åŒæ­¥ï¼š
- âœ… åªé‡å†™äº† 2 ä¸ªæ–¹æ³•ï¼š`to_cmd()` å’Œé…ç½®è§£æ
- âœ… å…¶ä»–åŠŸèƒ½ï¼ˆæœåŠ¡å™¨å¯åŠ¨ã€æ€§èƒ½æ”¶é›†ã€ä¸Šä¼ ç­‰ï¼‰å®Œå…¨å¤ç”¨çˆ¶ç±»
- âœ… çˆ¶ç±»æ›´æ–°è‡ªåŠ¨ç”Ÿæ•ˆ

### Q4: å¦‚æœ YAML é…ç½®ç¼ºå°‘ `use_nv_sa_benchmark`ï¼Œä¼šæ€æ ·ï¼Ÿ

**A:** é»˜è®¤ä½¿ç”¨ NV SA æ¨¡å¼ï¼ˆå‘åå…¼å®¹ï¼‰ï¼š
```python
use_nv_sa_benchmark = benchmark.get("use_nv_sa_benchmark", True)  # é»˜è®¤ True
```

### Q5: å¯ä»¥åˆ›å»º test_perf_qa.py å—ï¼Ÿ

**A:** å¯ä»¥ï¼ç»§æ‰¿æ–¹æ¡ˆæ”¯æŒå¤šä¸ªå®ç°ï¼š
```python
# tests/integration/defs/perf/test_perf_qa.py
from .test_perf_advanced import AdvancedClientConfig, AdvancedPerfTestConfig

class QAClientConfig(AdvancedClientConfig):
    # QA ç‰¹å®šçš„æ‰©å±•
    pass

class QAPerfTestConfig(AdvancedPerfTestConfig):
    # QA ç‰¹å®šçš„æ‰©å±•
    pass
```

---

## ğŸ“ ç›¸å…³æ–‡æ¡£

1. **å·®å¼‚åˆ†æ**: `jenkins_test/docs/DISAGG_VS_WIDEEP_ANALYSIS.md`
2. **run_disagg_test.sh æ›´æ–°**: `jenkins_test/docs/RUN_DISAGG_TEST_UPDATE.md`
3. **è‡ªå®šä¹‰æµ‹è¯•æ¨¡å—æŒ‡å—**: `jenkins_test/docs/CUSTOM_PERF_TEST_GUIDE.md`
4. **åŸå§‹æµ‹è¯•å®ç°**: `tests/integration/defs/perf/test_perf_sanity.py`

---

## âœ… æ€»ç»“

### æ ¸å¿ƒä¼˜åŠ¿

1. âœ… **é›¶é£é™©**ï¼šä¸ä¿®æ”¹åŸæ–‡ä»¶ï¼Œå®Œå…¨ç‹¬ç«‹
2. âœ… **é«˜å¤ç”¨**ï¼šç»§æ‰¿å¤§éƒ¨åˆ†ä»£ç ï¼Œåªé‡å†™å·®å¼‚éƒ¨åˆ†
3. âœ… **æ˜“ç»´æŠ¤**ï¼šæ¸…æ™°çš„èŒè´£åˆ†ç¦»
4. âœ… **çµæ´»åˆ‡æ¢**ï¼šé€šè¿‡ç¯å¢ƒå˜é‡æˆ– Jenkins å‚æ•°é€‰æ‹©
5. âœ… **å‘åå…¼å®¹**ï¼šä¸å½±å“ç°æœ‰ CI ä½œä¸š

### å…³é”®æ–‡ä»¶

| æ–‡ä»¶ | ä½œç”¨ | æ˜¯å¦ä¿®æ”¹ |
|------|------|---------|
| `test_perf_sanity.py` | åŸå§‹å®ç° | âŒ ä¸ä¿®æ”¹ |
| `test_perf_advanced.py` | å¢å¼ºå®ç°ï¼ˆæ–°å»ºï¼‰ | âœ… åˆ›å»º |
| `run_disagg_test.sh` | æµ‹è¯•è„šæœ¬ | âœ… å°ä¿®æ”¹ï¼ˆå·²å®Œæˆï¼‰ |
| `Perf_Test.groovy` | Jenkins pipeline | âœ… æ·»åŠ å‚æ•° |

### å®æ–½å»ºè®®

**æ¨èè·¯çº¿ï¼š**
1. åˆ›å»º `test_perf_advanced.py`ï¼ˆ1 å¤©ï¼‰
2. æœ¬åœ°æµ‹è¯•éªŒè¯ï¼ˆ0.5 å¤©ï¼‰
3. Jenkins é›†æˆï¼ˆ0.5 å¤©ï¼‰
4. æ–‡æ¡£å’ŒåŸ¹è®­ï¼ˆ0.5 å¤©ï¼‰

**æ€»æ—¶é—´ï¼š** 2-3 å¤©å³å¯å®Œæˆ

---

**ç°åœ¨ä½ æœ‰å®Œæ•´çš„å®ç°æ–¹æ¡ˆäº†ï¼éœ€è¦æˆ‘å¸®ä½ å®é™…åˆ›å»º `test_perf_advanced.py` æ–‡ä»¶å—ï¼Ÿ** ğŸš€
