# æ¾„æ¸…ï¼štest_perf_sanity.py ä¸ä½¿ç”¨ trtllm-llmapi-launch

> **é‡è¦å‘ç°ï¼šä½ æ˜¯å¯¹çš„ï¼test_perf_sanity.py ç¡®å®æ²¡æœ‰ä½¿ç”¨ trtllm-llmapi-launchï¼**

---

## ğŸ¯ æ ¸å¿ƒå‘ç°

### test_perf_sanity.py ä½¿ç”¨ `trtllm-serve` è€Œä¸æ˜¯ `trtllm-llmapi-launch`

**ä»£ç è¯æ®ï¼ˆtest_perf_sanity.py:250-258ï¼‰ï¼š**

```python
def to_cmd(
    self, output_dir: str, numa_bind: bool = False, disagg_serving_type: str = ""
) -> List[str]:
    """Generate server command."""
    model_dir = get_model_dir(self.model_name)
    self.model_path = model_dir if os.path.exists(model_dir) else self.model_name
    config_filename = f"extra-llm-api-config.{self.disagg_run_type}.{self.name}.yml"
    config_path = os.path.join(output_dir, config_filename)

    numa_bind_cmd = []
    if numa_bind:
        numa_bind_cmd = ["numactl", "-m 0,1"]

    cmd = numa_bind_cmd + [
        "trtllm-serve",  # â† ä½¿ç”¨ trtllm-serveï¼Œä¸æ˜¯ trtllm-llmapi-launchï¼
        self.model_path,
        "--backend",
        "pytorch",
        "--config",
        config_path,
    ]
    return cmd
```

---

## ğŸ“Š å®Œæ•´çš„å¯åŠ¨æµç¨‹åˆ†æ

### disaggregated æ¨¡å¼ä¸‹çš„å®é™…æ‰§è¡Œ

#### 1. slurm_launch_draft.sh å¯åŠ¨å¤šä¸ªç»„ä»¶

```bash
# GEN servers
for ((i=0; i<$numGenServers; i++)); do
    export DISAGG_SERVING_TYPE="GEN_$i"
    export pytestCommand="$pytestCommandWorker"
    srun ... slurm_run.sh  # â† æ¯ä¸ª srun å¯åŠ¨ä¸€ä¸ª pytest è¿›ç¨‹
done

# CTX servers  
for ((i=0; i<$numCtxServers; i++)); do
    export DISAGG_SERVING_TYPE="CTX_$i"
    export pytestCommand="$pytestCommandWorker"
    srun ... slurm_run.sh  # â† æ¯ä¸ª srun å¯åŠ¨ä¸€ä¸ª pytest è¿›ç¨‹
done

# DISAGG_SERVER
export DISAGG_SERVING_TYPE="DISAGG_SERVER"
export pytestCommand="$pytestCommandDisaggServer"
srun ... slurm_run.sh

# BENCHMARK
export DISAGG_SERVING_TYPE="BENCHMARK"
export pytestCommand="$pytestCommandBenchmark"
srun ... slurm_run.sh
```

#### 2. æ¯ä¸ª slurm_run.sh æ‰§è¡Œ pytest

```bash
# slurm_run.sh
eval $pytestCommand
# å®é™…æ‰§è¡Œ: pytest perf/test_perf_sanity.py::test_e2e[disagg_upload-deepseek-r1-fp4]
```

#### 3. pytest è°ƒç”¨ test_perf_sanity.py::test_e2e

```python
# test_perf_sanity.py
def test_e2e(test_case_name, request):
    config = PerfSanityTestConfig(...)
    config.parse_config_file()
    commands = config.get_commands()  # â† ç”Ÿæˆæ‰€æœ‰æœåŠ¡å™¨/å®¢æˆ·ç«¯å‘½ä»¤
    outputs = config.run_ex(commands)  # â† æ‰§è¡Œå‘½ä»¤
```

#### 4. test_perf_sanity.py æ ¹æ® DISAGG_SERVING_TYPE åˆ†æ”¯æ‰§è¡Œ

**ä»£ç ï¼ˆtest_perf_sanity.py:682-733ï¼‰ï¼š**

```python
def run_cmd(self, server_idx: int) -> List[str]:
    """Run commands for a server and return outputs."""
    outputs = []
    benchmark_status_file = os.path.join(self.output_dir, f"benchmark_status.{server_idx}.txt")
    port = get_free_port()

    ctx_cmd, gen_cmd, disagg_cmd = self.server_cmds[server_idx]
    
    # åˆ†æ”¯ 1: CTX æˆ– GEN servers
    if "CTX" in self.disagg_serving_type or "GEN" in self.disagg_serving_type:
        self._generate_hostname_file(server_idx, port)
        server_file_path = os.path.join(
            self.output_dir, f"trtllm-serve.{server_idx}.{self.disagg_serving_type}.log"
        )
        is_ctx = "CTX" in self.disagg_serving_type
        server_cmd = ctx_cmd if is_ctx else gen_cmd
        server_cmd = add_host_port_to_cmd(server_cmd, self.hostname, port)
        try:
            print_info(
                f"Starting server. disagg_serving_type: {self.disagg_serving_type} cmd is {server_cmd}"
            )
            with open(server_file_path, "w") as server_ctx:
                # â† ç›´æ¥ subprocess.Popen å¯åŠ¨ trtllm-serve
                server_proc = subprocess.Popen(
                    server_cmd,  # ["trtllm-serve", model_path, "--backend", "pytorch", ...]
                    stdout=server_ctx,
                    stderr=subprocess.STDOUT,
                    env=copy.deepcopy(os.environ),
                )
            self.wait_for_benchmark_ready(benchmark_status_file)
        finally:
            print_info(f"Server {self.disagg_serving_type} stopped")
            server_proc.terminate()
            server_proc.wait()

    # åˆ†æ”¯ 2: DISAGG_SERVER
    elif self.disagg_serving_type == "DISAGG_SERVER":
        disagg_server_file_path = os.path.join(
            self.output_dir, f"trtllm-serve.{server_idx}.{self.disagg_serving_type}.log"
        )
        try:
            self._generate_disagg_server_config(server_idx, port)
            print_info(f"Starting disagg server. cmd is {disagg_cmd}")
            with open(disagg_server_file_path, "w") as disagg_server_ctx:
                # â† å¯åŠ¨åè°ƒå™¨
                disagg_server_proc = subprocess.Popen(
                    disagg_cmd,  # ["trtllm-serve-coordinator", ...]
                    stdout=disagg_server_ctx,
                    stderr=subprocess.STDOUT,
                    env=copy.deepcopy(os.environ),
                )
            self.wait_for_benchmark_ready(benchmark_status_file)
        finally:
            print_info(f"Disagg server {self.disagg_serving_type} stopped")
            disagg_server_proc.terminate()
            disagg_server_proc.wait()

    # åˆ†æ”¯ 3: BENCHMARK
    elif self.disagg_serving_type == "BENCHMARK":
        try:
            disagg_server_hostname, disagg_server_port = (
                self._get_disagg_server_hostname_and_port(server_idx)
            )
            # ç­‰å¾…æ‰€æœ‰æœåŠ¡å™¨å¯åŠ¨
            wait_for_endpoint_ready(
                f"http://{disagg_server_hostname}:{disagg_server_port}/health",
                timeout=self.timeout,
                check_files=server_files,
            )

            # è¿è¡Œ benchmark å®¢æˆ·ç«¯
            for client_cmd in self.client_cmds[server_idx]:
                client_cmd_with_port = add_host_port_to_cmd(
                    client_cmd, disagg_server_hostname, disagg_server_port
                )
                print_info(f"Starting benchmark. cmd is {client_cmd_with_port}")

                # â† ç›´æ¥è¿è¡Œ benchmark_serving.py
                output = subprocess.check_output(
                    client_cmd_with_port,  # ["python", "-m", "tensorrt_llm.serve.scripts.benchmark_serving", ...]
                    env=copy.deepcopy(os.environ),
                    stderr=subprocess.STDOUT,
                ).decode()

                outputs.append(output)

            # é€šçŸ¥æ‰€æœ‰æœåŠ¡å™¨å¯ä»¥é€€å‡ºäº†
            with open(benchmark_status_file, "w") as f:
                f.write("done\n")
```

---

## ğŸ” å…³é”®åŒºåˆ«ï¼štrtllm-serve vs trtllm-llmapi-launch

### trtllm-llmapi-launchï¼ˆL0_Test.groovy ä½¿ç”¨ï¼‰

**ç”¨é€”ï¼š**
- pytest çš„å¤šè¿›ç¨‹å¯åŠ¨å™¨
- ç”¨äº MPI/åˆ†å¸ƒå¼æµ‹è¯•æ¡†æ¶
- å¯åŠ¨å¤šä¸ª pytest è¿›ç¨‹ï¼Œæ¯ä¸ªè¿›ç¨‹æœ‰ä¸åŒçš„ rank

**å‘½ä»¤æ ¼å¼ï¼š**
```bash
trtllm-llmapi-launch pytest test_module.py::test_func[test_case]
```

**æ‰§è¡Œæµç¨‹ï¼š**
```
trtllm-llmapi-launch
  â†“ å¯åŠ¨ N ä¸ª pytest è¿›ç¨‹
  â”œâ”€â”€ Rank 0: pytest test_module.py (MASTER)
  â”œâ”€â”€ Rank 1: pytest test_module.py (WORKER)
  â””â”€â”€ Rank N-1: pytest test_module.py (WORKER)
  â†“
æ¯ä¸ª pytest è¿›ç¨‹æ‰§è¡Œæµ‹è¯•
  â†“ æµ‹è¯•å†…éƒ¨å¯èƒ½å¯åŠ¨ TensorRT-LLM æœåŠ¡
```

**é€‚ç”¨åœºæ™¯ï¼š**
- é€šç”¨çš„å¤š GPU æµ‹è¯•
- éœ€è¦ MPI é€šä¿¡çš„æµ‹è¯•
- æ ‡å‡†çš„ pytest åˆ†å¸ƒå¼æ¡†æ¶

---

### trtllm-serveï¼ˆtest_perf_sanity.py ä½¿ç”¨ï¼‰

**ç”¨é€”ï¼š**
- TensorRT-LLM çš„æœåŠ¡å™¨å¯åŠ¨å‘½ä»¤
- ç›´æ¥å¯åŠ¨æ¨ç†æœåŠ¡
- ä¸æ¶‰åŠ pytest çš„å¤šè¿›ç¨‹ç®¡ç†

**å‘½ä»¤æ ¼å¼ï¼š**
```bash
trtllm-serve /path/to/model --backend pytorch --config extra-llm-api-config.yml
```

**æ‰§è¡Œæµç¨‹ï¼š**
```
pytest test_perf_sanity.py (å•ä¸ªè¿›ç¨‹)
  â†“ è¯»å– DISAGG_SERVING_TYPE ç¯å¢ƒå˜é‡
  â†“ æ ¹æ®ç±»å‹åˆ†æ”¯æ‰§è¡Œ
  â”œâ”€â”€ CTX/GEN: subprocess.Popen(["trtllm-serve", model, ...])
  â”œâ”€â”€ DISAGG_SERVER: subprocess.Popen(["trtllm-serve-coordinator", ...])
  â””â”€â”€ BENCHMARK: subprocess.check_output(["python", "-m", "benchmark_serving", ...])
```

**é€‚ç”¨åœºæ™¯ï¼š**
- disaggregated æ€§èƒ½æµ‹è¯•
- æ¯ä¸ªç»„ä»¶ç‹¬ç«‹å¯åŠ¨
- pytest ä½œä¸ºç¼–æ’å·¥å…·ï¼Œä¸éœ€è¦å¤šè¿›ç¨‹

---

## ğŸ¯ ä¸ºä»€ä¹ˆ test_perf_sanity.py ä¸éœ€è¦ trtllm-llmapi-launchï¼Ÿ

### åŸå›  1: Slurm å·²ç»è´Ÿè´£å¤šè¿›ç¨‹ç®¡ç†

```bash
# slurm_launch_draft.sh ä½¿ç”¨ srun ä¸ºæ¯ä¸ªç»„ä»¶å¯åŠ¨ç‹¬ç«‹çš„è¿›ç¨‹

# GEN_0
srun -N 1 -n $gpusPerNode ... slurm_run.sh
  â†“ DISAGG_SERVING_TYPE=GEN_0
  â†“ pytest test_perf_sanity.py
  â†“ subprocess.Popen(["trtllm-serve", ...])

# CTX_0
srun -N 1 -n $gpusPerNode ... slurm_run.sh
  â†“ DISAGG_SERVING_TYPE=CTX_0
  â†“ pytest test_perf_sanity.py
  â†“ subprocess.Popen(["trtllm-serve", ...])

# DISAGG_SERVER
srun -N 1 ... slurm_run.sh
  â†“ DISAGG_SERVING_TYPE=DISAGG_SERVER
  â†“ pytest test_perf_sanity.py
  â†“ subprocess.Popen(["trtllm-serve-coordinator", ...])

# BENCHMARK
srun -N 1 ... slurm_run.sh
  â†“ DISAGG_SERVING_TYPE=BENCHMARK
  â†“ pytest test_perf_sanity.py
  â†“ subprocess.check_output(["benchmark_serving", ...])
```

**å…³é”®ï¼š**
- âœ… æ¯ä¸ª `srun` å¯åŠ¨ä¸€ä¸ªç‹¬ç«‹çš„ **pytest è¿›ç¨‹**
- âœ… æ¯ä¸ª pytest è¿›ç¨‹æ ¹æ® `DISAGG_SERVING_TYPE` æ‰§è¡Œä¸åŒçš„ä»»åŠ¡
- âœ… ä¸éœ€è¦ `trtllm-llmapi-launch` æ¥ç®¡ç†å¤šä¸ª pytest è¿›ç¨‹

---

### åŸå›  2: trtllm-serve å·²ç»æ”¯æŒå¤š GPU

**trtllm-serve å†…éƒ¨å¤„ç†ï¼š**
- è‡ªåŠ¨æ£€æµ‹å¯ç”¨çš„ GPUï¼ˆé€šè¿‡ `CUDA_VISIBLE_DEVICES` æˆ– `NVIDIA_VISIBLE_DEVICES`ï¼‰
- æ ¹æ®é…ç½®ï¼ˆTP/PP/EPï¼‰è‡ªåŠ¨åˆ†é… GPU
- ä¸éœ€è¦å¤–éƒ¨çš„ MPI å¯åŠ¨å™¨

**ç¤ºä¾‹å‘½ä»¤ï¼š**
```bash
# Slurm è®¾ç½®ç¯å¢ƒå˜é‡
export CUDA_VISIBLE_DEVICES=0,1,2,3

# ç›´æ¥å¯åŠ¨ trtllm-serveï¼ˆä¼šè‡ªåŠ¨ä½¿ç”¨ 4 ä¸ª GPUï¼‰
trtllm-serve /path/to/model --backend pytorch --config config.yml
```

---

### åŸå›  3: disaggregated æ¶æ„çš„ç‰¹æ®Šæ€§

**disaggregated æ¶æ„ï¼š**
- æ¯ä¸ªç»„ä»¶ï¼ˆGEN/CTX/DISAGG_SERVER/BENCHMARKï¼‰æ˜¯**ç‹¬ç«‹çš„è¿›ç¨‹**
- é€šè¿‡ HTTP/gRPC é€šä¿¡ï¼Œä¸æ˜¯é€šè¿‡ MPI
- ä¸éœ€è¦å…±äº«çš„ pytest å¤šè¿›ç¨‹æ¡†æ¶

**å¯¹æ¯”æ ‡å‡† aggregated æ¨¡å¼ï¼ˆL0 æµ‹è¯•ï¼‰ï¼š**
- æ‰€æœ‰ GPU åœ¨ä¸€ä¸ªè¿›ç¨‹ç»„ä¸­
- éœ€è¦ MPI é€šä¿¡ï¼ˆNCCLï¼‰
- éœ€è¦ `trtllm-llmapi-launch` æ¥åè°ƒå¤šä¸ª pytest è¿›ç¨‹

---

## ğŸ“Š ä¸¤ç§æ¨¡å¼çš„å®Œæ•´å¯¹æ¯”

### L0_Test.groovy æ¨¡å¼ï¼ˆAggregatedï¼‰

```
sbatch launch.sh
  â†“
srun trtllm-llmapi-launch pytest test_module.py
  â†“
trtllm-llmapi-launch å¯åŠ¨ 8 ä¸ª pytest è¿›ç¨‹ï¼ˆ8 GPUsï¼‰
  â”œâ”€â”€ Rank 0 (GPU 0): pytest â†’ å¯åŠ¨ TensorRT-LLM (MASTER)
  â”œâ”€â”€ Rank 1 (GPU 1): pytest â†’ å¯åŠ¨ TensorRT-LLM (WORKER)
  â”œâ”€â”€ ...
  â””â”€â”€ Rank 7 (GPU 7): pytest â†’ å¯åŠ¨ TensorRT-LLM (WORKER)
  â†“
æ‰€æœ‰ GPU åœ¨åŒä¸€ä¸ªæ¨ç†å¼•æ“ä¸­åä½œï¼ˆTP=8ï¼‰
```

**å…³é”®ï¼š**
- éœ€è¦ `trtllm-llmapi-launch` å¯åŠ¨å¤šä¸ª pytest è¿›ç¨‹
- æ‰€æœ‰è¿›ç¨‹é€šè¿‡ MPI/NCCL é€šä¿¡
- æµ‹è¯•ä»£ç åœ¨æ¯ä¸ª rank ä¸­æ‰§è¡Œ

---

### test_perf_sanity.py æ¨¡å¼ï¼ˆDisaggregatedï¼‰

```
sbatch launch.sh
  â†“
slurm_launch_draft.sh å¯åŠ¨ 4 ä¸ªç‹¬ç«‹çš„ srun
  â†“
â”œâ”€â”€ srun -N 1 -n 4 pytest test_perf_sanity.py (GEN_0)
â”‚   â†“ subprocess.Popen(["trtllm-serve", model, ...])
â”‚   â†“ trtllm-serve è‡ªå·±ç®¡ç† 4 ä¸ª GPUï¼ˆTP=4ï¼‰
â”‚
â”œâ”€â”€ srun -N 1 -n 4 pytest test_perf_sanity.py (CTX_0)
â”‚   â†“ subprocess.Popen(["trtllm-serve", model, ...])
â”‚   â†“ trtllm-serve è‡ªå·±ç®¡ç† 4 ä¸ª GPUï¼ˆTP=4ï¼‰
â”‚
â”œâ”€â”€ srun -N 1 pytest test_perf_sanity.py (DISAGG_SERVER)
â”‚   â†“ subprocess.Popen(["trtllm-serve-coordinator", ...])
â”‚
â””â”€â”€ srun -N 1 pytest test_perf_sanity.py (BENCHMARK)
    â†“ subprocess.check_output(["benchmark_serving", ...])
```

**å…³é”®ï¼š**
- **ä¸éœ€è¦** `trtllm-llmapi-launch`
- æ¯ä¸ª `srun` å¯åŠ¨ä¸€ä¸ª pytest è¿›ç¨‹
- pytest å†…éƒ¨ç”¨ `subprocess.Popen` å¯åŠ¨æœåŠ¡
- `trtllm-serve` è‡ªå·±ç®¡ç†å¤š GPU

---

## âœ… ç»“è®º

### ä½ æ˜¯å®Œå…¨æ­£ç¡®çš„ï¼ âœ…

1. **test_perf_sanity.py ç¡®å®ä¸ä½¿ç”¨ trtllm-llmapi-launch**
   - å®ƒä½¿ç”¨ `trtllm-serve` ç›´æ¥å¯åŠ¨æœåŠ¡
   - é€šè¿‡ `subprocess.Popen` è°ƒç”¨

2. **run_disagg_test.sh ä¸éœ€è¦æ·»åŠ  trtllm-llmapi-launch**
   - å½“å‰å®ç°æ˜¯æ­£ç¡®çš„
   - åªéœ€è¦ç®€å•çš„ `pytest` å‘½ä»¤

3. **ä¸¤ç§æ¨¡å¼çš„æ ¹æœ¬åŒºåˆ«ï¼š**
   - **L0 æ¨¡å¼**: pytest å¤šè¿›ç¨‹ â†’ éœ€è¦ `trtllm-llmapi-launch`
   - **Disagg æ¨¡å¼**: pytest å•è¿›ç¨‹ â†’ å†…éƒ¨å¯åŠ¨å¤šä¸ªæœåŠ¡ â†’ ä¸éœ€è¦ `trtllm-llmapi-launch`

---

## ğŸ”§ æœ€ç»ˆçš„ run_disagg_test.sh å®ç°

### æ­£ç¡®çš„ç®€åŒ–ç‰ˆï¼ˆæ¨èï¼‰âœ…

```bash
# æ­¥éª¤ 0: è¯»å–è‡ªå®šä¹‰æµ‹è¯•æ¨¡å—é…ç½®
PERF_TEST_MODULE="${PERF_TEST_MODULE:-perf/test_perf_sanity.py}"
PERF_TEST_FUNCTION="${PERF_TEST_FUNCTION:-test_e2e}"
PERF_TEST_PREFIX="${PERF_TEST_PREFIX:-disagg_upload}"

# æ­¥éª¤ 4.2: ç”Ÿæˆ slurm_launch_prefix.sh
cat > "$SCRIPT_PREFIX_FILE" << EOFPREFIX
#!/bin/bash
# ... SBATCH directives ...

export pytestCommand="pytest ${PERF_TEST_MODULE}::${PERF_TEST_FUNCTION}[${PERF_TEST_PREFIX}-${CONFIG_NAME}] -vv --junit-xml=$WORKSPACE/results.xml"
# â†‘ ä¸éœ€è¦ trtllm-llmapi-launchï¼
EOFPREFIX
```

**è¿™æ˜¯æ­£ç¡®çš„å®ç°ï¼Œå› ä¸ºï¼š**
- âœ… test_perf_sanity.py å†…éƒ¨è‡ªå·±ç®¡ç†æœåŠ¡å¯åŠ¨
- âœ… Slurm çš„ srun è´Ÿè´£å¤šè¿›ç¨‹ç¼–æ’
- âœ… trtllm-serve è‡ªå·±ç®¡ç†å¤š GPU
- âœ… ä¸éœ€è¦é¢å¤–çš„ pytest å¤šè¿›ç¨‹æ¡†æ¶

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

1. **L0 vs Disagg å¯¹æ¯”**: `jenkins_test/docs/L0_VS_DISAGG_PYTEST_COMMAND.md`ï¼ˆéœ€è¦æ›´æ–°ï¼‰
2. **submit.py é€»è¾‘**: `jenkins_test/docs/SUBMIT_PY_PYTEST_COMMAND_LOGIC.md`
3. **å‚æ•°ä¼ é€’**: `jenkins_test/docs/SLURM_LAUNCH_PREFIX_PARAM_PASSING.md`

---

**æ€»ç»“ï¼šä½ çš„è§‚å¯Ÿéå¸¸æ•é”ï¼test_perf_sanity.py ä½¿ç”¨çš„æ˜¯ `trtllm-serve`ï¼ˆé€šè¿‡ subprocess.Popenï¼‰ï¼Œè€Œä¸æ˜¯ `trtllm-llmapi-launch`ã€‚è¿™æ˜¯ disaggregated æ¨¡å¼çš„æ­£ç¡®å®ç°æ–¹å¼ã€‚** ğŸ¯
